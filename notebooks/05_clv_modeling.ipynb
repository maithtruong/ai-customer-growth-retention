{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62021e59",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654be9e",
   "metadata": {},
   "source": [
    "**Task**\n",
    "Future value estimation based on customer lifetime.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "*Approach 1 – BG-NBD + Gamma–Gamma*\n",
    "- Fit BG-NBD. Predict:\n",
    "    - Expected number of future transactions\n",
    "    - Probability customer is alive\n",
    "- Fit Gamma–Gamma. Compute:\n",
    "    - Expected monetary value\n",
    "    - CLV over time horizon T\n",
    "\n",
    "*Approach 2 – Survival Analysis + Gamma–Gamma*\n",
    "- Use survival model output. Predict:\n",
    "    - Survival curve\n",
    "    - Expected remaining lifetime\n",
    "- Fit Gamma–Gamma. Compute:\n",
    "    - Time-dependent CLV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0921b2",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5870bc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f2e0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70127628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10768444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "daa43e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c04a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.transforms import (\n",
    "    transform_transactions_df,\n",
    "    transform_customers_df,\n",
    "    get_customers_screenshot_summary_from_transactions_df,\n",
    "    add_churn_status\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3ba6fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce9b536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifetimes import GammaGammaFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eabe917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb126e79",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "765a74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd9822b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_CUSTOMERS=os.getenv(\"SEED_CUSTOMERS\")\n",
    "SEED_TRANSACTIONS=os.getenv(\"SEED_TRANSACTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75bf7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVED_DATE = pd.Timestamp('2025-12-31')\n",
    "OBSERVED_DATE_STR = OBSERVED_DATE.strftime(\"%d_%m_%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33b99326",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE = OBSERVED_DATE - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15e5e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7c82669",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold\" / OBSERVED_DATE_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ef761b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLRUNS_DIR = PROJECT_ROOT / \"mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5dfe97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "672126d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"gamma-gamma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9b710ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=['is_churn_30_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4631505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db33a1",
   "metadata": {},
   "source": [
    "## 02 Notebook Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bd39f",
   "metadata": {},
   "source": [
    "I'm temporarily copying the functions from 02 because I haven't put them in the Python package yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1033e63",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60032b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfm_window_features(customers_df, transactions_df, observed_date):\n",
    "\n",
    "    rfm_time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    for rfm_time_window in rfm_time_windows:\n",
    "\n",
    "        if rfm_time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(rfm_time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "        # Get a Customers Screenshot Summary DataFrame. It has RFM features and other variables that RFM features depend on.\n",
    "        summary_modeling_df = get_customers_screenshot_summary_from_transactions_df(\n",
    "            transactions_df=filtered_transactions_df,\n",
    "            observed_date=observed_date,\n",
    "            column_names=[\"customer_id\", \"transaction_date\", \"amount\"]\n",
    "        )\n",
    "\n",
    "        # Keep only customer_id and the RFM columns we care about\n",
    "        summary_modeling_df = summary_modeling_df[[\n",
    "            'customer_id',\n",
    "            'days_until_observed',\n",
    "            'period_transaction_count',\n",
    "            'period_total_amount',\n",
    "            'period_tenure_days'\n",
    "        ]]\n",
    "\n",
    "        # Rename columns in the summary DF, not the main DF\n",
    "        summary_modeling_df = summary_modeling_df.rename(columns={\n",
    "            'days_until_observed': f'rfm_recency_{rfm_time_window}',\n",
    "            'period_transaction_count': f'rfm_frequency_{rfm_time_window}',\n",
    "            'period_total_amount': f'rfm_monetary_{rfm_time_window}',\n",
    "            'period_tenure_days': f'tenure_{rfm_time_window}'\n",
    "        })\n",
    "        \n",
    "        # Merge with current data used for modelling.\n",
    "        customers_df = pd.merge(\n",
    "            customers_df,\n",
    "            summary_modeling_df,\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38636fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_features(customers_df, transactions_df, observed_date, feature_list):\n",
    "\n",
    "    time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    for time_window in time_windows:\n",
    "\n",
    "        if time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "    customers_list = filtered_transactions_df['customer_id'].unique()\n",
    "\n",
    "    slopes = {}\n",
    "\n",
    "    for customer_id in customers_list:\n",
    "\n",
    "        customer_transactions = filtered_transactions_df[filtered_transactions_df['customer_id'] == customer_id]\n",
    "\n",
    "        x = np.arange(len(customer_transactions)) #time axis\n",
    "        slopes[customer_id] = {} #initiate value list\n",
    "\n",
    "        for feature_name in feature_list:\n",
    "            y = customer_transactions[feature_name].values\n",
    "            x_valid = x[~np.isnan(y)]\n",
    "            y_valid = y[~np.isnan(y)]\n",
    "\n",
    "            if len(y_valid) < 2:\n",
    "                slopes[customer_id][feature_name] = np.nan\n",
    "            else:\n",
    "                slope = np.polyfit(x_valid, y_valid, 1)[0]\n",
    "                slopes[customer_id][feature_name] = slope\n",
    "\n",
    "    # Convert dict of dicts into dataframe\n",
    "    slope_features_df = pd.DataFrame.from_dict(slopes, orient='index')\n",
    "\n",
    "    # Rename columns to have slope_ prefix\n",
    "    slope_features_df = slope_features_df.rename(columns={f: f'slope_{f}' for f in slope_features_df.columns})\n",
    "\n",
    "    # Reset index to have customer_id as a column\n",
    "    slope_features_df = slope_features_df.reset_index().rename(columns={'index': 'customer_id'})\n",
    "\n",
    "    # Merge with current data used for modelling.\n",
    "    customers_df = pd.merge(\n",
    "        customers_df,\n",
    "        slope_features_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdc84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_statistics_features(customers_df, transactions_df, observed_date, feature_list):\n",
    "\n",
    "    time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    all_stats_df_list = []\n",
    "\n",
    "    for time_window in time_windows:\n",
    "\n",
    "        if time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "        customers_list = filtered_transactions_df['customer_id'].unique()\n",
    "        stats_dict = {}\n",
    "\n",
    "        for customer_id in customers_list:\n",
    "\n",
    "            customer_transactions = filtered_transactions_df[\n",
    "                filtered_transactions_df['customer_id'] == customer_id\n",
    "            ]\n",
    "\n",
    "            stats_dict[customer_id] = {}\n",
    "\n",
    "            for feature_name in feature_list:\n",
    "\n",
    "                y = customer_transactions[feature_name].dropna().values\n",
    "\n",
    "                if len(y) < 2:\n",
    "                    # Less than 2 observations -> return NaN for all stats\n",
    "                    stats_dict[customer_id][f\"min_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"mean_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"mode_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"max_{feature_name}\"] = np.nan\n",
    "                    for q in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]:\n",
    "                        stats_dict[customer_id][f\"q{q}_{feature_name}\"] = np.nan\n",
    "                    continue\n",
    "\n",
    "                # Compute stats\n",
    "                stats_dict[customer_id][f\"min_{feature_name}\"] = np.min(y)\n",
    "                stats_dict[customer_id][f\"mean_{feature_name}\"] = np.mean(y)\n",
    "\n",
    "                # Compute mode safely\n",
    "                mode_result = stats.mode(y, nan_policy='omit')\n",
    "                if hasattr(mode_result.mode, \"__len__\"):\n",
    "                    # old SciPy: mode is array\n",
    "                    mode_val = mode_result.mode[0] if len(mode_result.mode) > 0 else np.nan\n",
    "                else:\n",
    "                    # new SciPy: mode is scalar\n",
    "                    mode_val = mode_result.mode if mode_result.count > 0 else np.nan\n",
    "\n",
    "                stats_dict[customer_id][f\"mode_{feature_name}\"] = mode_val\n",
    "\n",
    "                stats_dict[customer_id][f\"max_{feature_name}\"] = np.max(y)\n",
    "\n",
    "                # Quantiles\n",
    "                for q in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]:\n",
    "                    stats_dict[customer_id][f\"q{q}_{feature_name}\"] = np.percentile(y, q)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        stats_df = pd.DataFrame.from_dict(stats_dict, orient='index').reset_index().rename(columns={'index': 'customer_id'})\n",
    "        all_stats_df_list.append(stats_df)\n",
    "\n",
    "    # Merge with customers_df (only keep last time_window stats)\n",
    "    final_stats_df = all_stats_df_list[-1]  # or merge all windows if needed\n",
    "    customers_df = pd.merge(customers_df, final_stats_df, on='customer_id', how='left')\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c6e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_base(\n",
    "    seed_customers_path,\n",
    "    seed_transactions_path,\n",
    "    train_snapshot_date,\n",
    "    churn_windows=[30, 60, 90],\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads raw data, transforms it, limits it to modeling window,\n",
    "    builds customer modeling table, and adds churn labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Read data ---\n",
    "    customers_df = pd.read_csv(seed_customers_path)\n",
    "    transactions_df = pd.read_csv(seed_transactions_path)\n",
    "\n",
    "    # --- Transform data ---\n",
    "    transactions_df = transform_transactions_df(transactions_df)\n",
    "    customers_df = transform_customers_df(customers_df)\n",
    "\n",
    "    # --- Derive MAX_DATA_DATE internally ---\n",
    "    max_data_date = transactions_df[\"transaction_date\"].max()\n",
    "\n",
    "    # --- Limit transactions to snapshot ---\n",
    "    transactions_modeling_df = transactions_df.loc[\n",
    "        transactions_df[\"transaction_date\"] <= train_snapshot_date\n",
    "    ]\n",
    "\n",
    "    # --- Build customer modeling base ---\n",
    "    customers_modeling_df = (\n",
    "        pd.DataFrame({\n",
    "            \"customer_id\": transactions_modeling_df[\"customer_id\"].unique()\n",
    "        })\n",
    "        .merge(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "        .drop(columns=[\"signup_date\", \"true_lifetime_days\", \"termination_date\"])\n",
    "    )\n",
    "\n",
    "    # --- Add churn labels ---\n",
    "    for nday in churn_windows:\n",
    "        var_name = f\"is_churn_{nday}_days\"\n",
    "        observed_date = max_data_date - pd.Timedelta(days=nday)\n",
    "\n",
    "        customers_modeling_df = add_churn_status(\n",
    "            transformed_customers_df=customers_df,\n",
    "            observed_date=observed_date,\n",
    "            desired_df=None,\n",
    "        )\n",
    "        \n",
    "        customers_modeling_df = customers_modeling_df.rename(columns={'is_churn': var_name})\n",
    "\n",
    "    return transactions_modeling_df, customers_modeling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff1e08",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8822165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_in_df_cols(df):\n",
    "    # Get relative percentage of nulls by column\n",
    "    null_features_proportion = (\n",
    "        df.isna().sum() / len(df)\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    high_proportion = []\n",
    "    medium_proportion = []\n",
    "    low_proportion = []\n",
    "\n",
    "    for feature, proportion in null_features_proportion.items():\n",
    "        if proportion >= 0.20:\n",
    "            high_proportion.append(feature)\n",
    "        elif 0.05 <= proportion < 0.20:\n",
    "            medium_proportion.append(feature)\n",
    "        else:\n",
    "            low_proportion.append(feature)\n",
    "\n",
    "    # Build features DataFrame\n",
    "    features_df = null_features_proportion.reset_index()\n",
    "    features_df.columns = [\"feature\", \"nan_proportion\"]\n",
    "\n",
    "    features_df[\"NaN group\"] = features_df[\"feature\"].apply(\n",
    "        lambda f: (\n",
    "            \"High\" if f in high_proportion\n",
    "            else \"Medium\" if f in medium_proportion\n",
    "            else \"Low\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Print counts (same behavior as before)\n",
    "    print(\"Total features:\", len(df.columns))\n",
    "    print(\"Information on NaN values\")\n",
    "    print(\"====================================\")\n",
    "    print(\"Number of High Proportion Features:\", len(high_proportion))\n",
    "    print(\"Number of Medium Proportion Features:\", len(medium_proportion))\n",
    "    print(\"Number of Low Proportion Features:\", len(low_proportion))\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab32c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_X_csv(X_by_target, BASE_GOLD_DIR):\n",
    "\n",
    "    for target in X_by_target.keys():\n",
    "\n",
    "        target_dir = BASE_GOLD_DIR / target\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_by_target[target].to_csv(\n",
    "            target_dir / \"X_train.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "        print(f\"[{target}] written to {target_dir}\")\n",
    "    \n",
    "    return \"All data saved successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6212924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_y_csv(\n",
    "        X_by_target,\n",
    "        y,\n",
    "        BASE_GOLD_DIR\n",
    "    ):\n",
    "\n",
    "    for target in targets:\n",
    "        target_dir = BASE_GOLD_DIR / target\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ----------------------------\n",
    "        # TRAIN labels\n",
    "        # ----------------------------\n",
    "        y.loc[\n",
    "            X_by_target[target].index, target\n",
    "        ].to_csv(\n",
    "            target_dir / \"y_train.csv\",\n",
    "            header=True,\n",
    "        )\n",
    "    \n",
    "    return \"All data saved successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c276364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_features_csv(df, split, base_gold_dir, index_name='customer_id'):\n",
    "    \n",
    "    path = Path(base_gold_dir) / \"raw\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"WRITING TO:\", path.resolve())\n",
    "\n",
    "    df.index.name = index_name\n",
    "    df.to_csv(\n",
    "        path / f\"{split}_features.csv\",\n",
    "        index=True, # keep customer_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5a82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_by_target_csv(X_by_target, split, base_gold_dir, index_name='customer_id'):\n",
    "\n",
    "    for target, df in X_by_target.items():\n",
    "        \n",
    "        base_path = Path(base_gold_dir) / \"transformed\" / target\n",
    "        base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df.index.name = index_name\n",
    "        df.to_csv(\n",
    "            base_path / f\"X_{split}.csv\",\n",
    "            index=True,  # keep customer_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "946c00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformed(BASE_GOLD_DIR, split, target):\n",
    "    return pd.read_csv(\n",
    "        BASE_GOLD_DIR / \"transformed\" / target / f\"X_{split}.csv\",\n",
    "        index_col=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219282e",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0c52815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_feature_selection(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    target,\n",
    "    cutoff=0.0,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform mutual information–based feature selection for a given target.\n",
    "\n",
    "    Returns:\n",
    "        selected_df: DataFrame with selected features\n",
    "        mi_scores: DataFrame with MI scores per feature\n",
    "        selected_features: Index of selected feature names\n",
    "    \"\"\"\n",
    "\n",
    "    assert X_train.index.equals(y_train.index)\n",
    "\n",
    "    mi_train = mutual_info_classif(\n",
    "        X_train,\n",
    "        y_train[target],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    mi_scores = (\n",
    "        pd.DataFrame(\n",
    "            mi_train,\n",
    "            index=X_train.columns,\n",
    "            columns=[\"mutual_info\"]\n",
    "        )\n",
    "        .sort_values(by=\"mutual_info\", ascending=False)\n",
    "    )\n",
    "\n",
    "    selected_features = mi_scores.loc[\n",
    "        mi_scores[\"mutual_info\"] > cutoff\n",
    "    ].index\n",
    "\n",
    "    selected_df = X_train[selected_features]\n",
    "\n",
    "    return selected_df, mi_scores, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc92a7",
   "metadata": {},
   "source": [
    "### Feature Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d072a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_time_features(transactions_df):\n",
    "    \"\"\"\n",
    "    Add time-based and order-based transaction features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions_df : pd.DataFrame\n",
    "        Must contain: customer_id, transaction_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of transactions_df with added features\n",
    "    \"\"\"\n",
    "\n",
    "    df = transactions_df.sort_values(\n",
    "        [\"customer_id\", \"transaction_date\"]\n",
    "    ).copy()\n",
    "\n",
    "    df[\"customer_transaction_order\"] = (\n",
    "        df.groupby(\"customer_id\").cumcount()\n",
    "    )\n",
    "\n",
    "    df[\"prev_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"].shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"next_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"].shift(-1)\n",
    "    )\n",
    "\n",
    "    df[\"days_since_previous_transaction\"] = (\n",
    "        df[\"transaction_date\"] - df[\"prev_transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    df[\"days_until_next_transaction\"] = (\n",
    "        df[\"next_transaction_date\"] - df[\"transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    df[\"first_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"]\n",
    "        .transform(\"min\")\n",
    "    )\n",
    "\n",
    "    df[\"days_since_first_transaction\"] = (\n",
    "        df[\"transaction_date\"] - df[\"first_transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca8a37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_customer_features(\n",
    "    transactions_modeling_df,\n",
    "    customers_modeling_df,\n",
    "    observed_date,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Build raw customer-level features from transactions and customers data.\n",
    "    No imputing, scaling, or selection is performed here.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Transaction-level features\n",
    "    transactions_df = add_transaction_time_features(\n",
    "        transactions_modeling_df\n",
    "    )\n",
    "\n",
    "    # 2. RFM window features\n",
    "    customers_df = get_rfm_window_features(\n",
    "        customers_df=customers_modeling_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "    )\n",
    "\n",
    "    # 3. Activity trend (slopes)\n",
    "    customers_df = get_slope_features(\n",
    "        customers_df=customers_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # 4. Transaction statistics\n",
    "    customers_df = get_transaction_statistics_features(\n",
    "        customers_df=customers_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a887e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_numeric_transformers(\n",
    "    X_train_numeric_df,\n",
    "    imputer_params=None,\n",
    "    scaler_params=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit numeric imputer and scaler on training data only.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_numeric_imputed_scaled_df : pd.DataFrame\n",
    "    numeric_imputer : fitted IterativeImputer\n",
    "    scaler : fitted StandardScaler\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Defaults\n",
    "    # -------------------------------\n",
    "    if imputer_params is None:\n",
    "        imputer_params = dict(\n",
    "            estimator=LinearRegression(),\n",
    "            max_iter=20,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    if scaler_params is None:\n",
    "        scaler_params = {}\n",
    "\n",
    "    # -------------------------------\n",
    "    # Imputation (FIT)\n",
    "    # -------------------------------\n",
    "    numeric_imputer = IterativeImputer(**imputer_params)\n",
    "    X_train_numeric_imputed = numeric_imputer.fit_transform(X_train_numeric_df)\n",
    "\n",
    "    X_train_numeric_imputed_df = pd.DataFrame(\n",
    "        X_train_numeric_imputed,\n",
    "        columns=X_train_numeric_df.columns,\n",
    "        index=X_train_numeric_df.index,\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Scaling (FIT)\n",
    "    # -------------------------------\n",
    "    scaler = StandardScaler(**scaler_params)\n",
    "    X_train_numeric_imputed_scaled = scaler.fit_transform(\n",
    "        X_train_numeric_imputed_df\n",
    "    )\n",
    "\n",
    "    X_train_numeric_imputed_scaled_df = pd.DataFrame(\n",
    "        X_train_numeric_imputed_scaled,\n",
    "        columns=X_train_numeric_df.columns,\n",
    "        index=X_train_numeric_df.index,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "503b7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_customers_numeric_features(\n",
    "    X_numeric,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply fitted numeric imputer and scaler.\n",
    "    \"\"\"\n",
    "\n",
    "    X_numeric_imputed = numeric_imputer.transform(X_numeric)\n",
    "    X_numeric_imputed_df = pd.DataFrame(\n",
    "        X_numeric_imputed,\n",
    "        columns=X_numeric.columns,\n",
    "        index=X_numeric.index,\n",
    "    )\n",
    "\n",
    "    X_numeric_scaled = scaler.transform(X_numeric_imputed_df)\n",
    "    X_numeric_scaled_df = pd.DataFrame(\n",
    "        X_numeric_scaled,\n",
    "        columns=X_numeric.columns,\n",
    "        index=X_numeric.index,\n",
    "    )\n",
    "\n",
    "    return X_numeric_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9468412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_per_target(\n",
    "    X_train_transformed_df,\n",
    "    y_train,\n",
    "    targets,\n",
    "    artifact_dir=None,\n",
    "    cutoff=0.0,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform feature selection per target using mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    assert X_train_transformed_df.index.equals(y_train.index), (\n",
    "        \"X_train and y_train must be index-aligned\"\n",
    "    )\n",
    "\n",
    "    X_train_by_target = {}\n",
    "    selected_features_by_target = {}\n",
    "    mi_scores_by_target = {}\n",
    "\n",
    "    for target in targets:\n",
    "        X_selected_df, mi_scores, selected_features = (\n",
    "            mutual_information_feature_selection(\n",
    "                X_train=X_train_transformed_df,\n",
    "                y_train=y_train,\n",
    "                target=target,\n",
    "                cutoff=cutoff,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if artifact_dir is not None:\n",
    "            with open(\n",
    "                artifact_dir / f\"selected_features_{target}.json\",\n",
    "                \"w\",\n",
    "            ) as f:\n",
    "                json.dump(list(selected_features), f)\n",
    "\n",
    "        X_train_by_target[target] = X_selected_df\n",
    "        selected_features_by_target[target] = list(selected_features)\n",
    "        mi_scores_by_target[target] = mi_scores\n",
    "\n",
    "        print(f\"[{target}] selected {len(selected_features)} features\")\n",
    "\n",
    "    return (\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "691f4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_per_target(\n",
    "    X_transformed_df,\n",
    "    selected_features_by_target\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform feature selection per target using mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    for target, selected_features in selected_features_by_target.items():\n",
    "\n",
    "        missing_features = set(selected_features) - set(\n",
    "            X_transformed_df.columns\n",
    "        )\n",
    "        if missing_features:\n",
    "            raise ValueError(\n",
    "                f\"Missing selected features at inference time: {missing_features}\"\n",
    "            )\n",
    "\n",
    "        X_selected_features = X_transformed_df[selected_features]\n",
    "        X_by_target[target] = X_selected_features\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23bf7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(\n",
    "    customers_modeling_df,\n",
    "    targets,\n",
    "    test_size=0.33,\n",
    "    val_size=0.33,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split customer modeling dataframe into train / val / test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    customers_modeling_df : pd.DataFrame\n",
    "        Must contain customer_id and target columns.\n",
    "    targets : list[str]\n",
    "        Target column names.\n",
    "    test_size : float\n",
    "        Proportion of data used for test+val split.\n",
    "    val_size : float\n",
    "        Proportion of test split used for validation.\n",
    "    random_state : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Feature / target separation\n",
    "    # -------------------------------\n",
    "    X_df = customers_modeling_df.drop(columns=targets)\n",
    "    X_df = X_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    y_df = customers_modeling_df[[\"customer_id\"] + targets]\n",
    "    y_df = y_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Train / temp split\n",
    "    # -------------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_df,\n",
    "        y_df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Test / validation split\n",
    "    # -------------------------------\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880df847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_customer_features_pipeline_train(\n",
    "    transactions_modeling_df,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    observed_date,\n",
    "    targets,\n",
    "    ARTIFACT_DIR=None,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for TRAIN data.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Build raw customer features\n",
    "    # --------------------------------------------------\n",
    "    X_train_raw_features_df = build_customer_features(\n",
    "        transactions_modeling_df=transactions_modeling_df,\n",
    "        customers_modeling_df=X_train,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Numeric transform (impute + scale)\n",
    "    # --------------------------------------------------\n",
    "    X_train_raw_features_df = X_train_raw_features_df.set_index(\"customer_id\", drop=False)\n",
    "    X_train_raw_features_numeric_df = X_train_raw_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    numeric_imputer, scaler = fit_numeric_transformers(\n",
    "        X_train_raw_features_numeric_df,\n",
    "        imputer_params=None,\n",
    "        scaler_params=None,\n",
    "    )\n",
    "\n",
    "    X_train_transformed_df = transform_customers_numeric_features(\n",
    "        X_train_raw_features_numeric_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Feature selection per target (EXTRACTED)\n",
    "    # --------------------------------------------------\n",
    "    (\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "    ) = select_features_per_target(\n",
    "        X_train_transformed_df=X_train_transformed_df,\n",
    "        y_train=y_train,\n",
    "        targets=targets,\n",
    "        artifact_dir=ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Save transformers ONCE\n",
    "    # --------------------------------------------------\n",
    "    if ARTIFACT_DIR is not None:\n",
    "        joblib.dump(\n",
    "            numeric_imputer,\n",
    "            ARTIFACT_DIR / \"numeric_imputer.joblib\",\n",
    "        )\n",
    "        joblib.dump(\n",
    "            scaler,\n",
    "            ARTIFACT_DIR / \"scaler.joblib\",\n",
    "        )\n",
    "\n",
    "    X_train_raw_features_df = X_train_raw_features_df.drop(columns=['customer_id'])\n",
    "\n",
    "    return (\n",
    "        X_train_raw_features_df,\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f713e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_customer_features_pipeline_test(\n",
    "    transactions_modeling_df,\n",
    "    X_test,\n",
    "    observed_date,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for TEST / VAL / INFERENCE data.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Build raw customer-level features from transactions\n",
    "    2. Remove customer_id from feature space\n",
    "    3. Apply fitted numeric transformations (imputer + scaler)\n",
    "    4. Select precomputed feature subset (STRICT reuse)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Build raw customer features\n",
    "    # --------------------------------------------------\n",
    "    X_test_features_df = build_customer_features(\n",
    "        transactions_modeling_df=transactions_modeling_df,\n",
    "        customers_modeling_df=X_test,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Set customer_id as index and REMOVE from features\n",
    "    # --------------------------------------------------\n",
    "    if \"customer_id\" not in X_test_features_df.columns:\n",
    "        raise ValueError(\"customer_id column missing after feature building\")\n",
    "\n",
    "    X_test_features_df = X_test_features_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Select numeric features and enforce column order\n",
    "    # --------------------------------------------------\n",
    "    X_test_numeric_features_df = X_test_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Enforce training-time column order (critical for IterativeImputer)\n",
    "    X_test_numeric_features_df = X_test_numeric_features_df[\n",
    "        numeric_imputer.feature_names_in_\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Apply fitted numeric transformations (NO FIT)\n",
    "    # --------------------------------------------------\n",
    "    X_test_numeric_features_transformed_df = transform_customers_numeric_features(\n",
    "        X_test_numeric_features_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Feature selection (STRICT reuse)\n",
    "    # --------------------------------------------------\n",
    "    missing_features = set(selected_features) - set(\n",
    "        X_test_numeric_features_transformed_df.columns\n",
    "    )\n",
    "    if missing_features:\n",
    "        raise ValueError(\n",
    "            f\"Missing selected features at inference time: {missing_features}\"\n",
    "        )\n",
    "\n",
    "    X_test_final_df = X_test_numeric_features_transformed_df[selected_features]\n",
    "\n",
    "    return X_test_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6284c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_select_for_multiple_targets_test(\n",
    "    X_test_raw_features_df,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features_by_target\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and transform customer features for multiple targets\n",
    "    (test / val / inference).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_by_target : dict[str, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    # Select numeric features and enforce column order\n",
    "    X_test_numeric_features_df = X_test_raw_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Enforce training-time column order (critical for IterativeImputer)\n",
    "    X_test_numeric_features_df = X_test_numeric_features_df[\n",
    "        numeric_imputer.feature_names_in_\n",
    "    ]\n",
    "\n",
    "    X_test_transformed_df = transform_customers_numeric_features(\n",
    "        X_test_numeric_features_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    X_by_target = get_features_per_target(\n",
    "        X_test_transformed_df,\n",
    "        selected_features_by_target\n",
    "    )\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8756f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_for_multiple_targets(\n",
    "    transactions_modeling_df,\n",
    "    X_df,\n",
    "    observed_date,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features_by_target,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and transform customer features for multiple targets\n",
    "    (test / val / inference).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_by_target : dict[str, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    for target, selected_features in selected_features_by_target.items():\n",
    "        X_by_target[target] = build_and_transform_customer_features_pipeline_test(\n",
    "            transactions_modeling_df=transactions_modeling_df,\n",
    "            X_test=X_df,\n",
    "            observed_date=observed_date,\n",
    "            numeric_imputer=numeric_imputer,\n",
    "            scaler=scaler,\n",
    "            selected_features=selected_features,\n",
    "            feature_list=[\n",
    "                \"amount\",\n",
    "                \"days_since_previous_transaction\",\n",
    "                \"days_until_next_transaction\",\n",
    "                \"customer_transaction_order\",\n",
    "                \"days_since_first_transaction\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac9f62",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83301a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lgb_feature_importance(\n",
    "    model,\n",
    "    importance_type=\"gain\",   # \"gain\" or \"split\"\n",
    "    normalize=False,\n",
    "    top_n=None,\n",
    "    title=None,\n",
    "    height=600,\n",
    "    as_percent=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot LightGBM feature importance for sklearn API models.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract feature names ---\n",
    "    if hasattr(model, \"feature_name_\"):\n",
    "        features = model.feature_name_\n",
    "    else:\n",
    "        raise ValueError(\"Model does not contain feature names\")\n",
    "\n",
    "    # --- Extract importance correctly ---\n",
    "    if importance_type == \"split\":\n",
    "        importance = model.feature_importances_\n",
    "    elif importance_type == \"gain\":\n",
    "        importance = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "    else:\n",
    "        raise ValueError(\"importance_type must be 'gain' or 'split'\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"importance\": importance\n",
    "    })\n",
    "\n",
    "    # Remove zero-importance features\n",
    "    df = df[df[\"importance\"] > 0]\n",
    "\n",
    "    # --- Normalize if requested ---\n",
    "    if normalize:\n",
    "        total = df[\"importance\"].sum()\n",
    "        df[\"importance\"] = df[\"importance\"] / total\n",
    "        if as_percent:\n",
    "            df[\"importance\"] *= 100\n",
    "            importance_label = \"Normalized Gain (%)\"\n",
    "            text_fmt = \".2f\"\n",
    "        else:\n",
    "            importance_label = \"Normalized Gain\"\n",
    "            text_fmt = \".4f\"\n",
    "    else:\n",
    "        importance_label = (\n",
    "            \"Gain\" if importance_type == \"gain\" else \"Split Count\"\n",
    "        )\n",
    "        text_fmt = \".2f\"\n",
    "\n",
    "    # Sort and keep top N\n",
    "    df = df.sort_values(\"importance\", ascending=False)\n",
    "    if top_n is not None:\n",
    "        df = df.head(top_n)\n",
    "\n",
    "    # Reverse for horizontal bar chart\n",
    "    df = df.sort_values(\"importance\", ascending=True)\n",
    "\n",
    "    if title is None:\n",
    "        norm_tag = \" (Normalized)\" if normalize else \"\"\n",
    "        title = f\"LightGBM Feature Importance ({importance_type.capitalize()}){norm_tag}\"\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x=\"importance\",\n",
    "        y=\"feature\",\n",
    "        orientation=\"h\",\n",
    "        title=title,\n",
    "        labels={\n",
    "            \"importance\": importance_label,\n",
    "            \"feature\": \"Feature\"\n",
    "        },\n",
    "        text=df[\"importance\"]\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        texttemplate=f\"%{{text:{text_fmt}}}\",\n",
    "        textposition=\"outside\",\n",
    "        cliponaxis=False\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        yaxis=dict(categoryorder=\"total ascending\"),\n",
    "        margin=dict(r=120)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab481200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_model(model, X, y, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a binary classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    y_proba = model.predict(X, num_iteration=model.best_iteration_)\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(y, y_proba),\n",
    "        \"pr_auc\": average_precision_score(y, y_proba),\n",
    "        \"confusion_matrix\": confusion_matrix(y, y_pred)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "109be13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_styled_df_confusion_matrix(cm):\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"Actual 0\", \"Actual 1\"],\n",
    "        columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "    )\n",
    "\n",
    "    styled_df = (\n",
    "        cm_df.style\n",
    "        .background_gradient(cmap=\"Blues\")\n",
    "        .format(\"{:.0f}\")\n",
    "    )\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b273cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, X_val, y_val, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a binary classifier on train, validation, and test sets.\n",
    "    Prints:\n",
    "    - ROC-AUC\n",
    "    - PR-AUC (Precision–Recall)\n",
    "    - Accuracy\n",
    "    - Confusion Matrix\n",
    "    - Classification Report\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "\n",
    "    for split_name, X, y in [\n",
    "        (\"TRAIN\", X_train, y_train),\n",
    "        (\"TEST\", X_test, y_test),\n",
    "        (\"VALIDATION\", X_val, y_val),\n",
    "    ]:\n",
    "        # Predicted probabilities and labels\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Metrics\n",
    "        roc_auc = roc_auc_score(y, y_proba)\n",
    "        pr_auc = average_precision_score(y, y_proba)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[\"Actual 0\", \"Actual 1\"],\n",
    "            columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{split_name}\")\n",
    "        print(\"-\" * len(split_name))\n",
    "        print(f\"ROC-AUC:      {roc_auc:.4f}\")\n",
    "        print(f\"PR-AUC:       {pr_auc:.4f}\")\n",
    "        print(f\"Accuracy:     {acc:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm_df)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf9a032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    target,\n",
    "    dataset_version,\n",
    "):\n",
    "    param_grid = {\n",
    "        \"num_leaves\": [31, 63],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [-1, 6],\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=3,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train[target])\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # ---------- Validation predictions ----------\n",
    "    val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    val_pred = (val_proba >= 0.5).astype(int)  # explicit threshold\n",
    "\n",
    "    # ---------- Metrics ----------\n",
    "    roc_auc = roc_auc_score(y_val[target], val_proba)\n",
    "    pr_auc = average_precision_score(y_val[target], val_proba)\n",
    "    precision = precision_score(y_val[target], val_pred)\n",
    "    recall = recall_score(y_val[target], val_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val[target], val_pred)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_0\", \"actual_1\"],\n",
    "        columns=[\"pred_0\", \"pred_1\"],\n",
    "    )\n",
    "\n",
    "    # ---------- MLflow ----------\n",
    "    input_example = X_train.iloc[:5]\n",
    "    signature = infer_signature(\n",
    "        X_train,\n",
    "        best_model.predict_proba(X_train)[:, 1],\n",
    "    )\n",
    "\n",
    "    mlflow.log_param(\"dataset_version\", dataset_version)\n",
    "    mlflow.log_param(\"target\", target)\n",
    "    mlflow.log_params(grid.best_params_)\n",
    "\n",
    "    mlflow.log_metric(\"val_roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"val_pr_auc\", pr_auc)\n",
    "    mlflow.log_metric(\"val_precision\", precision)\n",
    "    mlflow.log_metric(\"val_recall\", recall)\n",
    "\n",
    "    mlflow.log_text(\n",
    "        cm_df.to_string(),\n",
    "        artifact_file=f\"confusion_matrix/{dataset_version}_{target}.txt\",\n",
    "    )\n",
    "\n",
    "    mlflow.lightgbm.log_model(\n",
    "        best_model,\n",
    "        name=f\"{dataset_version}_{target}\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac68fcc",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9db082bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promote_to_production(run_id):\n",
    "    client.set_tag(run_id, \"stage\", \"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ea8902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_runs():\n",
    "    return mlflow.search_runs(\n",
    "        filter_string=\"tags.stage = 'production'\",\n",
    "        search_all_experiments=True,\n",
    "        output_format=\"pandas\",\n",
    "    )\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4da9eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_production_models():\n",
    "    prod_runs = get_production_runs()\n",
    "\n",
    "    models = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for _, row in prod_runs.iterrows():\n",
    "        target = row[\"params.target\"]\n",
    "        dataset_version = row[\"params.dataset_version\"]\n",
    "        run_id = row[\"run_id\"]\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{dataset_version}_{target}\"\n",
    "        model = mlflow.lightgbm.load_model(model_uri)\n",
    "\n",
    "        models[target] = model\n",
    "        metadata[target] = {\n",
    "            \"dataset_version\": dataset_version,\n",
    "            \"run_id\": run_id,\n",
    "        }\n",
    "\n",
    "    return models, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7cdc6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prod_bg_nbd():\n",
    "    exp = mlflow.get_experiment_by_name(\"bg-nbd\")\n",
    "    if exp is None:\n",
    "        raise ValueError(\"Experiment 'bg-nbd' not found\")\n",
    "\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=\"tags.stage = 'production'\",\n",
    "        output_format=\"pandas\",\n",
    "    )\n",
    "\n",
    "    if runs.empty:\n",
    "        raise ValueError(\"No production BG-NBD run found\")\n",
    "\n",
    "    run = runs.iloc[0]\n",
    "    run_id = run[\"run_id\"]\n",
    "\n",
    "    metadata = {\n",
    "        \"run_id\": run_id,\n",
    "        \"experiment_id\": exp.experiment_id,\n",
    "        \"experiment_name\": exp.name,\n",
    "        \"params\": run.filter(like=\"params.\").to_dict(),\n",
    "        \"metrics\": run.filter(like=\"metrics.\").to_dict(),\n",
    "        \"tags\": run.filter(like=\"tags.\").to_dict(),\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        path = mlflow.artifacts.download_artifacts(\n",
    "            run_id=run_id,\n",
    "            artifact_path=\"bg_nbd_model/bg_nbd.pkl\",\n",
    "            dst_path=d,\n",
    "        )\n",
    "        model = cloudpickle.load(open(path, \"rb\"))\n",
    "\n",
    "    return model, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75ee8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_features(\n",
    "    customer_ids,\n",
    "    target,\n",
    "    metadata,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "):\n",
    "    if isinstance(customer_ids, str):\n",
    "        customer_ids = [customer_ids]\n",
    "\n",
    "    dataset_version = metadata[target][\"dataset_version\"]\n",
    "\n",
    "    if dataset_version == \"raw\":\n",
    "        X = raw_features_df.loc[customer_ids]\n",
    "    elif dataset_version == \"transformed\":\n",
    "        X = transformed_features_by_target[target].loc[customer_ids]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset version: {dataset_version}\")\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e54855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn(\n",
    "    customer_id: str,\n",
    "    horizon_days: int,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "    models,\n",
    "    metadata,\n",
    "):\n",
    "    # ------------------\n",
    "    # Validate horizon\n",
    "    # ------------------\n",
    "    if horizon_days not in {30, 60, 90}:\n",
    "        raise ValueError(\"horizon_days must be one of {30, 60, 90}\")\n",
    "\n",
    "    target = f\"is_churn_{horizon_days}_days\"\n",
    "\n",
    "    if target not in models:\n",
    "        raise KeyError(f\"No production model loaded for target: {target}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Select features\n",
    "    # ------------------\n",
    "    X = get_customer_features(\n",
    "        customer_ids=[customer_id],\n",
    "        target=target,\n",
    "        metadata=metadata,\n",
    "        raw_features_df=raw_features_df,\n",
    "        transformed_features_by_target=transformed_features_by_target,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Predict\n",
    "    # ------------------\n",
    "    model = models[target]\n",
    "    churn_prob = float(model.predict_proba(X)[0, 1])\n",
    "\n",
    "    # ------------------\n",
    "    # Risk labeling (explicit, adjustable)\n",
    "    # ------------------\n",
    "    if churn_prob >= 0.7:\n",
    "        churn_label = \"high_risk\"\n",
    "    elif churn_prob >= 0.4:\n",
    "        churn_label = \"medium_risk\"\n",
    "    else:\n",
    "        churn_label = \"low_risk\"\n",
    "\n",
    "    return {\n",
    "        \"churn_probability\": round(churn_prob, 4),\n",
    "        \"churn_label\": churn_label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3d02de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churns(\n",
    "    customer_ids: list[str],\n",
    "    horizon_days: int,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "    models,\n",
    "    metadata,\n",
    "):\n",
    "    # ------------------\n",
    "    # Validate horizon\n",
    "    # ------------------\n",
    "    if horizon_days not in {30, 60, 90}:\n",
    "        raise ValueError(\"horizon_days must be one of {30, 60, 90}\")\n",
    "\n",
    "    target = f\"is_churn_{horizon_days}_days\"\n",
    "\n",
    "    if target not in models:\n",
    "        raise KeyError(f\"No production model loaded for target: {target}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Feature extraction (BULK)\n",
    "    # ------------------\n",
    "    X = get_customer_features(\n",
    "        customer_ids=customer_ids,\n",
    "        target=target,\n",
    "        metadata=metadata,\n",
    "        raw_features_df=raw_features_df,\n",
    "        transformed_features_by_target=transformed_features_by_target,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Predict (BULK)\n",
    "    # ------------------\n",
    "    model = models[target]\n",
    "    churn_probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # ------------------\n",
    "    # Risk labeling (vectorized)\n",
    "    # ------------------\n",
    "    churn_labels = np.where(\n",
    "        churn_probs >= 0.7,\n",
    "        \"high_risk\",\n",
    "        np.where(\n",
    "            churn_probs >= 0.4,\n",
    "            \"medium_risk\",\n",
    "            \"low_risk\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Output (aligned, explicit)\n",
    "    # ------------------\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"customer_id\": customer_ids,\n",
    "                \"churn_probability\": churn_probs.round(4),\n",
    "                \"churn_label\": churn_labels,\n",
    "            }\n",
    "        )\n",
    "        .set_index(\"customer_id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23f043bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(\n",
    "        dataset_version,\n",
    "        gold_data_version,\n",
    "        gold_dir=\"default\",\n",
    "        targets=targets\n",
    "    ):\n",
    "    '''\n",
    "        The service preloads the feature dataframes for faster search.\n",
    "    '''\n",
    "    if gold_dir == \"default\":\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        gold_dir = PROJECT_ROOT / \"data\" / \"gold\" / gold_data_version\n",
    "    \n",
    "    if dataset_version == \"raw\":\n",
    "        feature_df = pd.read_csv(gold_dir / dataset_version / \"all_features.csv\", index_col=0)\n",
    "        return feature_df\n",
    "    elif dataset_version == \"transformed\":\n",
    "        X_by_target = {}\n",
    "        for target in targets:\n",
    "            feature_df = pd.read_csv(gold_dir / dataset_version / target / \"X_all.csv\", index_col=0)\n",
    "            X_by_target[target] = feature_df\n",
    "        return X_by_target\n",
    "    else:\n",
    "        return \"Invalid dataset version. Please use only `raw` and `transformed`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8bcbd",
   "metadata": {},
   "source": [
    "## 03 Notebook Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebc01d",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af6fa6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cut_30d_features(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    customers_df: pd.DataFrame,\n",
    "    n_days: int,\n",
    "    observed_date\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds 30-day cutoff features for BG-NBD modeling.\n",
    "\n",
    "    Returns:\n",
    "    - features_df: customer-level modeling table\n",
    "    - transactions_cut_df: truncated transactions\n",
    "    \"\"\"\n",
    "\n",
    "    cutoff_date = observed_date - pd.Timedelta(n_days, unit='d')\n",
    "\n",
    "    (\n",
    "        transactions_cut_df,\n",
    "        customer_ids_cut_df\n",
    "    ) = cut_off_customers_and_transactions_df(\n",
    "        transactions_df=transactions_df,\n",
    "        customers_df=customers_df,\n",
    "        n_days=n_days,\n",
    "        end_date=observed_date\n",
    "    )\n",
    "\n",
    "    summary_cut_df = get_lifetimes_summary_df(\n",
    "        transactions_df=transactions_cut_df,\n",
    "        observed_date=cutoff_date,\n",
    "        column_names=[\"customer_id\", \"transaction_date\"]\n",
    "    )\n",
    "\n",
    "    summary_cut_df = add_churn_status(\n",
    "        transformed_customers_df=customer_ids_cut_df,\n",
    "        observed_date=cutoff_date,\n",
    "        desired_df=summary_cut_df,\n",
    "    )\n",
    "\n",
    "    summary_cut_df = summary_cut_df.drop(columns=[\"termination_date\"])\n",
    "\n",
    "    customer_ids_cut_df = customer_ids_cut_df.set_index(\"customer_id\")\n",
    "\n",
    "    customer_ids_cut_df[\"n_purchase_30d\"] = get_truth_number_of_purchases(\n",
    "        transactions_df,\n",
    "        customer_ids_cut_df.reset_index(),\n",
    "        n_days,\n",
    "        observed_date\n",
    "    )\n",
    "\n",
    "    customer_ids_cut_df = customer_ids_cut_df.reset_index()\n",
    "\n",
    "    features_df = pd.merge(\n",
    "        customer_ids_cut_df,\n",
    "        summary_cut_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cea68e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lifetimes_summary_df(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    observed_date: pd.Timestamp,\n",
    "    column_names: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a per-customer snapshot summary from a transactions DataFrame.\n",
    "\n",
    "    The function filters transactions up to the observed date and computes,\n",
    "    per customer:\n",
    "        - first transaction date in the period\n",
    "        - last transaction date in the period\n",
    "        - number of transactions in the period\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions_df : pd.DataFrame\n",
    "        Input transactions data.\n",
    "    observed_date : pd.Timestamp\n",
    "        Cutoff date for the snapshot.\n",
    "    column_names : list\n",
    "        Column names in the following order:\n",
    "        [customer_id, transaction_date]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Customer-level snapshot summary.\n",
    "    \"\"\"\n",
    "\n",
    "    customer_col, transaction_date_col = column_names\n",
    "\n",
    "    filtered_df = transactions_df[\n",
    "        transactions_df[transaction_date_col] <= observed_date\n",
    "    ]\n",
    "\n",
    "    summary_df = (\n",
    "        filtered_df\n",
    "        .groupby(customer_col, as_index=False)\n",
    "        .agg(\n",
    "            period_first_transaction_date=(transaction_date_col, 'min'),\n",
    "            period_last_transaction_date=(transaction_date_col, 'max'),\n",
    "            frequency=(customer_col, 'size')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    summary_df['frequency'] = summary_df['frequency'] - 1\n",
    "\n",
    "\n",
    "    summary_df['T'] = (\n",
    "        observed_date - summary_df['period_first_transaction_date']\n",
    "    ).dt.days\n",
    "\n",
    "    summary_df['recency'] = (\n",
    "        summary_df['period_last_transaction_date']\n",
    "        -\n",
    "        summary_df['period_first_transaction_date']\n",
    "    ).dt.days\n",
    "\n",
    "    return summary_df[[customer_col, 'frequency', 'T', 'recency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52d555da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lifetimes_summary_arrays(summary_df):\n",
    "\n",
    "    n = summary_df.shape[0]\n",
    "    x = summary_df[\"frequency\"].to_numpy()\n",
    "    t_x = summary_df[\"recency\"].to_numpy()\n",
    "    T = summary_df[\"T\"].to_numpy()\n",
    "\n",
    "    return (n, x, t_x, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f76a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_off_customers_and_transactions_df(\n",
    "    transactions_df,\n",
    "    customers_df,\n",
    "    n_days=30,\n",
    "    end_date=pd.Timestamp | None\n",
    "):\n",
    "    if end_date==None:\n",
    "        end_date = transactions_df['transaction_date'].max()\n",
    "    \n",
    "    cutoff_date = end_date - pd.Timedelta(days=n_days)\n",
    "    \n",
    "    transactions_cut_df = transactions_df[\n",
    "        (transactions_df[\"transaction_date\"] <= cutoff_date)\n",
    "    ]\n",
    "\n",
    "    customer_ids_cut = customers_df[\n",
    "        customers_df['customer_id'].isin(transactions_cut_df['customer_id'])\n",
    "    ].reset_index(drop=True)\n",
    "    \n",
    "    return transactions_cut_df, customer_ids_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b1930",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7fbca34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_churn_predictions(\n",
    "    feature_df: pd.DataFrame,\n",
    "    target_df: pd.DataFrame | None = None,\n",
    "    threshold: float = 0.5,\n",
    "    target_col: str = \"is_churn\",\n",
    "    proba_col: str = \"p_churn\",\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_df: must contain [proba_col]\n",
    "    target_df:\n",
    "        - if provided: must contain [target_col]\n",
    "        - if None: target_col must already be in feature_df\n",
    "\n",
    "    Assumes customer_id is the index in both DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decide where labels come from\n",
    "    if target_df is None:\n",
    "        missing = {target_col, proba_col} - set(feature_df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(\n",
    "                f\"feature_df is missing required columns: {missing}\"\n",
    "            )\n",
    "        eval_df = feature_df.copy()\n",
    "\n",
    "    else:\n",
    "        missing_f = {proba_col} - set(feature_df.columns)\n",
    "        missing_t = {target_col} - set(target_df.columns)\n",
    "\n",
    "        if missing_f or missing_t:\n",
    "            raise ValueError(\n",
    "                f\"Missing columns — \"\n",
    "                f\"feature_df: {missing_f}, target_df: {missing_t}\"\n",
    "            )\n",
    "\n",
    "        # Index-based alignment\n",
    "        eval_df = feature_df.join(\n",
    "            target_df[[target_col]],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Binary predictions\n",
    "    eval_df[\"pred_churn\"] = (eval_df[proba_col] >= threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        eval_df[target_col],\n",
    "        eval_df[\"pred_churn\"],\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_no_churn\", \"actual_churn\"],\n",
    "        columns=[\"pred_no_churn\", \"pred_churn\"]\n",
    "    )\n",
    "\n",
    "    return metrics, cm_df, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63a94e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truth_number_of_purchases(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    customer_cut_ids: pd.DataFrame,\n",
    "    n_days: int,\n",
    "    end_date=pd.Timestamp | None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the number of purchases per customer in the last `n_days`\n",
    "    ending at the max transaction_date.\n",
    "    \"\"\"\n",
    "\n",
    "    if end_date==None:\n",
    "        end_date = transactions_df['transaction_date'].max()\n",
    "    \n",
    "    cutoff_date = end_date - pd.Timedelta(days=n_days)\n",
    "    \n",
    "    # Filter transactions to relevant customers\n",
    "    sel_transactions_df = transactions_df[\n",
    "        transactions_df[\"customer_id\"].isin(customer_cut_ids[\"customer_id\"])\n",
    "    ]\n",
    "\n",
    "    # Keep only transactions in the future window\n",
    "    sel_transactions_df = sel_transactions_df[\n",
    "        sel_transactions_df[\"transaction_date\"] > cutoff_date\n",
    "    ]\n",
    "\n",
    "    # Count purchases per customer\n",
    "    purchase_counts = (\n",
    "        sel_transactions_df\n",
    "        .groupby(\"customer_id\")\n",
    "        .size()\n",
    "        .reindex(customer_cut_ids[\"customer_id\"], fill_value=0)\n",
    "    )\n",
    "\n",
    "    return purchase_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37084076",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e290d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bg_nbd_model(bgf):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        path = Path(tmpdir) / \"bg_nbd.pkl\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            cloudpickle.dump(bgf, f)\n",
    "\n",
    "        mlflow.log_artifact(path, artifact_path=\"bg_nbd_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ac32acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bg_nbd(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame | None = None,\n",
    "    X_val: pd.DataFrame | None = None,\n",
    "    y_val: pd.DataFrame | None = None,\n",
    "    X_test: pd.DataFrame | None = None,\n",
    "    y_test: pd.DataFrame | None = None,\n",
    "    target_2: str = \"n_purchase_30d\",\n",
    "    horizon_days=30,\n",
    "    threshold: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a BG-NBD model and evaluates churn when labels are available.\n",
    "\n",
    "    Assumptions:\n",
    "    - X_* index = customer_id\n",
    "    - X_* contain ['frequency', 'recency', 'T']\n",
    "    - y_* contain ['is_churn'] and share index with X_*\n",
    "    \"\"\"\n",
    "\n",
    "    pred_2 = f'pred_{target_2}'\n",
    "\n",
    "    # ======================\n",
    "    # Train BG-NBD\n",
    "    # ======================\n",
    "    bgf = BetaGeoFitter()\n",
    "    bgf.fit(\n",
    "        frequency=X_train[\"frequency\"],\n",
    "        recency=X_train[\"recency\"],\n",
    "        T=X_train[\"T\"],\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # Add predictions (IN PLACE)\n",
    "    # ======================\n",
    "    X_train = predict_p_alive_churn_bg_nbd(X_train, bgf)\n",
    "    X_train = predict_n_purchase_bg_nbd(X_train, bgf, t=horizon_days)\n",
    "\n",
    "    if X_val is not None:\n",
    "        X_val = predict_p_alive_churn_bg_nbd(X_val, bgf)\n",
    "        X_val = predict_n_purchase_bg_nbd(X_val, bgf, t=horizon_days)\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test = predict_p_alive_churn_bg_nbd(X_test, bgf)\n",
    "        X_test = predict_n_purchase_bg_nbd(X_test, bgf, t=horizon_days)\n",
    "\n",
    "    # ======================\n",
    "    # Evaluation helper (inline, no copies)\n",
    "    # ======================\n",
    "    def evaluate(X, y, split_name):\n",
    "        eval_df = X.join(y, how=\"inner\")\n",
    "\n",
    "        eval_df.loc[:, \"pred_churn\"] = (\n",
    "            eval_df[\"p_churn\"] >= threshold\n",
    "        ).astype(int)\n",
    "\n",
    "        mae = (\n",
    "            eval_df[target_2] - eval_df[pred_2]\n",
    "        ).abs().mean()\n",
    "\n",
    "        metrics = {\n",
    "            \"roc_auc\": roc_auc_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"p_churn\"]\n",
    "            ),\n",
    "            \"pr_auc\": average_precision_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"p_churn\"]\n",
    "            ),\n",
    "            \"precision\": precision_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"pred_churn\"]\n",
    "            ),\n",
    "            \"recall\": recall_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"pred_churn\"]\n",
    "            ),\n",
    "            \"mae\": mae,\n",
    "        }\n",
    "\n",
    "        cm = confusion_matrix(\n",
    "            eval_df[\"is_churn\"],\n",
    "            eval_df[\"pred_churn\"],\n",
    "        )\n",
    "\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[\"actual_no_churn\", \"actual_churn\"],\n",
    "            columns=[\"pred_no_churn\", \"pred_churn\"],\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(f\"{split_name}_{k}\", v)\n",
    "\n",
    "        mlflow.log_text(\n",
    "            cm_df.to_string(),\n",
    "            artifact_file=f\"confusion_matrix/{split_name}_churn.txt\",\n",
    "        )\n",
    "\n",
    "        return metrics, cm_df\n",
    "\n",
    "    results = {\n",
    "        \"model\": bgf,\n",
    "        \"train\": X_train,\n",
    "    }\n",
    "\n",
    "    # ======================\n",
    "    # Validation evaluation\n",
    "    # ======================\n",
    "    if X_val is not None and y_val is not None:\n",
    "        results[\"val\"] = evaluate(X_val, y_val, \"val\")\n",
    "\n",
    "    # ======================\n",
    "    # Test evaluation\n",
    "    # ======================\n",
    "    if X_test is not None and y_test is not None:\n",
    "        results[\"test\"] = evaluate(X_test, y_test, \"test\")\n",
    "\n",
    "    # ======================\n",
    "    # MLflow model logging (correct for BG-NBD)\n",
    "    # ======================\n",
    "    mlflow.log_param(\"model_type\", \"BG-NBD\")\n",
    "    mlflow.log_param(\"target_1\", \"p_churn\")\n",
    "    mlflow.log_param(\"target_2\", target_2)\n",
    "    mlflow.log_param(\"threshold\", threshold)\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"BG-NBD\")\n",
    "    log_bg_nbd_model(bgf)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e5ccf",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "451b61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_n_purchase_bg_nbd(\n",
    "    X,\n",
    "    bgf,\n",
    "    t=30\n",
    "):\n",
    "    X[\"n_purchase\"] = bgf.conditional_expected_number_of_purchases_up_to_time(\n",
    "        t=t,\n",
    "        frequency=X[\"frequency\"],\n",
    "        recency=X[\"recency\"],\n",
    "        T=X[\"T\"]\n",
    "    )\n",
    "\n",
    "    X = X.rename(columns={'n_purchase': f'pred_n_purchase_{t}d'})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef7211b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_p_alive_churn_bg_nbd(\n",
    "    X,\n",
    "    bgf\n",
    "):\n",
    "    X[\"p_alive\"] = bgf.conditional_probability_alive(\n",
    "        X[\"frequency\"],\n",
    "        X[\"recency\"],\n",
    "        X[\"T\"]\n",
    "    )\n",
    "\n",
    "    X[\"p_churn\"] = 1 - X[\"p_alive\"]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67fd554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_bg_nbd(\n",
    "    customer_id: str,\n",
    "    horizon_days: int,\n",
    "    summary_df: pd.DataFrame,\n",
    "    model,\n",
    ") -> dict:\n",
    "    X = summary_df.loc[\n",
    "        summary_df[\"customer_id\"] == customer_id\n",
    "    ]\n",
    "\n",
    "    p_alive = (\n",
    "        predict_p_alive_churn_bg_nbd(X, model)[\"p_alive\"]\n",
    "        .iloc[0]\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "    n_purchase = (\n",
    "        predict_n_purchase_bg_nbd(X, model, t=horizon_days)\n",
    "        [f\"pred_n_purchase_{horizon_days}d\"]\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"p_alive\": p_alive,\n",
    "        f\"pred_n_purchase_{horizon_days}d\": n_purchase,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d28447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_users_bg_nbd(\n",
    "    customer_ids: list[str],\n",
    "    horizon_days: int,\n",
    "    summary_df: pd.DataFrame,\n",
    "    model,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    X = summary_df.loc[\n",
    "        summary_df[\"customer_id\"].isin(customer_ids)\n",
    "    ]\n",
    "\n",
    "    p_alive = predict_p_alive_churn_bg_nbd(X, model)['p_alive']\n",
    "    n_purchase = predict_n_purchase_bg_nbd(X, model, t=horizon_days)[f\"pred_n_purchase_{horizon_days}d\"]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"p_alive\": p_alive.round(4),\n",
    "            f\"pred_n_purchase_{horizon_days}d\": n_purchase,\n",
    "        },\n",
    "        index=X.index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f02829",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed9123",
   "metadata": {},
   "source": [
    "### Read Seed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "310a4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(f\"../{SEED_TRANSACTIONS}\")\n",
    "transactions_df = transform_transactions_df(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a4bcd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = pd.read_csv(f\"../{SEED_CUSTOMERS}\")\n",
    "customers_df = transform_customers_df(customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2273aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cut_30d_df = pd.read_csv(BASE_GOLD_DIR / \"clv\" / \"summary_cut_30d_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "84a3f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_cut_30d_df = get_customers_screenshot_summary_from_transactions_df(\n",
    "    transactions_df=transactions_df,\n",
    "    observed_date=CUTOFF_DATE,\n",
    "    column_names=[\"customer_id\", \"transaction_date\", \"amount\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "571747f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_features_cut_30d_df = pd.merge(\n",
    "    summary_cut_30d_df,\n",
    "    rfm_cut_30d_df,\n",
    "    on='customer_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6a9c2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_window_30d_df = transactions_df.loc[\n",
    "    (transactions_df[\"transaction_date\"] > CUTOFF_DATE) &\n",
    "    (transactions_df[\"transaction_date\"] <= OBSERVED_DATE)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ce74ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_clv_30d_df = (\n",
    "    transactions_window_30d_df\n",
    "    .groupby(\"customer_id\", as_index=False)\n",
    "    .agg(CLV_30d=(\"amount\", \"sum\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "714a0484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>T</th>\n",
       "      <th>recency</th>\n",
       "      <th>period_total_amount</th>\n",
       "      <th>period_first_transaction_date</th>\n",
       "      <th>period_last_transaction_date</th>\n",
       "      <th>period_transaction_count</th>\n",
       "      <th>days_until_observed</th>\n",
       "      <th>period_tenure_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>7</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "      <td>1001.05</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>15</td>\n",
       "      <td>259</td>\n",
       "      <td>255</td>\n",
       "      <td>975.54</td>\n",
       "      <td>2025-03-17</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00002</td>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>37</td>\n",
       "      <td>910.64</td>\n",
       "      <td>2025-08-20</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00003</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>114.71</td>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>2025-11-18</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00004</td>\n",
       "      <td>18</td>\n",
       "      <td>176</td>\n",
       "      <td>98</td>\n",
       "      <td>2018.94</td>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>2025-09-14</td>\n",
       "      <td>19</td>\n",
       "      <td>78</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>C02995</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>231.28</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>C02996</td>\n",
       "      <td>8</td>\n",
       "      <td>164</td>\n",
       "      <td>161</td>\n",
       "      <td>638.36</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>C02997</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>116.89</td>\n",
       "      <td>2025-11-02</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>C02998</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>545.58</td>\n",
       "      <td>2025-10-26</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>C02999</td>\n",
       "      <td>60</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>2319.34</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2830 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  frequency    T  recency  period_total_amount  \\\n",
       "0         C00000          7   82       78              1001.05   \n",
       "1         C00001         15  259      255               975.54   \n",
       "2         C00002         10  103       37               910.64   \n",
       "3         C00003          3   58       45               114.71   \n",
       "4         C00004         18  176       98              2018.94   \n",
       "...          ...        ...  ...      ...                  ...   \n",
       "2825      C02995          8   39       36               231.28   \n",
       "2826      C02996          8  164      161               638.36   \n",
       "2827      C02997          7   29       26               116.89   \n",
       "2828      C02998         10   36       36               545.58   \n",
       "2829      C02999         60  189      189              2319.34   \n",
       "\n",
       "     period_first_transaction_date period_last_transaction_date  \\\n",
       "0                       2025-09-10                   2025-11-27   \n",
       "1                       2025-03-17                   2025-11-27   \n",
       "2                       2025-08-20                   2025-09-26   \n",
       "3                       2025-10-04                   2025-11-18   \n",
       "4                       2025-06-08                   2025-09-14   \n",
       "...                            ...                          ...   \n",
       "2825                    2025-10-23                   2025-11-28   \n",
       "2826                    2025-06-20                   2025-11-28   \n",
       "2827                    2025-11-02                   2025-11-28   \n",
       "2828                    2025-10-26                   2025-12-01   \n",
       "2829                    2025-05-26                   2025-12-01   \n",
       "\n",
       "      period_transaction_count  days_until_observed  period_tenure_days  \n",
       "0                            8                    4                  78  \n",
       "1                           16                    4                 255  \n",
       "2                           11                   66                  37  \n",
       "3                            4                   13                  45  \n",
       "4                           19                   78                  98  \n",
       "...                        ...                  ...                 ...  \n",
       "2825                         9                    3                  36  \n",
       "2826                         9                    3                 161  \n",
       "2827                         8                    3                  26  \n",
       "2828                        11                    0                  36  \n",
       "2829                        61                    0                 189  \n",
       "\n",
       "[2830 rows x 10 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clv_features_cut_30d_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23cd76",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5fa2bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_features_cut_30d_df = clv_features_cut_30d_df.merge(\n",
    "    truth_clv_30d_df,\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fc389cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test\n",
    ") = split_train_test_val(\n",
    "    customers_modeling_df=clv_features_cut_30d_df,\n",
    "    targets=['CLV_30d'],\n",
    "    test_size=0.33,\n",
    "    val_size=0.33,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32ef06",
   "metadata": {},
   "source": [
    "# Approach 1 - BG-NBD + Gamma–Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87f553",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2b5ca",
   "metadata": {},
   "source": [
    "I already trained and logged a model on 30 days cut off data, so I will just call the model instead of repeating the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360a34d",
   "metadata": {},
   "source": [
    "### BG-NBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20c2c458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfd378da6804e02b048ac149419ce3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bgf, metadata = load_prod_bg_nbd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bedaa",
   "metadata": {},
   "source": [
    "### Gamma-Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "39edc32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifetimes.GammaGammaFitter: fitted with 763 subjects, p: 3.18, q: 0.27, v: 3.18>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggf = GammaGammaFitter(penalizer_coef=0.01)\n",
    "\n",
    "summary_gg = X_train[X_train[\"period_total_amount\"] > 0]\n",
    "\n",
    "ggf.fit(\n",
    "    summary_gg[\"period_transaction_count\"],\n",
    "    summary_gg[\"period_total_amount\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d373222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[summary_gg.index, \"expected_avg_order_value\"] = (\n",
    "    ggf.conditional_expected_average_profit(\n",
    "        summary_gg[\"period_transaction_count\"],\n",
    "        summary_gg[\"period_total_amount\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4ea48ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    X_train[\"frequency\"],\n",
    "    X_train[\"recency\"],\n",
    "    X_train[\"T\"],\n",
    "    X_train[\"period_total_amount\"],\n",
    "    time=30,\n",
    "    discount_rate=0.0,\n",
    "    freq=\"D\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1b99af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    X_test[\"frequency\"],\n",
    "    X_test[\"recency\"],\n",
    "    X_test[\"T\"],\n",
    "    X_test[\"period_total_amount\"],\n",
    "    time=30,\n",
    "    discount_rate=0.0,\n",
    "    freq=\"D\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6aae5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    X_val[\"frequency\"],\n",
    "    X_val[\"recency\"],\n",
    "    X_val[\"T\"],\n",
    "    X_val[\"period_total_amount\"],\n",
    "    time=30,\n",
    "    discount_rate=0.0,\n",
    "    freq=\"D\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f34f1",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b358f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs = {\n",
    "    'train': [X_train, y_train],\n",
    "    'test': [X_test, y_test],\n",
    "    'val': [X_val, y_val]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e63ee66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for train set:\n",
      "52095.543989646816\n",
      "Evaluation results for test set:\n",
      "56282.077407443365\n",
      "Evaluation results for val set:\n",
      "62091.24040247358\n"
     ]
    }
   ],
   "source": [
    "results_df = {}\n",
    "for split in split_dfs.keys():\n",
    "\n",
    "    print(f'Evaluation results for {split} set:')\n",
    "\n",
    "    X = split_dfs[split][0]\n",
    "    y = split_dfs[split][1]\n",
    "\n",
    "\n",
    "    mae = (y[\"CLV_30d\"] - X[\"pred_CLV_30d\"]).abs().mean()\n",
    "\n",
    "    results_df[split] = mae\n",
    "\n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a8802",
   "metadata": {},
   "source": [
    "That seems like a pretty big deviation. I will benchmark it with LGBM some time later."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "maipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
