{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62021e59",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654be9e",
   "metadata": {},
   "source": [
    "**Task**\n",
    "Future value estimation based on customer lifetime.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "*Approach 1 – BG-NBD + Gamma–Gamma*\n",
    "- Fit BG-NBD. Predict:\n",
    "    - Expected number of future transactions\n",
    "    - Probability customer is alive\n",
    "- Fit Gamma–Gamma. Compute:\n",
    "    - Expected monetary value\n",
    "    - CLV over time horizon T\n",
    "\n",
    "*Approach 2 – Survival Analysis + Gamma–Gamma*\n",
    "- Use survival model output. Predict:\n",
    "    - Survival curve\n",
    "    - Expected remaining lifetime\n",
    "- Fit Gamma–Gamma. Compute:\n",
    "    - Time-dependent CLV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0921b2",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5870bc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2e0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70127628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10768444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa43e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c04a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.transforms import (\n",
    "    transform_transactions_df,\n",
    "    transform_customers_df,\n",
    "    get_customers_screenshot_summary_from_transactions_df,\n",
    "    add_churn_status\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba6fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9b536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifetimes import GammaGammaFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e8fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabe917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "094d09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb126e79",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "765a74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd9822b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_CUSTOMERS=os.getenv(\"SEED_CUSTOMERS\")\n",
    "SEED_TRANSACTIONS=os.getenv(\"SEED_TRANSACTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bf7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVED_DATE = pd.Timestamp('2025-12-31')\n",
    "OBSERVED_DATE_STR = OBSERVED_DATE.strftime(\"%d_%m_%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33b99326",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE = OBSERVED_DATE - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e5e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c82669",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold\" / OBSERVED_DATE_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7ef761b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLRUNS_DIR = PROJECT_ROOT / \"mlruns\"\n",
    "mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR}\")\n",
    "EXPERIMENT_NAME = \"customer_monetary_modeling\"\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9b710ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=['is_churn_30_days']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db33a1",
   "metadata": {},
   "source": [
    "## 02 Notebook Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bd39f",
   "metadata": {},
   "source": [
    "I'm temporarily copying the functions from 02 because I haven't put them in the Python package yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1033e63",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f60032b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfm_window_features(customers_df, transactions_df, observed_date):\n",
    "\n",
    "    rfm_time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    for rfm_time_window in rfm_time_windows:\n",
    "\n",
    "        if rfm_time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(rfm_time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "        # Get a Customers Screenshot Summary DataFrame. It has RFM features and other variables that RFM features depend on.\n",
    "        summary_modeling_df = get_customers_screenshot_summary_from_transactions_df(\n",
    "            transactions_df=filtered_transactions_df,\n",
    "            observed_date=observed_date,\n",
    "            column_names=[\"customer_id\", \"transaction_date\", \"amount\"]\n",
    "        )\n",
    "\n",
    "        # Keep only customer_id and the RFM columns we care about\n",
    "        summary_modeling_df = summary_modeling_df[[\n",
    "            'customer_id',\n",
    "            'days_until_observed',\n",
    "            'period_transaction_count',\n",
    "            'period_total_amount',\n",
    "            'period_tenure_days'\n",
    "        ]]\n",
    "\n",
    "        # Rename columns in the summary DF, not the main DF\n",
    "        summary_modeling_df = summary_modeling_df.rename(columns={\n",
    "            'days_until_observed': f'rfm_recency_{rfm_time_window}',\n",
    "            'period_transaction_count': f'rfm_frequency_{rfm_time_window}',\n",
    "            'period_total_amount': f'rfm_monetary_{rfm_time_window}',\n",
    "            'period_tenure_days': f'tenure_{rfm_time_window}'\n",
    "        })\n",
    "        \n",
    "        # Merge with current data used for modelling.\n",
    "        customers_df = pd.merge(\n",
    "            customers_df,\n",
    "            summary_modeling_df,\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38636fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_features(customers_df, transactions_df, observed_date, feature_list):\n",
    "\n",
    "    time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    for time_window in time_windows:\n",
    "\n",
    "        if time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "    customers_list = filtered_transactions_df['customer_id'].unique()\n",
    "\n",
    "    slopes = {}\n",
    "\n",
    "    for customer_id in customers_list:\n",
    "\n",
    "        customer_transactions = filtered_transactions_df[filtered_transactions_df['customer_id'] == customer_id]\n",
    "\n",
    "        x = np.arange(len(customer_transactions)) #time axis\n",
    "        slopes[customer_id] = {} #initiate value list\n",
    "\n",
    "        for feature_name in feature_list:\n",
    "            y = customer_transactions[feature_name].values\n",
    "            x_valid = x[~np.isnan(y)]\n",
    "            y_valid = y[~np.isnan(y)]\n",
    "\n",
    "            if len(y_valid) < 2:\n",
    "                slopes[customer_id][feature_name] = np.nan\n",
    "            else:\n",
    "                slope = np.polyfit(x_valid, y_valid, 1)[0]\n",
    "                slopes[customer_id][feature_name] = slope\n",
    "\n",
    "    # Convert dict of dicts into dataframe\n",
    "    slope_features_df = pd.DataFrame.from_dict(slopes, orient='index')\n",
    "\n",
    "    # Rename columns to have slope_ prefix\n",
    "    slope_features_df = slope_features_df.rename(columns={f: f'slope_{f}' for f in slope_features_df.columns})\n",
    "\n",
    "    # Reset index to have customer_id as a column\n",
    "    slope_features_df = slope_features_df.reset_index().rename(columns={'index': 'customer_id'})\n",
    "\n",
    "    # Merge with current data used for modelling.\n",
    "    customers_df = pd.merge(\n",
    "        customers_df,\n",
    "        slope_features_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bdc84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_statistics_features(customers_df, transactions_df, observed_date, feature_list):\n",
    "\n",
    "    time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    all_stats_df_list = []\n",
    "\n",
    "    for time_window in time_windows:\n",
    "\n",
    "        if time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "        customers_list = filtered_transactions_df['customer_id'].unique()\n",
    "        stats_dict = {}\n",
    "\n",
    "        for customer_id in customers_list:\n",
    "\n",
    "            customer_transactions = filtered_transactions_df[\n",
    "                filtered_transactions_df['customer_id'] == customer_id\n",
    "            ]\n",
    "\n",
    "            stats_dict[customer_id] = {}\n",
    "\n",
    "            for feature_name in feature_list:\n",
    "\n",
    "                y = customer_transactions[feature_name].dropna().values\n",
    "\n",
    "                if len(y) < 2:\n",
    "                    # Less than 2 observations -> return NaN for all stats\n",
    "                    stats_dict[customer_id][f\"min_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"mean_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"mode_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"max_{feature_name}\"] = np.nan\n",
    "                    for q in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]:\n",
    "                        stats_dict[customer_id][f\"q{q}_{feature_name}\"] = np.nan\n",
    "                    continue\n",
    "\n",
    "                # Compute stats\n",
    "                stats_dict[customer_id][f\"min_{feature_name}\"] = np.min(y)\n",
    "                stats_dict[customer_id][f\"mean_{feature_name}\"] = np.mean(y)\n",
    "\n",
    "                # Compute mode safely\n",
    "                mode_result = stats.mode(y, nan_policy='omit')\n",
    "                if hasattr(mode_result.mode, \"__len__\"):\n",
    "                    # old SciPy: mode is array\n",
    "                    mode_val = mode_result.mode[0] if len(mode_result.mode) > 0 else np.nan\n",
    "                else:\n",
    "                    # new SciPy: mode is scalar\n",
    "                    mode_val = mode_result.mode if mode_result.count > 0 else np.nan\n",
    "\n",
    "                stats_dict[customer_id][f\"mode_{feature_name}\"] = mode_val\n",
    "\n",
    "                stats_dict[customer_id][f\"max_{feature_name}\"] = np.max(y)\n",
    "\n",
    "                # Quantiles\n",
    "                for q in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]:\n",
    "                    stats_dict[customer_id][f\"q{q}_{feature_name}\"] = np.percentile(y, q)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        stats_df = pd.DataFrame.from_dict(stats_dict, orient='index').reset_index().rename(columns={'index': 'customer_id'})\n",
    "        all_stats_df_list.append(stats_df)\n",
    "\n",
    "    # Merge with customers_df (only keep last time_window stats)\n",
    "    final_stats_df = all_stats_df_list[-1]  # or merge all windows if needed\n",
    "    customers_df = pd.merge(customers_df, final_stats_df, on='customer_id', how='left')\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2c6e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_base(\n",
    "    seed_customers_path,\n",
    "    seed_transactions_path,\n",
    "    train_snapshot_date,\n",
    "    churn_windows=[30, 60, 90],\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads raw data, transforms it, limits it to modeling window,\n",
    "    builds customer modeling table, and adds churn labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Read data ---\n",
    "    customers_df = pd.read_csv(seed_customers_path)\n",
    "    transactions_df = pd.read_csv(seed_transactions_path)\n",
    "\n",
    "    # --- Transform data ---\n",
    "    transactions_df = transform_transactions_df(transactions_df)\n",
    "    customers_df = transform_customers_df(customers_df)\n",
    "\n",
    "    # --- Derive MAX_DATA_DATE internally ---\n",
    "    max_data_date = transactions_df[\"transaction_date\"].max()\n",
    "\n",
    "    # --- Limit transactions to snapshot ---\n",
    "    transactions_modeling_df = transactions_df.loc[\n",
    "        transactions_df[\"transaction_date\"] <= train_snapshot_date\n",
    "    ]\n",
    "\n",
    "    # --- Build customer modeling base ---\n",
    "    customers_modeling_df = (\n",
    "        pd.DataFrame({\n",
    "            \"customer_id\": transactions_modeling_df[\"customer_id\"].unique()\n",
    "        })\n",
    "        .merge(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "        .drop(columns=[\"signup_date\", \"true_lifetime_days\", \"termination_date\"])\n",
    "    )\n",
    "\n",
    "    # --- Add churn labels ---\n",
    "    for nday in churn_windows:\n",
    "        var_name = f\"is_churn_{nday}_days\"\n",
    "        observed_date = max_data_date - pd.Timedelta(days=nday)\n",
    "\n",
    "        customers_modeling_df = add_churn_status(\n",
    "            transformed_customers_df=customers_df,\n",
    "            observed_date=observed_date,\n",
    "            desired_df=None,\n",
    "        )\n",
    "        \n",
    "        customers_modeling_df = customers_modeling_df.rename(columns={'is_churn': var_name})\n",
    "\n",
    "    return transactions_modeling_df, customers_modeling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff1e08",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8822165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_in_df_cols(df):\n",
    "    # Get relative percentage of nulls by column\n",
    "    null_features_proportion = (\n",
    "        df.isna().sum() / len(df)\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    high_proportion = []\n",
    "    medium_proportion = []\n",
    "    low_proportion = []\n",
    "\n",
    "    for feature, proportion in null_features_proportion.items():\n",
    "        if proportion >= 0.20:\n",
    "            high_proportion.append(feature)\n",
    "        elif 0.05 <= proportion < 0.20:\n",
    "            medium_proportion.append(feature)\n",
    "        else:\n",
    "            low_proportion.append(feature)\n",
    "\n",
    "    # Build features DataFrame\n",
    "    features_df = null_features_proportion.reset_index()\n",
    "    features_df.columns = [\"feature\", \"nan_proportion\"]\n",
    "\n",
    "    features_df[\"NaN group\"] = features_df[\"feature\"].apply(\n",
    "        lambda f: (\n",
    "            \"High\" if f in high_proportion\n",
    "            else \"Medium\" if f in medium_proportion\n",
    "            else \"Low\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Print counts (same behavior as before)\n",
    "    print(\"Total features:\", len(df.columns))\n",
    "    print(\"Information on NaN values\")\n",
    "    print(\"====================================\")\n",
    "    print(\"Number of High Proportion Features:\", len(high_proportion))\n",
    "    print(\"Number of Medium Proportion Features:\", len(medium_proportion))\n",
    "    print(\"Number of Low Proportion Features:\", len(low_proportion))\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ab32c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_X_csv(X_by_target, BASE_GOLD_DIR):\n",
    "\n",
    "    for target in X_by_target.keys():\n",
    "\n",
    "        target_dir = BASE_GOLD_DIR / target\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_by_target[target].to_csv(\n",
    "            target_dir / \"X_train.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "        print(f\"[{target}] written to {target_dir}\")\n",
    "    \n",
    "    return \"All data saved successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6212924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_y_csv(\n",
    "        X_by_target,\n",
    "        y,\n",
    "        BASE_GOLD_DIR\n",
    "    ):\n",
    "\n",
    "    for target in targets:\n",
    "        target_dir = BASE_GOLD_DIR / target\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ----------------------------\n",
    "        # TRAIN labels\n",
    "        # ----------------------------\n",
    "        y.loc[\n",
    "            X_by_target[target].index, target\n",
    "        ].to_csv(\n",
    "            target_dir / \"y_train.csv\",\n",
    "            header=True,\n",
    "        )\n",
    "    \n",
    "    return \"All data saved successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c276364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_features_csv(df, split, base_gold_dir, index_name='customer_id'):\n",
    "    \n",
    "    path = Path(base_gold_dir) / \"raw\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"WRITING TO:\", path.resolve())\n",
    "\n",
    "    df.index.name = index_name\n",
    "    df.to_csv(\n",
    "        path / f\"{split}_features.csv\",\n",
    "        index=True, # keep customer_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f5a82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_by_target_csv(X_by_target, split, base_gold_dir, index_name='customer_id'):\n",
    "\n",
    "    for target, df in X_by_target.items():\n",
    "        \n",
    "        base_path = Path(base_gold_dir) / \"transformed\" / target\n",
    "        base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df.index.name = index_name\n",
    "        df.to_csv(\n",
    "            base_path / f\"X_{split}.csv\",\n",
    "            index=True,  # keep customer_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "946c00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformed(BASE_GOLD_DIR, split, target):\n",
    "    return pd.read_csv(\n",
    "        BASE_GOLD_DIR / \"transformed\" / target / f\"X_{split}.csv\",\n",
    "        index_col=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219282e",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0c52815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_feature_selection(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    target,\n",
    "    cutoff=0.0,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform mutual information–based feature selection for a given target.\n",
    "\n",
    "    Returns:\n",
    "        selected_df: DataFrame with selected features\n",
    "        mi_scores: DataFrame with MI scores per feature\n",
    "        selected_features: Index of selected feature names\n",
    "    \"\"\"\n",
    "\n",
    "    assert X_train.index.equals(y_train.index)\n",
    "\n",
    "    mi_train = mutual_info_classif(\n",
    "        X_train,\n",
    "        y_train[target],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    mi_scores = (\n",
    "        pd.DataFrame(\n",
    "            mi_train,\n",
    "            index=X_train.columns,\n",
    "            columns=[\"mutual_info\"]\n",
    "        )\n",
    "        .sort_values(by=\"mutual_info\", ascending=False)\n",
    "    )\n",
    "\n",
    "    selected_features = mi_scores.loc[\n",
    "        mi_scores[\"mutual_info\"] > cutoff\n",
    "    ].index\n",
    "\n",
    "    selected_df = X_train[selected_features]\n",
    "\n",
    "    return selected_df, mi_scores, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc92a7",
   "metadata": {},
   "source": [
    "### Feature Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d072a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_time_features(transactions_df):\n",
    "    \"\"\"\n",
    "    Add time-based and order-based transaction features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions_df : pd.DataFrame\n",
    "        Must contain: customer_id, transaction_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of transactions_df with added features\n",
    "    \"\"\"\n",
    "\n",
    "    df = transactions_df.sort_values(\n",
    "        [\"customer_id\", \"transaction_date\"]\n",
    "    ).copy()\n",
    "\n",
    "    df[\"customer_transaction_order\"] = (\n",
    "        df.groupby(\"customer_id\").cumcount()\n",
    "    )\n",
    "\n",
    "    df[\"prev_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"].shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"next_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"].shift(-1)\n",
    "    )\n",
    "\n",
    "    df[\"days_since_previous_transaction\"] = (\n",
    "        df[\"transaction_date\"] - df[\"prev_transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    df[\"days_until_next_transaction\"] = (\n",
    "        df[\"next_transaction_date\"] - df[\"transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    df[\"first_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"]\n",
    "        .transform(\"min\")\n",
    "    )\n",
    "\n",
    "    df[\"days_since_first_transaction\"] = (\n",
    "        df[\"transaction_date\"] - df[\"first_transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca8a37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_customer_features(\n",
    "    transactions_modeling_df,\n",
    "    customers_modeling_df,\n",
    "    observed_date,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Build raw customer-level features from transactions and customers data.\n",
    "    No imputing, scaling, or selection is performed here.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Transaction-level features\n",
    "    transactions_df = add_transaction_time_features(\n",
    "        transactions_modeling_df\n",
    "    )\n",
    "\n",
    "    # 2. RFM window features\n",
    "    customers_df = get_rfm_window_features(\n",
    "        customers_df=customers_modeling_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "    )\n",
    "\n",
    "    # 3. Activity trend (slopes)\n",
    "    customers_df = get_slope_features(\n",
    "        customers_df=customers_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # 4. Transaction statistics\n",
    "    customers_df = get_transaction_statistics_features(\n",
    "        customers_df=customers_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a887e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_numeric_transformers(\n",
    "    X_train_numeric_df,\n",
    "    imputer_params=None,\n",
    "    scaler_params=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit numeric imputer and scaler on training data only.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_numeric_imputed_scaled_df : pd.DataFrame\n",
    "    numeric_imputer : fitted IterativeImputer\n",
    "    scaler : fitted StandardScaler\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Defaults\n",
    "    # -------------------------------\n",
    "    if imputer_params is None:\n",
    "        imputer_params = dict(\n",
    "            estimator=LinearRegression(),\n",
    "            max_iter=20,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    if scaler_params is None:\n",
    "        scaler_params = {}\n",
    "\n",
    "    # -------------------------------\n",
    "    # Imputation (FIT)\n",
    "    # -------------------------------\n",
    "    numeric_imputer = IterativeImputer(**imputer_params)\n",
    "    X_train_numeric_imputed = numeric_imputer.fit_transform(X_train_numeric_df)\n",
    "\n",
    "    X_train_numeric_imputed_df = pd.DataFrame(\n",
    "        X_train_numeric_imputed,\n",
    "        columns=X_train_numeric_df.columns,\n",
    "        index=X_train_numeric_df.index,\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Scaling (FIT)\n",
    "    # -------------------------------\n",
    "    scaler = StandardScaler(**scaler_params)\n",
    "    X_train_numeric_imputed_scaled = scaler.fit_transform(\n",
    "        X_train_numeric_imputed_df\n",
    "    )\n",
    "\n",
    "    X_train_numeric_imputed_scaled_df = pd.DataFrame(\n",
    "        X_train_numeric_imputed_scaled,\n",
    "        columns=X_train_numeric_df.columns,\n",
    "        index=X_train_numeric_df.index,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "503b7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_customers_numeric_features(\n",
    "    X_numeric,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply fitted numeric imputer and scaler.\n",
    "    \"\"\"\n",
    "\n",
    "    X_numeric_imputed = numeric_imputer.transform(X_numeric)\n",
    "    X_numeric_imputed_df = pd.DataFrame(\n",
    "        X_numeric_imputed,\n",
    "        columns=X_numeric.columns,\n",
    "        index=X_numeric.index,\n",
    "    )\n",
    "\n",
    "    X_numeric_scaled = scaler.transform(X_numeric_imputed_df)\n",
    "    X_numeric_scaled_df = pd.DataFrame(\n",
    "        X_numeric_scaled,\n",
    "        columns=X_numeric.columns,\n",
    "        index=X_numeric.index,\n",
    "    )\n",
    "\n",
    "    return X_numeric_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9468412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_per_target(\n",
    "    X_train_transformed_df,\n",
    "    y_train,\n",
    "    targets,\n",
    "    artifact_dir=None,\n",
    "    cutoff=0.0,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform feature selection per target using mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    assert X_train_transformed_df.index.equals(y_train.index), (\n",
    "        \"X_train and y_train must be index-aligned\"\n",
    "    )\n",
    "\n",
    "    X_train_by_target = {}\n",
    "    selected_features_by_target = {}\n",
    "    mi_scores_by_target = {}\n",
    "\n",
    "    for target in targets:\n",
    "        X_selected_df, mi_scores, selected_features = (\n",
    "            mutual_information_feature_selection(\n",
    "                X_train=X_train_transformed_df,\n",
    "                y_train=y_train,\n",
    "                target=target,\n",
    "                cutoff=cutoff,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if artifact_dir is not None:\n",
    "            with open(\n",
    "                artifact_dir / f\"selected_features_{target}.json\",\n",
    "                \"w\",\n",
    "            ) as f:\n",
    "                json.dump(list(selected_features), f)\n",
    "\n",
    "        X_train_by_target[target] = X_selected_df\n",
    "        selected_features_by_target[target] = list(selected_features)\n",
    "        mi_scores_by_target[target] = mi_scores\n",
    "\n",
    "        print(f\"[{target}] selected {len(selected_features)} features\")\n",
    "\n",
    "    return (\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "691f4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_per_target(\n",
    "    X_transformed_df,\n",
    "    selected_features_by_target\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform feature selection per target using mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    for target, selected_features in selected_features_by_target.items():\n",
    "\n",
    "        missing_features = set(selected_features) - set(\n",
    "            X_transformed_df.columns\n",
    "        )\n",
    "        if missing_features:\n",
    "            raise ValueError(\n",
    "                f\"Missing selected features at inference time: {missing_features}\"\n",
    "            )\n",
    "\n",
    "        X_selected_features = X_transformed_df[selected_features]\n",
    "        X_by_target[target] = X_selected_features\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23bf7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(\n",
    "    customers_modeling_df,\n",
    "    targets,\n",
    "    test_size=0.33,\n",
    "    val_size=0.33,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split customer modeling dataframe into train / val / test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    customers_modeling_df : pd.DataFrame\n",
    "        Must contain customer_id and target columns.\n",
    "    targets : list[str]\n",
    "        Target column names.\n",
    "    test_size : float\n",
    "        Proportion of data used for test+val split.\n",
    "    val_size : float\n",
    "        Proportion of test split used for validation.\n",
    "    random_state : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Feature / target separation\n",
    "    # -------------------------------\n",
    "    X_df = customers_modeling_df.drop(columns=targets)\n",
    "    X_df = X_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    y_df = customers_modeling_df[[\"customer_id\"] + targets]\n",
    "    y_df = y_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Train / temp split\n",
    "    # -------------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_df,\n",
    "        y_df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Test / validation split\n",
    "    # -------------------------------\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "880df847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_customer_features_pipeline_train(\n",
    "    transactions_modeling_df,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    observed_date,\n",
    "    targets,\n",
    "    ARTIFACT_DIR=None,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for TRAIN data.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Build raw customer features\n",
    "    # --------------------------------------------------\n",
    "    X_train_raw_features_df = build_customer_features(\n",
    "        transactions_modeling_df=transactions_modeling_df,\n",
    "        customers_modeling_df=X_train,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Numeric transform (impute + scale)\n",
    "    # --------------------------------------------------\n",
    "    X_train_raw_features_df = X_train_raw_features_df.set_index(\"customer_id\", drop=False)\n",
    "    X_train_raw_features_numeric_df = X_train_raw_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    numeric_imputer, scaler = fit_numeric_transformers(\n",
    "        X_train_raw_features_numeric_df,\n",
    "        imputer_params=None,\n",
    "        scaler_params=None,\n",
    "    )\n",
    "\n",
    "    X_train_transformed_df = transform_customers_numeric_features(\n",
    "        X_train_raw_features_numeric_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Feature selection per target (EXTRACTED)\n",
    "    # --------------------------------------------------\n",
    "    (\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "    ) = select_features_per_target(\n",
    "        X_train_transformed_df=X_train_transformed_df,\n",
    "        y_train=y_train,\n",
    "        targets=targets,\n",
    "        artifact_dir=ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Save transformers ONCE\n",
    "    # --------------------------------------------------\n",
    "    if ARTIFACT_DIR is not None:\n",
    "        joblib.dump(\n",
    "            numeric_imputer,\n",
    "            ARTIFACT_DIR / \"numeric_imputer.joblib\",\n",
    "        )\n",
    "        joblib.dump(\n",
    "            scaler,\n",
    "            ARTIFACT_DIR / \"scaler.joblib\",\n",
    "        )\n",
    "\n",
    "    X_train_raw_features_df = X_train_raw_features_df.drop(columns=['customer_id'])\n",
    "\n",
    "    return (\n",
    "        X_train_raw_features_df,\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f713e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_customer_features_pipeline_test(\n",
    "    transactions_modeling_df,\n",
    "    X_test,\n",
    "    observed_date,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for TEST / VAL / INFERENCE data.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Build raw customer-level features from transactions\n",
    "    2. Remove customer_id from feature space\n",
    "    3. Apply fitted numeric transformations (imputer + scaler)\n",
    "    4. Select precomputed feature subset (STRICT reuse)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Build raw customer features\n",
    "    # --------------------------------------------------\n",
    "    X_test_features_df = build_customer_features(\n",
    "        transactions_modeling_df=transactions_modeling_df,\n",
    "        customers_modeling_df=X_test,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Set customer_id as index and REMOVE from features\n",
    "    # --------------------------------------------------\n",
    "    if \"customer_id\" not in X_test_features_df.columns:\n",
    "        raise ValueError(\"customer_id column missing after feature building\")\n",
    "\n",
    "    X_test_features_df = X_test_features_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Select numeric features and enforce column order\n",
    "    # --------------------------------------------------\n",
    "    X_test_numeric_features_df = X_test_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Enforce training-time column order (critical for IterativeImputer)\n",
    "    X_test_numeric_features_df = X_test_numeric_features_df[\n",
    "        numeric_imputer.feature_names_in_\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Apply fitted numeric transformations (NO FIT)\n",
    "    # --------------------------------------------------\n",
    "    X_test_numeric_features_transformed_df = transform_customers_numeric_features(\n",
    "        X_test_numeric_features_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Feature selection (STRICT reuse)\n",
    "    # --------------------------------------------------\n",
    "    missing_features = set(selected_features) - set(\n",
    "        X_test_numeric_features_transformed_df.columns\n",
    "    )\n",
    "    if missing_features:\n",
    "        raise ValueError(\n",
    "            f\"Missing selected features at inference time: {missing_features}\"\n",
    "        )\n",
    "\n",
    "    X_test_final_df = X_test_numeric_features_transformed_df[selected_features]\n",
    "\n",
    "    return X_test_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6284c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_select_for_multiple_targets_test(\n",
    "    X_test_raw_features_df,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features_by_target\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and transform customer features for multiple targets\n",
    "    (test / val / inference).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_by_target : dict[str, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    # Select numeric features and enforce column order\n",
    "    X_test_numeric_features_df = X_test_raw_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Enforce training-time column order (critical for IterativeImputer)\n",
    "    X_test_numeric_features_df = X_test_numeric_features_df[\n",
    "        numeric_imputer.feature_names_in_\n",
    "    ]\n",
    "\n",
    "    X_test_transformed_df = transform_customers_numeric_features(\n",
    "        X_test_numeric_features_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    X_by_target = get_features_per_target(\n",
    "        X_test_transformed_df,\n",
    "        selected_features_by_target\n",
    "    )\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8756f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_for_multiple_targets(\n",
    "    transactions_modeling_df,\n",
    "    X_df,\n",
    "    observed_date,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features_by_target,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and transform customer features for multiple targets\n",
    "    (test / val / inference).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_by_target : dict[str, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    for target, selected_features in selected_features_by_target.items():\n",
    "        X_by_target[target] = build_and_transform_customer_features_pipeline_test(\n",
    "            transactions_modeling_df=transactions_modeling_df,\n",
    "            X_test=X_df,\n",
    "            observed_date=observed_date,\n",
    "            numeric_imputer=numeric_imputer,\n",
    "            scaler=scaler,\n",
    "            selected_features=selected_features,\n",
    "            feature_list=[\n",
    "                \"amount\",\n",
    "                \"days_since_previous_transaction\",\n",
    "                \"days_until_next_transaction\",\n",
    "                \"customer_transaction_order\",\n",
    "                \"days_since_first_transaction\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac9f62",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83301a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lgb_feature_importance(\n",
    "    model,\n",
    "    importance_type=\"gain\",   # \"gain\" or \"split\"\n",
    "    normalize=False,\n",
    "    top_n=None,\n",
    "    title=None,\n",
    "    height=600,\n",
    "    as_percent=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot LightGBM feature importance for sklearn API models.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract feature names ---\n",
    "    if hasattr(model, \"feature_name_\"):\n",
    "        features = model.feature_name_\n",
    "    else:\n",
    "        raise ValueError(\"Model does not contain feature names\")\n",
    "\n",
    "    # --- Extract importance correctly ---\n",
    "    if importance_type == \"split\":\n",
    "        importance = model.feature_importances_\n",
    "    elif importance_type == \"gain\":\n",
    "        importance = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "    else:\n",
    "        raise ValueError(\"importance_type must be 'gain' or 'split'\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"importance\": importance\n",
    "    })\n",
    "\n",
    "    # Remove zero-importance features\n",
    "    df = df[df[\"importance\"] > 0]\n",
    "\n",
    "    # --- Normalize if requested ---\n",
    "    if normalize:\n",
    "        total = df[\"importance\"].sum()\n",
    "        df[\"importance\"] = df[\"importance\"] / total\n",
    "        if as_percent:\n",
    "            df[\"importance\"] *= 100\n",
    "            importance_label = \"Normalized Gain (%)\"\n",
    "            text_fmt = \".2f\"\n",
    "        else:\n",
    "            importance_label = \"Normalized Gain\"\n",
    "            text_fmt = \".4f\"\n",
    "    else:\n",
    "        importance_label = (\n",
    "            \"Gain\" if importance_type == \"gain\" else \"Split Count\"\n",
    "        )\n",
    "        text_fmt = \".2f\"\n",
    "\n",
    "    # Sort and keep top N\n",
    "    df = df.sort_values(\"importance\", ascending=False)\n",
    "    if top_n is not None:\n",
    "        df = df.head(top_n)\n",
    "\n",
    "    # Reverse for horizontal bar chart\n",
    "    df = df.sort_values(\"importance\", ascending=True)\n",
    "\n",
    "    if title is None:\n",
    "        norm_tag = \" (Normalized)\" if normalize else \"\"\n",
    "        title = f\"LightGBM Feature Importance ({importance_type.capitalize()}){norm_tag}\"\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x=\"importance\",\n",
    "        y=\"feature\",\n",
    "        orientation=\"h\",\n",
    "        title=title,\n",
    "        labels={\n",
    "            \"importance\": importance_label,\n",
    "            \"feature\": \"Feature\"\n",
    "        },\n",
    "        text=df[\"importance\"]\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        texttemplate=f\"%{{text:{text_fmt}}}\",\n",
    "        textposition=\"outside\",\n",
    "        cliponaxis=False\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        yaxis=dict(categoryorder=\"total ascending\"),\n",
    "        margin=dict(r=120)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab481200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_model(model, X, y, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a binary classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    y_proba = model.predict(X, num_iteration=model.best_iteration_)\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(y, y_proba),\n",
    "        \"pr_auc\": average_precision_score(y, y_proba),\n",
    "        \"confusion_matrix\": confusion_matrix(y, y_pred)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "109be13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_styled_df_confusion_matrix(cm):\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"Actual 0\", \"Actual 1\"],\n",
    "        columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "    )\n",
    "\n",
    "    styled_df = (\n",
    "        cm_df.style\n",
    "        .background_gradient(cmap=\"Blues\")\n",
    "        .format(\"{:.0f}\")\n",
    "    )\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b273cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, X_val, y_val, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a binary classifier on train, validation, and test sets.\n",
    "    Prints:\n",
    "    - ROC-AUC\n",
    "    - PR-AUC (Precision–Recall)\n",
    "    - Accuracy\n",
    "    - Confusion Matrix\n",
    "    - Classification Report\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "\n",
    "    for split_name, X, y in [\n",
    "        (\"TRAIN\", X_train, y_train),\n",
    "        (\"TEST\", X_test, y_test),\n",
    "        (\"VALIDATION\", X_val, y_val),\n",
    "    ]:\n",
    "        # Predicted probabilities and labels\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Metrics\n",
    "        roc_auc = roc_auc_score(y, y_proba)\n",
    "        pr_auc = average_precision_score(y, y_proba)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[\"Actual 0\", \"Actual 1\"],\n",
    "            columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{split_name}\")\n",
    "        print(\"-\" * len(split_name))\n",
    "        print(f\"ROC-AUC:      {roc_auc:.4f}\")\n",
    "        print(f\"PR-AUC:       {pr_auc:.4f}\")\n",
    "        print(f\"Accuracy:     {acc:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm_df)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf9a032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    target,\n",
    "    dataset_version,\n",
    "):\n",
    "    param_grid = {\n",
    "        \"num_leaves\": [31, 63],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [-1, 6],\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=3,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train[target])\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # ---------- Validation predictions ----------\n",
    "    val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    val_pred = (val_proba >= 0.5).astype(int)  # explicit threshold\n",
    "\n",
    "    # ---------- Metrics ----------\n",
    "    roc_auc = roc_auc_score(y_val[target], val_proba)\n",
    "    pr_auc = average_precision_score(y_val[target], val_proba)\n",
    "    precision = precision_score(y_val[target], val_pred)\n",
    "    recall = recall_score(y_val[target], val_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val[target], val_pred)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_0\", \"actual_1\"],\n",
    "        columns=[\"pred_0\", \"pred_1\"],\n",
    "    )\n",
    "\n",
    "    # ---------- MLflow ----------\n",
    "    input_example = X_train.iloc[:5]\n",
    "    signature = infer_signature(\n",
    "        X_train,\n",
    "        best_model.predict_proba(X_train)[:, 1],\n",
    "    )\n",
    "\n",
    "    mlflow.log_param(\"dataset_version\", dataset_version)\n",
    "    mlflow.log_param(\"target\", target)\n",
    "    mlflow.log_params(grid.best_params_)\n",
    "\n",
    "    mlflow.log_metric(\"val_roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"val_pr_auc\", pr_auc)\n",
    "    mlflow.log_metric(\"val_precision\", precision)\n",
    "    mlflow.log_metric(\"val_recall\", recall)\n",
    "\n",
    "    mlflow.log_text(\n",
    "        cm_df.to_string(),\n",
    "        artifact_file=f\"confusion_matrix/{dataset_version}_{target}.txt\",\n",
    "    )\n",
    "\n",
    "    mlflow.lightgbm.log_model(\n",
    "        best_model,\n",
    "        name=f\"{dataset_version}_{target}\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac68fcc",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9db082bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promote_to_production(run_id):\n",
    "    client.set_tag(run_id, \"stage\", \"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ea8902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_runs():\n",
    "    return mlflow.search_runs(\n",
    "        filter_string=\"tags.stage = 'production'\",\n",
    "        search_all_experiments=True,\n",
    "        output_format=\"pandas\",\n",
    "    )\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4da9eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_production_models():\n",
    "    prod_runs = get_production_runs()\n",
    "\n",
    "    models = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for _, row in prod_runs.iterrows():\n",
    "        target = row[\"params.target\"]\n",
    "        dataset_version = row[\"params.dataset_version\"]\n",
    "        run_id = row[\"run_id\"]\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{dataset_version}_{target}\"\n",
    "        model = mlflow.lightgbm.load_model(model_uri)\n",
    "\n",
    "        models[target] = model\n",
    "        metadata[target] = {\n",
    "            \"dataset_version\": dataset_version,\n",
    "            \"run_id\": run_id,\n",
    "        }\n",
    "\n",
    "    return models, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cdc6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prod_bg_nbd():\n",
    "    exp = mlflow.get_experiment_by_name(\"bg-nbd\")\n",
    "    if exp is None:\n",
    "        raise ValueError(\"Experiment 'bg-nbd' not found\")\n",
    "\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=\"tags.stage = 'production'\",\n",
    "        output_format=\"pandas\",\n",
    "    )\n",
    "\n",
    "    if runs.empty:\n",
    "        raise ValueError(\"No production BG-NBD run found\")\n",
    "\n",
    "    run = runs.iloc[0]\n",
    "    run_id = run[\"run_id\"]\n",
    "\n",
    "    metadata = {\n",
    "        \"run_id\": run_id,\n",
    "        \"experiment_id\": exp.experiment_id,\n",
    "        \"experiment_name\": exp.name,\n",
    "        \"params\": run.filter(like=\"params.\").to_dict(),\n",
    "        \"metrics\": run.filter(like=\"metrics.\").to_dict(),\n",
    "        \"tags\": run.filter(like=\"tags.\").to_dict(),\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        path = mlflow.artifacts.download_artifacts(\n",
    "            run_id=run_id,\n",
    "            artifact_path=\"bg_nbd_model/bg_nbd.pkl\",\n",
    "            dst_path=d,\n",
    "        )\n",
    "        model = cloudpickle.load(open(path, \"rb\"))\n",
    "\n",
    "    return model, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75ee8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_features(\n",
    "    customer_ids,\n",
    "    target,\n",
    "    metadata,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "):\n",
    "    if isinstance(customer_ids, str):\n",
    "        customer_ids = [customer_ids]\n",
    "\n",
    "    dataset_version = metadata[target][\"dataset_version\"]\n",
    "\n",
    "    if dataset_version == \"raw\":\n",
    "        X = raw_features_df.loc[customer_ids]\n",
    "    elif dataset_version == \"transformed\":\n",
    "        X = transformed_features_by_target[target].loc[customer_ids]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset version: {dataset_version}\")\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e54855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn(\n",
    "    customer_id: str,\n",
    "    horizon_days: int,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "    models,\n",
    "    metadata,\n",
    "):\n",
    "    # ------------------\n",
    "    # Validate horizon\n",
    "    # ------------------\n",
    "    if horizon_days not in {30, 60, 90}:\n",
    "        raise ValueError(\"horizon_days must be one of {30, 60, 90}\")\n",
    "\n",
    "    target = f\"is_churn_{horizon_days}_days\"\n",
    "\n",
    "    if target not in models:\n",
    "        raise KeyError(f\"No production model loaded for target: {target}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Select features\n",
    "    # ------------------\n",
    "    X = get_customer_features(\n",
    "        customer_ids=[customer_id],\n",
    "        target=target,\n",
    "        metadata=metadata,\n",
    "        raw_features_df=raw_features_df,\n",
    "        transformed_features_by_target=transformed_features_by_target,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Predict\n",
    "    # ------------------\n",
    "    model = models[target]\n",
    "    churn_prob = float(model.predict_proba(X)[0, 1])\n",
    "\n",
    "    # ------------------\n",
    "    # Risk labeling (explicit, adjustable)\n",
    "    # ------------------\n",
    "    if churn_prob >= 0.7:\n",
    "        churn_label = \"high_risk\"\n",
    "    elif churn_prob >= 0.4:\n",
    "        churn_label = \"medium_risk\"\n",
    "    else:\n",
    "        churn_label = \"low_risk\"\n",
    "\n",
    "    return {\n",
    "        \"churn_probability\": round(churn_prob, 4),\n",
    "        \"churn_label\": churn_label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3d02de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churns(\n",
    "    customer_ids: list[str],\n",
    "    horizon_days: int,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "    models,\n",
    "    metadata,\n",
    "):\n",
    "    # ------------------\n",
    "    # Validate horizon\n",
    "    # ------------------\n",
    "    if horizon_days not in {30, 60, 90}:\n",
    "        raise ValueError(\"horizon_days must be one of {30, 60, 90}\")\n",
    "\n",
    "    target = f\"is_churn_{horizon_days}_days\"\n",
    "\n",
    "    if target not in models:\n",
    "        raise KeyError(f\"No production model loaded for target: {target}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Feature extraction (BULK)\n",
    "    # ------------------\n",
    "    X = get_customer_features(\n",
    "        customer_ids=customer_ids,\n",
    "        target=target,\n",
    "        metadata=metadata,\n",
    "        raw_features_df=raw_features_df,\n",
    "        transformed_features_by_target=transformed_features_by_target,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Predict (BULK)\n",
    "    # ------------------\n",
    "    model = models[target]\n",
    "    churn_probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # ------------------\n",
    "    # Risk labeling (vectorized)\n",
    "    # ------------------\n",
    "    churn_labels = np.where(\n",
    "        churn_probs >= 0.7,\n",
    "        \"high_risk\",\n",
    "        np.where(\n",
    "            churn_probs >= 0.4,\n",
    "            \"medium_risk\",\n",
    "            \"low_risk\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Output (aligned, explicit)\n",
    "    # ------------------\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"customer_id\": customer_ids,\n",
    "                \"churn_probability\": churn_probs.round(4),\n",
    "                \"churn_label\": churn_labels,\n",
    "            }\n",
    "        )\n",
    "        .set_index(\"customer_id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23f043bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(\n",
    "        dataset_version,\n",
    "        gold_data_version,\n",
    "        gold_dir=\"default\",\n",
    "        targets=targets\n",
    "    ):\n",
    "    '''\n",
    "        The service preloads the feature dataframes for faster search.\n",
    "    '''\n",
    "    if gold_dir == \"default\":\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        gold_dir = PROJECT_ROOT / \"data\" / \"gold\" / gold_data_version\n",
    "    \n",
    "    if dataset_version == \"raw\":\n",
    "        feature_df = pd.read_csv(gold_dir / dataset_version / \"all_features.csv\", index_col=0)\n",
    "        return feature_df\n",
    "    elif dataset_version == \"transformed\":\n",
    "        X_by_target = {}\n",
    "        for target in targets:\n",
    "            feature_df = pd.read_csv(gold_dir / dataset_version / target / \"X_all.csv\", index_col=0)\n",
    "            X_by_target[target] = feature_df\n",
    "        return X_by_target\n",
    "    else:\n",
    "        return \"Invalid dataset version. Please use only `raw` and `transformed`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8bcbd",
   "metadata": {},
   "source": [
    "## 03 Notebook Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebc01d",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af6fa6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cut_30d_features(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    customers_df: pd.DataFrame,\n",
    "    n_days: int,\n",
    "    observed_date\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds 30-day cutoff features for BG-NBD modeling.\n",
    "\n",
    "    Returns:\n",
    "    - features_df: customer-level modeling table\n",
    "    - transactions_cut_df: truncated transactions\n",
    "    \"\"\"\n",
    "\n",
    "    cutoff_date = observed_date - pd.Timedelta(n_days, unit='d')\n",
    "\n",
    "    (\n",
    "        transactions_cut_df,\n",
    "        customer_ids_cut_df\n",
    "    ) = cut_off_customers_and_transactions_df(\n",
    "        transactions_df=transactions_df,\n",
    "        customers_df=customers_df,\n",
    "        n_days=n_days,\n",
    "        end_date=observed_date\n",
    "    )\n",
    "\n",
    "    summary_cut_df = get_lifetimes_summary_df(\n",
    "        transactions_df=transactions_cut_df,\n",
    "        observed_date=cutoff_date,\n",
    "        column_names=[\"customer_id\", \"transaction_date\"]\n",
    "    )\n",
    "\n",
    "    summary_cut_df = add_churn_status(\n",
    "        transformed_customers_df=customer_ids_cut_df,\n",
    "        observed_date=cutoff_date,\n",
    "        desired_df=summary_cut_df,\n",
    "    )\n",
    "\n",
    "    summary_cut_df = summary_cut_df.drop(columns=[\"termination_date\"])\n",
    "\n",
    "    customer_ids_cut_df = customer_ids_cut_df.set_index(\"customer_id\")\n",
    "\n",
    "    customer_ids_cut_df[\"n_purchase_30d\"] = get_truth_number_of_purchases(\n",
    "        transactions_df,\n",
    "        customer_ids_cut_df.reset_index(),\n",
    "        n_days,\n",
    "        observed_date\n",
    "    )\n",
    "\n",
    "    customer_ids_cut_df = customer_ids_cut_df.reset_index()\n",
    "\n",
    "    features_df = pd.merge(\n",
    "        customer_ids_cut_df,\n",
    "        summary_cut_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cea68e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lifetimes_summary_df(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    observed_date: pd.Timestamp,\n",
    "    column_names: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a per-customer snapshot summary from a transactions DataFrame.\n",
    "\n",
    "    The function filters transactions up to the observed date and computes,\n",
    "    per customer:\n",
    "        - first transaction date in the period\n",
    "        - last transaction date in the period\n",
    "        - number of transactions in the period\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions_df : pd.DataFrame\n",
    "        Input transactions data.\n",
    "    observed_date : pd.Timestamp\n",
    "        Cutoff date for the snapshot.\n",
    "    column_names : list\n",
    "        Column names in the following order:\n",
    "        [customer_id, transaction_date]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Customer-level snapshot summary.\n",
    "    \"\"\"\n",
    "\n",
    "    customer_col, transaction_date_col = column_names\n",
    "\n",
    "    filtered_df = transactions_df[\n",
    "        transactions_df[transaction_date_col] <= observed_date\n",
    "    ]\n",
    "\n",
    "    summary_df = (\n",
    "        filtered_df\n",
    "        .groupby(customer_col, as_index=False)\n",
    "        .agg(\n",
    "            period_first_transaction_date=(transaction_date_col, 'min'),\n",
    "            period_last_transaction_date=(transaction_date_col, 'max'),\n",
    "            frequency=(customer_col, 'size')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    summary_df['frequency'] = summary_df['frequency'] - 1\n",
    "\n",
    "\n",
    "    summary_df['T'] = (\n",
    "        observed_date - summary_df['period_first_transaction_date']\n",
    "    ).dt.days\n",
    "\n",
    "    summary_df['recency'] = (\n",
    "        summary_df['period_last_transaction_date']\n",
    "        -\n",
    "        summary_df['period_first_transaction_date']\n",
    "    ).dt.days\n",
    "\n",
    "    return summary_df[[customer_col, 'frequency', 'T', 'recency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52d555da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lifetimes_summary_arrays(summary_df):\n",
    "\n",
    "    n = summary_df.shape[0]\n",
    "    x = summary_df[\"frequency\"].to_numpy()\n",
    "    t_x = summary_df[\"recency\"].to_numpy()\n",
    "    T = summary_df[\"T\"].to_numpy()\n",
    "\n",
    "    return (n, x, t_x, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f76a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_off_customers_and_transactions_df(\n",
    "    transactions_df,\n",
    "    customers_df,\n",
    "    n_days=30,\n",
    "    end_date=pd.Timestamp | None\n",
    "):\n",
    "    if end_date==None:\n",
    "        end_date = transactions_df['transaction_date'].max()\n",
    "    \n",
    "    cutoff_date = end_date - pd.Timedelta(days=n_days)\n",
    "    \n",
    "    transactions_cut_df = transactions_df[\n",
    "        (transactions_df[\"transaction_date\"] <= cutoff_date)\n",
    "    ]\n",
    "\n",
    "    customer_ids_cut = customers_df[\n",
    "        customers_df['customer_id'].isin(transactions_cut_df['customer_id'])\n",
    "    ].reset_index(drop=True)\n",
    "    \n",
    "    return transactions_cut_df, customer_ids_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cb6f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgf_clv_features_df(\n",
    "    transactions_path: str,\n",
    "    customers_path: str,\n",
    "    observed_date,\n",
    "    cutoff_days: int,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame | None, pd.DataFrame, pd.Index]:\n",
    "    \"\"\"\n",
    "    Load raw data, apply transformations, cut data by inactivity window,\n",
    "    build BG-NBD + RFM features, and optionally compute future CLV ground truth.\n",
    "\n",
    "    If cutoff_days == 0, future CLV ground truth is skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    # ======================\n",
    "    # Load & transform raw data\n",
    "    # ======================\n",
    "    transactions_df = pd.read_csv(transactions_path)\n",
    "    transactions_df = transform_transactions_df(transactions_df)\n",
    "\n",
    "    customers_df = pd.read_csv(customers_path)\n",
    "    customers_df = transform_customers_df(customers_df)\n",
    "\n",
    "    # ======================\n",
    "    # Define cutoff date\n",
    "    # ======================\n",
    "    cutoff_date = observed_date - pd.Timedelta(days=cutoff_days)\n",
    "\n",
    "    # ======================\n",
    "    # Cut customers & transactions\n",
    "    # ======================\n",
    "    transactions_cut_df, customer_ids_cut = cut_off_customers_and_transactions_df(\n",
    "        transactions_df=transactions_df,\n",
    "        customers_df=customers_df,\n",
    "        n_days=cutoff_days,\n",
    "        end_date=observed_date,\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # Lifetimes (BG-NBD) summary\n",
    "    # ======================\n",
    "    summary_cut_df = get_lifetimes_summary_df(\n",
    "        transactions_df=transactions_cut_df,\n",
    "        observed_date=cutoff_date,\n",
    "        column_names=[\"customer_id\", \"transaction_date\"],\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # RFM features\n",
    "    # ======================\n",
    "    rfm_cut_df = get_customers_screenshot_summary_from_transactions_df(\n",
    "        transactions_df=transactions_cut_df,\n",
    "        observed_date=cutoff_date,\n",
    "        column_names=[\"customer_id\", \"transaction_date\", \"amount\"],\n",
    "    )\n",
    "\n",
    "    summary_rfm_features_cut_df = summary_cut_df.merge(\n",
    "        rfm_cut_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # Optional: Future CLV ground truth\n",
    "    # ======================\n",
    "    if cutoff_days > 0:\n",
    "        transactions_future_df = transactions_df.loc[\n",
    "            (transactions_df[\"transaction_date\"] > cutoff_date) &\n",
    "            (transactions_df[\"transaction_date\"] <= observed_date)\n",
    "        ]\n",
    "\n",
    "        truth_clv_df = (\n",
    "            transactions_future_df\n",
    "            .groupby(\"customer_id\", as_index=False)\n",
    "            .agg(CLV_30d=(\"amount\", \"sum\"))\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        truth_clv_df = None\n",
    "\n",
    "    clv_features_cut_df = summary_rfm_features_cut_df.copy()\n",
    "\n",
    "    return (\n",
    "        clv_features_cut_df,\n",
    "        truth_clv_df,\n",
    "        transactions_cut_df,\n",
    "        customer_ids_cut,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b1930",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fbca34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_churn_predictions(\n",
    "    feature_df: pd.DataFrame,\n",
    "    target_df: pd.DataFrame | None = None,\n",
    "    threshold: float = 0.5,\n",
    "    target_col: str = \"is_churn\",\n",
    "    proba_col: str = \"p_churn\",\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_df: must contain [proba_col]\n",
    "    target_df:\n",
    "        - if provided: must contain [target_col]\n",
    "        - if None: target_col must already be in feature_df\n",
    "\n",
    "    Assumes customer_id is the index in both DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decide where labels come from\n",
    "    if target_df is None:\n",
    "        missing = {target_col, proba_col} - set(feature_df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(\n",
    "                f\"feature_df is missing required columns: {missing}\"\n",
    "            )\n",
    "        eval_df = feature_df.copy()\n",
    "\n",
    "    else:\n",
    "        missing_f = {proba_col} - set(feature_df.columns)\n",
    "        missing_t = {target_col} - set(target_df.columns)\n",
    "\n",
    "        if missing_f or missing_t:\n",
    "            raise ValueError(\n",
    "                f\"Missing columns — \"\n",
    "                f\"feature_df: {missing_f}, target_df: {missing_t}\"\n",
    "            )\n",
    "\n",
    "        # Index-based alignment\n",
    "        eval_df = feature_df.join(\n",
    "            target_df[[target_col]],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Binary predictions\n",
    "    eval_df[\"pred_churn\"] = (eval_df[proba_col] >= threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        eval_df[target_col],\n",
    "        eval_df[\"pred_churn\"],\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_no_churn\", \"actual_churn\"],\n",
    "        columns=[\"pred_no_churn\", \"pred_churn\"]\n",
    "    )\n",
    "\n",
    "    return metrics, cm_df, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63a94e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truth_number_of_purchases(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    customer_cut_ids: pd.DataFrame,\n",
    "    n_days: int,\n",
    "    end_date=pd.Timestamp | None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the number of purchases per customer in the last `n_days`\n",
    "    ending at the max transaction_date.\n",
    "    \"\"\"\n",
    "\n",
    "    if end_date==None:\n",
    "        end_date = transactions_df['transaction_date'].max()\n",
    "    \n",
    "    cutoff_date = end_date - pd.Timedelta(days=n_days)\n",
    "    \n",
    "    # Filter transactions to relevant customers\n",
    "    sel_transactions_df = transactions_df[\n",
    "        transactions_df[\"customer_id\"].isin(customer_cut_ids[\"customer_id\"])\n",
    "    ]\n",
    "\n",
    "    # Keep only transactions in the future window\n",
    "    sel_transactions_df = sel_transactions_df[\n",
    "        sel_transactions_df[\"transaction_date\"] > cutoff_date\n",
    "    ]\n",
    "\n",
    "    # Count purchases per customer\n",
    "    purchase_counts = (\n",
    "        sel_transactions_df\n",
    "        .groupby(\"customer_id\")\n",
    "        .size()\n",
    "        .reindex(customer_cut_ids[\"customer_id\"], fill_value=0)\n",
    "    )\n",
    "\n",
    "    return purchase_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37084076",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e290d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bg_nbd_model(bgf):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        path = Path(tmpdir) / \"bg_nbd.pkl\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            cloudpickle.dump(bgf, f)\n",
    "\n",
    "        mlflow.log_artifact(path, artifact_path=\"bg_nbd_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ac32acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bg_nbd(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame | None = None,\n",
    "    X_val: pd.DataFrame | None = None,\n",
    "    y_val: pd.DataFrame | None = None,\n",
    "    X_test: pd.DataFrame | None = None,\n",
    "    y_test: pd.DataFrame | None = None,\n",
    "    target_2: str = \"n_purchase_30d\",\n",
    "    horizon_days=30,\n",
    "    threshold: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a BG-NBD model and evaluates churn when labels are available.\n",
    "\n",
    "    Assumptions:\n",
    "    - X_* index = customer_id\n",
    "    - X_* contain ['frequency', 'recency', 'T']\n",
    "    - y_* contain ['is_churn'] and share index with X_*\n",
    "    \"\"\"\n",
    "\n",
    "    pred_2 = f'pred_{target_2}'\n",
    "\n",
    "    # ======================\n",
    "    # Train BG-NBD\n",
    "    # ======================\n",
    "    bgf = BetaGeoFitter()\n",
    "    bgf.fit(\n",
    "        frequency=X_train[\"frequency\"],\n",
    "        recency=X_train[\"recency\"],\n",
    "        T=X_train[\"T\"],\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # Add predictions (IN PLACE)\n",
    "    # ======================\n",
    "    X_train = predict_p_alive_churn_bg_nbd(X_train, bgf)\n",
    "    X_train = predict_n_purchase_bg_nbd(X_train, bgf, t=horizon_days)\n",
    "\n",
    "    if X_val is not None:\n",
    "        X_val = predict_p_alive_churn_bg_nbd(X_val, bgf)\n",
    "        X_val = predict_n_purchase_bg_nbd(X_val, bgf, t=horizon_days)\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test = predict_p_alive_churn_bg_nbd(X_test, bgf)\n",
    "        X_test = predict_n_purchase_bg_nbd(X_test, bgf, t=horizon_days)\n",
    "\n",
    "    # ======================\n",
    "    # Evaluation helper (inline, no copies)\n",
    "    # ======================\n",
    "    def evaluate(X, y, split_name):\n",
    "        eval_df = X.join(y, how=\"inner\")\n",
    "\n",
    "        eval_df.loc[:, \"pred_churn\"] = (\n",
    "            eval_df[\"p_churn\"] >= threshold\n",
    "        ).astype(int)\n",
    "\n",
    "        mae = (\n",
    "            eval_df[target_2] - eval_df[pred_2]\n",
    "        ).abs().mean()\n",
    "\n",
    "        metrics = {\n",
    "            \"roc_auc\": roc_auc_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"p_churn\"]\n",
    "            ),\n",
    "            \"pr_auc\": average_precision_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"p_churn\"]\n",
    "            ),\n",
    "            \"precision\": precision_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"pred_churn\"]\n",
    "            ),\n",
    "            \"recall\": recall_score(\n",
    "                eval_df[\"is_churn\"], eval_df[\"pred_churn\"]\n",
    "            ),\n",
    "            \"mae\": mae,\n",
    "        }\n",
    "\n",
    "        cm = confusion_matrix(\n",
    "            eval_df[\"is_churn\"],\n",
    "            eval_df[\"pred_churn\"],\n",
    "        )\n",
    "\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[\"actual_no_churn\", \"actual_churn\"],\n",
    "            columns=[\"pred_no_churn\", \"pred_churn\"],\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(f\"{split_name}_{k}\", v)\n",
    "\n",
    "        mlflow.log_text(\n",
    "            cm_df.to_string(),\n",
    "            artifact_file=f\"confusion_matrix/{split_name}_churn.txt\",\n",
    "        )\n",
    "\n",
    "        return metrics, cm_df\n",
    "\n",
    "    results = {\n",
    "        \"model\": bgf,\n",
    "        \"train\": X_train,\n",
    "    }\n",
    "\n",
    "    # ======================\n",
    "    # Validation evaluation\n",
    "    # ======================\n",
    "    if X_val is not None and y_val is not None:\n",
    "        results[\"val\"] = evaluate(X_val, y_val, \"val\")\n",
    "\n",
    "    # ======================\n",
    "    # Test evaluation\n",
    "    # ======================\n",
    "    if X_test is not None and y_test is not None:\n",
    "        results[\"test\"] = evaluate(X_test, y_test, \"test\")\n",
    "\n",
    "    # ======================\n",
    "    # MLflow model logging (correct for BG-NBD)\n",
    "    # ======================\n",
    "    mlflow.log_param(\"model_type\", \"BG-NBD\")\n",
    "    mlflow.log_param(\"target_1\", \"p_churn\")\n",
    "    mlflow.log_param(\"target_2\", target_2)\n",
    "    mlflow.log_param(\"threshold\", threshold)\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"BG-NBD\")\n",
    "    log_bg_nbd_model(bgf)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e5ccf",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "451b61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_n_purchase_bg_nbd(\n",
    "    X,\n",
    "    bgf,\n",
    "    t=30\n",
    "):\n",
    "    X[\"n_purchase\"] = bgf.conditional_expected_number_of_purchases_up_to_time(\n",
    "        t=t,\n",
    "        frequency=X[\"frequency\"],\n",
    "        recency=X[\"recency\"],\n",
    "        T=X[\"T\"]\n",
    "    )\n",
    "\n",
    "    X = X.rename(columns={'n_purchase': f'pred_n_purchase_{t}d'})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef7211b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_p_alive_churn_bg_nbd(\n",
    "    X,\n",
    "    bgf\n",
    "):\n",
    "    X[\"p_alive\"] = bgf.conditional_probability_alive(\n",
    "        X[\"frequency\"],\n",
    "        X[\"recency\"],\n",
    "        X[\"T\"]\n",
    "    )\n",
    "\n",
    "    X[\"p_churn\"] = 1 - X[\"p_alive\"]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67fd554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_bg_nbd(\n",
    "    customer_id: str,\n",
    "    horizon_days: int,\n",
    "    summary_df: pd.DataFrame,\n",
    "    model,\n",
    ") -> dict:\n",
    "    X = summary_df.loc[\n",
    "        summary_df[\"customer_id\"] == customer_id\n",
    "    ]\n",
    "\n",
    "    p_alive = (\n",
    "        predict_p_alive_churn_bg_nbd(X, model)[\"p_alive\"]\n",
    "        .iloc[0]\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "    n_purchase = (\n",
    "        predict_n_purchase_bg_nbd(X, model, t=horizon_days)\n",
    "        [f\"pred_n_purchase_{horizon_days}d\"]\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"p_alive\": p_alive,\n",
    "        f\"pred_n_purchase_{horizon_days}d\": n_purchase,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d28447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_users_bg_nbd(\n",
    "    customer_ids: list[str],\n",
    "    horizon_days: int,\n",
    "    summary_df: pd.DataFrame,\n",
    "    model,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    X = summary_df.loc[\n",
    "        summary_df[\"customer_id\"].isin(customer_ids)\n",
    "    ]\n",
    "\n",
    "    p_alive = predict_p_alive_churn_bg_nbd(X, model)['p_alive']\n",
    "    n_purchase = predict_n_purchase_bg_nbd(X, model, t=horizon_days)[f\"pred_n_purchase_{horizon_days}d\"]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"p_alive\": p_alive.round(4),\n",
    "            f\"pred_n_purchase_{horizon_days}d\": n_purchase,\n",
    "        },\n",
    "        index=X.index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568a625",
   "metadata": {},
   "source": [
    "## 04 Notebook Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9f9e8",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9fc766ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_duration_event(\n",
    "    customers_df: pd.DataFrame,\n",
    "    obs_end_date: pd.Timestamp,\n",
    "    start_col: str = \"signup_date\",\n",
    "    termination_col: str = \"termination_date\",\n",
    ") -> pd.DataFrame:\n",
    "    df = customers_df.copy()\n",
    "\n",
    "    # Ensure datetime\n",
    "    df[start_col] = pd.to_datetime(df[start_col])\n",
    "    df[termination_col] = pd.to_datetime(df[termination_col])\n",
    "\n",
    "    # Event indicator: churn happened by obs_end_date\n",
    "    df[\"E\"] = (\n",
    "        df[termination_col].notna()\n",
    "        & (df[termination_col] <= obs_end_date)\n",
    "    ).astype(int)\n",
    "\n",
    "    # End date for duration calculation\n",
    "    df[\"end_date\"] = df[termination_col].where(\n",
    "        df[\"E\"] == 1,\n",
    "        obs_end_date,\n",
    "    )\n",
    "\n",
    "    # Duration (in days)\n",
    "    df[\"T\"] = (df[\"end_date\"] - df[start_col]).dt.days\n",
    "\n",
    "    # Safety checks\n",
    "    if (df[\"T\"] < 0).any():\n",
    "        raise ValueError(\"Negative durations found — check date logic\")\n",
    "\n",
    "    return df.drop(columns=[\"end_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "287003ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_to_churn_proba(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    horizon_days: int,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns P(churn within horizon_days)\n",
    "    \"\"\"\n",
    "\n",
    "    surv_fn = model.predict_survival_function(X)\n",
    "\n",
    "    probs = []\n",
    "    for i in range(surv_fn.shape[1]):\n",
    "        s = surv_fn.iloc[:, i]\n",
    "        s_h = (\n",
    "            s.loc[s.index <= horizon_days].iloc[-1]\n",
    "            if (s.index <= horizon_days).any()\n",
    "            else s.iloc[0]\n",
    "        )\n",
    "        probs.append(1 - s_h)\n",
    "\n",
    "    return pd.Series(probs, index=X.index, name=\"p_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "567eaebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_survival_dataset(\n",
    "    seed_customers_path: str,\n",
    "    seed_transactions_path: str,\n",
    "    cutoff_date,\n",
    "    max_data_date,\n",
    "    churn_windows=[30],\n",
    "):\n",
    "    \"\"\"\n",
    "    Build survival analysis dataset from customer & transaction CSV paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    survival_df : pd.DataFrame\n",
    "        Feature matrix indexed by customer_id, including duration & event columns\n",
    "    label_df : pd.DataFrame\n",
    "        Churn labels indexed by customer_id\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load raw data ---\n",
    "    customers_df = pd.read_csv(seed_customers_path)\n",
    "    transactions_df = pd.read_csv(seed_transactions_path)\n",
    "\n",
    "    # --- Build modeling base ---\n",
    "    transactions_modeling_df, customers_modeling_df = build_training_base(\n",
    "        seed_customers_path=seed_customers_path,\n",
    "        seed_transactions_path=seed_transactions_path,\n",
    "        train_snapshot_date=cutoff_date,\n",
    "        churn_windows=list(churn_windows),\n",
    "    )\n",
    "\n",
    "    # --- Feature engineering ---\n",
    "    raw_features_df = build_customer_features(\n",
    "        transactions_modeling_df,\n",
    "        customers_modeling_df,\n",
    "        observed_date=cutoff_date,\n",
    "    )\n",
    "\n",
    "    # --- Add survival targets (T, E) ---\n",
    "    survival_df = add_duration_event(\n",
    "        customers_df=raw_features_df,\n",
    "        obs_end_date=max_data_date,\n",
    "        start_col=\"signup_date\",\n",
    "        termination_col=\"termination_date\",\n",
    "    )\n",
    "\n",
    "    # --- Extract labels ---\n",
    "    label_cols = [f\"is_churn_{w}_days\" for w in churn_windows]\n",
    "    label_df = survival_df[[\"customer_id\", *label_cols]].set_index(\"customer_id\")\n",
    "\n",
    "    # --- Drop target / leakage columns ---\n",
    "    survival_df = survival_df.drop(\n",
    "        columns=[\n",
    "            \"signup_date\",\n",
    "            \"termination_date\",\n",
    "            \"true_lifetime_days\",\n",
    "            *label_cols,\n",
    "        ]\n",
    "    ).set_index(\"customer_id\")\n",
    "\n",
    "    return survival_df, label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "decf19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalFeaturePipeline(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Feature transformation pipeline for survival analysis.\n",
    "\n",
    "    Steps:\n",
    "    1. Select feature columns (exclude T, E)\n",
    "    2. Median imputation\n",
    "    3. Drop features highly correlated with T\n",
    "    4. Drop near-zero variance features\n",
    "    5. Standard scaling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corr_threshold: float = 0.95,\n",
    "        std_threshold: float = 1e-6,\n",
    "    ):\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.std_threshold = std_threshold\n",
    "\n",
    "        self.imputer = SimpleImputer(strategy=\"median\")\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # learned attributes\n",
    "        self.feature_cols_ = None\n",
    "        self.keep_cols_ = None\n",
    "\n",
    "    def fit(self, raw_survival_df: pd.DataFrame, y=None):\n",
    "        df = raw_survival_df.copy()\n",
    "\n",
    "        # --- identify feature columns ---\n",
    "        self.feature_cols_ = [\n",
    "            c for c in df.columns if c not in {\"customer_id\", \"T\", \"E\"}\n",
    "        ]\n",
    "\n",
    "        # --- imputation (fit only) ---\n",
    "        X = self.imputer.fit_transform(df[self.feature_cols_])\n",
    "        X = pd.DataFrame(X, columns=self.feature_cols_, index=df.index)\n",
    "\n",
    "        # --- correlation with T ---\n",
    "        corr_with_T = X.corrwith(df[\"T\"]).abs()\n",
    "        keep = corr_with_T[corr_with_T < self.corr_threshold].index\n",
    "\n",
    "        # --- variance filter ---\n",
    "        std = X[keep].std()\n",
    "        self.keep_cols_ = std[std > self.std_threshold].index.tolist()\n",
    "\n",
    "        # --- fit scaler ---\n",
    "        self.scaler.fit(X[self.keep_cols_])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_survival_df: pd.DataFrame):\n",
    "        df = raw_survival_df.copy()\n",
    "\n",
    "        X = pd.DataFrame(\n",
    "            self.imputer.transform(df[self.feature_cols_]),\n",
    "            columns=self.feature_cols_,\n",
    "            index=df.index,\n",
    "        )\n",
    "\n",
    "        X = self.scaler.transform(X[self.keep_cols_])\n",
    "        X = pd.DataFrame(X, columns=self.keep_cols_, index=df.index)\n",
    "\n",
    "        return pd.concat(\n",
    "            [X, df[[\"T\", \"E\"]]],\n",
    "            axis=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2f0cf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d8bec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_churn_predictions(\n",
    "    feature_df: pd.DataFrame,\n",
    "    target_df: pd.DataFrame | None = None,\n",
    "    threshold: float = 0.5,\n",
    "    target_col: str = \"is_churn\",\n",
    "    proba_col: str = \"p_churn\",\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_df: must contain [proba_col]\n",
    "    target_df:\n",
    "        - if provided: must contain [target_col]\n",
    "        - if None: target_col must already be in feature_df\n",
    "\n",
    "    Assumes customer_id is the index in both DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decide where labels come from\n",
    "    if target_df is None:\n",
    "        missing = {target_col, proba_col} - set(feature_df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(\n",
    "                f\"feature_df is missing required columns: {missing}\"\n",
    "            )\n",
    "        eval_df = feature_df.copy()\n",
    "\n",
    "    else:\n",
    "        missing_f = {proba_col} - set(feature_df.columns)\n",
    "        missing_t = {target_col} - set(target_df.columns)\n",
    "\n",
    "        if missing_f or missing_t:\n",
    "            raise ValueError(\n",
    "                f\"Missing columns — \"\n",
    "                f\"feature_df: {missing_f}, target_df: {missing_t}\"\n",
    "            )\n",
    "\n",
    "        # Index-based alignment\n",
    "        eval_df = feature_df.join(\n",
    "            target_df[[target_col]],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Binary predictions\n",
    "    eval_df[\"pred_churn\"] = (eval_df[proba_col] >= threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        eval_df[target_col],\n",
    "        eval_df[\"pred_churn\"],\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_no_churn\", \"actual_churn\"],\n",
    "        columns=[\"pred_no_churn\", \"pred_churn\"]\n",
    "    )\n",
    "\n",
    "    return metrics, cm_df, eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c677141",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7546f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cox(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame | None = None,\n",
    "    X_val: pd.DataFrame | None = None,\n",
    "    y_val: pd.DataFrame | None = None,\n",
    "    X_test: pd.DataFrame | None = None,\n",
    "    y_test: pd.DataFrame | None = None,\n",
    "    target: str = \"is_churn_30_days\",\n",
    "    horizon_days: int = 30,\n",
    "    threshold: float = 0.5,\n",
    "    model_params: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a Cox model and evaluates churn when labels are available.\n",
    "    \"\"\"\n",
    "\n",
    "    model_params = model_params or {}\n",
    "    mlflow.log_param(\"model_type\", \"cox\")\n",
    "\n",
    "    try:\n",
    "        # ======================\n",
    "        # Train Cox\n",
    "        # ======================\n",
    "        cph = CoxPHFitter(**model_params)\n",
    "        cph.fit(\n",
    "            X_train,\n",
    "            duration_col=\"T\",\n",
    "            event_col=\"E\",\n",
    "        )\n",
    "\n",
    "        proba_col = f\"p_churn_{horizon_days}_days\"\n",
    "\n",
    "        # ======================\n",
    "        # Add predictions (IN PLACE)\n",
    "        # ======================\n",
    "        X_train[proba_col] = survival_to_churn_proba(\n",
    "            model=cph,\n",
    "            X=X_train,\n",
    "            horizon_days=horizon_days,\n",
    "        )\n",
    "\n",
    "        if X_val is not None:\n",
    "            X_val[proba_col] = survival_to_churn_proba(\n",
    "                model=cph,\n",
    "                X=X_val,\n",
    "                horizon_days=horizon_days,\n",
    "            )\n",
    "\n",
    "        if X_test is not None:\n",
    "            X_test[proba_col] = survival_to_churn_proba(\n",
    "                model=cph,\n",
    "                X=X_test,\n",
    "                horizon_days=horizon_days,\n",
    "            )\n",
    "\n",
    "        # ======================\n",
    "        # Evaluation helper\n",
    "        # ======================\n",
    "        def evaluate(X, y, split_name):\n",
    "            if X is None or y is None:\n",
    "                return None\n",
    "\n",
    "            metrics, cm_df, eval_df = evaluate_churn_predictions(\n",
    "                feature_df=X,\n",
    "                target_df=y,\n",
    "                threshold=threshold,\n",
    "                target_col=target,\n",
    "                proba_col=proba_col,\n",
    "            )\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                mlflow.log_metric(f\"{split_name}_{k}\", v)\n",
    "\n",
    "            mlflow.log_text(\n",
    "                cm_df.to_string(),\n",
    "                artifact_file=f\"confusion_matrix/{split_name}_churn.txt\",\n",
    "            )\n",
    "\n",
    "            return metrics, cm_df\n",
    "\n",
    "        results = {\n",
    "            \"model\": cph,\n",
    "            \"train\": evaluate(X_train, y_train, \"train\"),\n",
    "        }\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            results[\"val\"] = evaluate(X_val, y_val, \"val\")\n",
    "\n",
    "        if X_test is not None and y_test is not None:\n",
    "            results[\"test\"] = evaluate(X_test, y_test, \"test\")\n",
    "\n",
    "        # ======================\n",
    "        # MLflow logging\n",
    "        # ======================\n",
    "        mlflow.log_param(\"target\", target)\n",
    "        mlflow.log_param(\"horizon_days\", horizon_days)\n",
    "        mlflow.log_param(\"threshold\", threshold)\n",
    "\n",
    "        for k, v in model_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        log_lifetimes_model(cph)\n",
    "\n",
    "        mlflow.set_tag(\"run_status\", \"success\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # ======================\n",
    "        # Failure handling\n",
    "        # ======================\n",
    "        error_msg = traceback.format_exc()\n",
    "\n",
    "        mlflow.set_tag(\"run_status\", \"failed\")\n",
    "        mlflow.log_text(error_msg, artifact_file=\"training_error.txt\")\n",
    "\n",
    "        # Still log params so you know what failed\n",
    "        for k, v in model_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        # Optional: surface error in logs, but do NOT crash grid search\n",
    "        print(f\"[WARN] Cox training failed with params={model_params}\")\n",
    "        print(e)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d798279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_weibull(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame | None = None,\n",
    "    X_val: pd.DataFrame | None = None,\n",
    "    y_val: pd.DataFrame | None = None,\n",
    "    X_test: pd.DataFrame | None = None,\n",
    "    y_test: pd.DataFrame | None = None,\n",
    "    target: str = \"is_churn_30_days\",\n",
    "    horizon_days: int = 30,\n",
    "    threshold: float = 0.5,\n",
    "    model_params: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a Weibull AFT model and evaluates churn when labels are available.\n",
    "    \"\"\"\n",
    "\n",
    "    model_params = model_params or {}\n",
    "    mlflow.log_param(\"model_type\", \"weibull\")\n",
    "\n",
    "    try:\n",
    "        # ======================\n",
    "        # Train Weibull\n",
    "        # ======================\n",
    "        model = WeibullAFTFitter(**model_params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            duration_col=\"T\",\n",
    "            event_col=\"E\",\n",
    "        )\n",
    "\n",
    "        proba_col = f\"p_churn_{horizon_days}_days\"\n",
    "\n",
    "        # ======================\n",
    "        # Add predictions (IN PLACE)\n",
    "        # ======================\n",
    "        X_train[proba_col] = survival_to_churn_proba(\n",
    "            model=model,\n",
    "            X=X_train,\n",
    "            horizon_days=horizon_days,\n",
    "        )\n",
    "\n",
    "        if X_val is not None:\n",
    "            X_val[proba_col] = survival_to_churn_proba(\n",
    "                model=model,\n",
    "                X=X_val,\n",
    "                horizon_days=horizon_days,\n",
    "            )\n",
    "\n",
    "        if X_test is not None:\n",
    "            X_test[proba_col] = survival_to_churn_proba(\n",
    "                model=model,\n",
    "                X=X_test,\n",
    "                horizon_days=horizon_days,\n",
    "            )\n",
    "\n",
    "        # ======================\n",
    "        # Evaluation helper\n",
    "        # ======================\n",
    "        def evaluate(X, y, split_name):\n",
    "            if X is None or y is None:\n",
    "                return None\n",
    "\n",
    "            metrics, cm_df, eval_df = evaluate_churn_predictions(\n",
    "                feature_df=X,\n",
    "                target_df=y,\n",
    "                threshold=threshold,\n",
    "                target_col=target,\n",
    "                proba_col=proba_col,\n",
    "            )\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                mlflow.log_metric(f\"{split_name}_{k}\", v)\n",
    "\n",
    "            mlflow.log_text(\n",
    "                cm_df.to_string(),\n",
    "                artifact_file=f\"confusion_matrix/{split_name}_churn.txt\",\n",
    "            )\n",
    "\n",
    "            return metrics, cm_df\n",
    "\n",
    "        results = {\n",
    "            \"model\": model,\n",
    "            \"train\": evaluate(X_train, y_train, \"train\"),\n",
    "        }\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            results[\"val\"] = evaluate(X_val, y_val, \"val\")\n",
    "\n",
    "        if X_test is not None and y_test is not None:\n",
    "            results[\"test\"] = evaluate(X_test, y_test, \"test\")\n",
    "\n",
    "        # ======================\n",
    "        # MLflow logging\n",
    "        # ======================\n",
    "        mlflow.log_param(\"target\", target)\n",
    "        mlflow.log_param(\"horizon_days\", horizon_days)\n",
    "        mlflow.log_param(\"threshold\", threshold)\n",
    "\n",
    "        for k, v in model_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        log_lifetimes_model(model)\n",
    "\n",
    "        mlflow.set_tag(\"run_status\", \"success\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # ======================\n",
    "        # Failure handling\n",
    "        # ======================\n",
    "        error_msg = traceback.format_exc()\n",
    "\n",
    "        mlflow.set_tag(\"run_status\", \"failed\")\n",
    "        mlflow.log_text(error_msg, artifact_file=\"training_error.txt\")\n",
    "\n",
    "        for k, v in model_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        print(f\"[WARN] Weibull training failed with params={model_params}\")\n",
    "        print(e)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e524c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lifetimes_model(model, filename: str = \"model.pkl\"):\n",
    "    tmp_path = Path(filename)\n",
    "\n",
    "    with open(tmp_path, \"wb\") as f:\n",
    "        cloudpickle.dump(model, f)\n",
    "\n",
    "    mlflow.log_artifact(\n",
    "        local_path=str(tmp_path),\n",
    "        artifact_path=\"model\"\n",
    "    )\n",
    "\n",
    "    tmp_path.unlink()  # cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddc70629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_survival(\n",
    "    customer_id: str,\n",
    "    survival_df: pd.DataFrame,\n",
    "    model,\n",
    "    horizons: list[int] = [30, 60, 90],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns survival curve at given horizons + expected remaining lifetime\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract row\n",
    "    row = survival_df.loc[customer_id]\n",
    "\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Customer {customer_id} not found\")\n",
    "\n",
    "    X = row.drop(columns=[\"customer_id\", \"T\", \"E\"], errors=\"ignore\")\n",
    "\n",
    "    # Survival function (continuous)\n",
    "    surv_fn = model.predict_survival_function(X)\n",
    "\n",
    "    # Discrete horizon extraction\n",
    "    survival_curve = []\n",
    "    for h in horizons:\n",
    "        prob = float(\n",
    "            surv_fn.loc[surv_fn.index <= h].iloc[-1, 0]\n",
    "            if (surv_fn.index <= h).any()\n",
    "            else surv_fn.iloc[0, 0]\n",
    "        )\n",
    "        survival_curve.append(\n",
    "            {\"day\": h, \"prob\": round(prob, 4)}\n",
    "        )\n",
    "\n",
    "    # Expected remaining lifetime\n",
    "    expected_lifetime = float(\n",
    "        model.predict_expectation(X).iloc[0]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"survival_curve\": survival_curve,\n",
    "        \"expected_remaining_lifetime\": round(expected_lifetime, 2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca38df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_survival_analysis_model():\n",
    "    exp = mlflow.get_experiment_by_name(\"customer_lifetime_modeling\")\n",
    "    if exp is None:\n",
    "        raise ValueError(f\"Experiment {exp} not found\")\n",
    "\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=\"tags.stage = 'production'\",\n",
    "        output_format=\"pandas\",\n",
    "    )\n",
    "\n",
    "    if runs.empty:\n",
    "        raise ValueError(\"No production run found\")\n",
    "\n",
    "    run = runs.iloc[0]\n",
    "    run_id = run[\"run_id\"]\n",
    "\n",
    "    metadata = {\n",
    "        \"run_id\": run_id,\n",
    "        \"experiment_id\": exp.experiment_id,\n",
    "        \"experiment_name\": exp.name,\n",
    "        \"params\": run.filter(like=\"params.\").to_dict(),\n",
    "        \"metrics\": run.filter(like=\"metrics.\").to_dict(),\n",
    "        \"tags\": run.filter(like=\"tags.\").to_dict(),\n",
    "    }\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "\n",
    "        path = mlflow.artifacts.download_artifacts(\n",
    "            run_id=run_id,\n",
    "            artifact_path=\"model/model.pkl\",\n",
    "            dst_path=d,\n",
    "        )\n",
    "        model = cloudpickle.load(open(path, \"rb\"))\n",
    "\n",
    "    return model, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee78f7b",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "232e8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_survival_clv_features_df(\n",
    "    seed_customers_path: str,\n",
    "    seed_transactions_path: str,\n",
    "    cutoff_date,\n",
    "    max_data_date,\n",
    "    churn_windows=[30],\n",
    "):\n",
    "    \"\"\"\n",
    "    Build survival analysis dataset from customer & transaction CSV paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    survival_df : pd.DataFrame\n",
    "        Feature matrix indexed by customer_id, including duration & event columns\n",
    "    label_df : pd.DataFrame\n",
    "        Churn labels indexed by customer_id\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load raw data ---\n",
    "    customers_df = pd.read_csv(seed_customers_path)\n",
    "    transactions_df = pd.read_csv(seed_transactions_path)\n",
    "\n",
    "    # --- Build modeling base ---\n",
    "    transactions_modeling_df, customers_modeling_df = build_training_base(\n",
    "        seed_customers_path=seed_customers_path,\n",
    "        seed_transactions_path=seed_transactions_path,\n",
    "        train_snapshot_date=cutoff_date,\n",
    "        churn_windows=list(churn_windows),\n",
    "    )\n",
    "\n",
    "    # --- Feature engineering ---\n",
    "    # --- Add transaction features ---\n",
    "    raw_features_df = build_customer_features(\n",
    "        transactions_modeling_df,\n",
    "        customers_modeling_df,\n",
    "        observed_date=cutoff_date,\n",
    "    )\n",
    "\n",
    "    # --- Add RFM features ---\n",
    "    rfm_features_df = get_customers_screenshot_summary_from_transactions_df(\n",
    "        transactions_df=transactions_modeling_df,\n",
    "        observed_date=cutoff_date,\n",
    "        column_names=[\"customer_id\", \"transaction_date\", \"amount\"],\n",
    "    )\n",
    "\n",
    "    # --- Add survival targets (T, E) ---\n",
    "    survival_df = add_duration_event(\n",
    "        customers_df=raw_features_df,\n",
    "        obs_end_date=max_data_date,\n",
    "        start_col=\"signup_date\",\n",
    "        termination_col=\"termination_date\",\n",
    "    )\n",
    "\n",
    "    # --- Merge datasets ---\n",
    "    survival_df = survival_df.merge(\n",
    "        rfm_features_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # --- Extract labels ---\n",
    "    label_cols = [f\"is_churn_{w}_days\" for w in churn_windows]\n",
    "    label_df = survival_df[[\"customer_id\", *label_cols]].set_index(\"customer_id\")\n",
    "\n",
    "    # --- Drop target / leakage columns ---\n",
    "    survival_df = survival_df.drop(\n",
    "        columns=[\n",
    "            \"signup_date\",\n",
    "            \"termination_date\",\n",
    "            \"true_lifetime_days\",\n",
    "            \"period_first_transaction_date\",\n",
    "            \"period_last_transaction_date\",\n",
    "            \"days_until_observed\",\n",
    "            *label_cols,\n",
    "        ]\n",
    "    ).set_index(\"customer_id\")\n",
    "\n",
    "    return survival_df, label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "06c4a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_survival_clv(\n",
    "    survival_model,\n",
    "    ggf,\n",
    "    X,\n",
    "    horizon_days=30,\n",
    "    frequency_col=\"period_transaction_count\",\n",
    "    tenure_col=\"T\"\n",
    "):\n",
    "    surv = survival_model.predict_survival_function(X)\n",
    "\n",
    "    expected_aov = ggf.conditional_expected_average_profit(\n",
    "        X[\"period_transaction_count\"],\n",
    "        X[\"period_total_amount\"],\n",
    "    )\n",
    "\n",
    "    lambda_rate = X[frequency_col] / X[tenure_col]\n",
    "\n",
    "    clv = []\n",
    "    for i in range(len(X)):\n",
    "        s = surv.iloc[:, i]\n",
    "        s = s.loc[s.index <= horizon_days]\n",
    "\n",
    "        clv_i = (\n",
    "            expected_aov.iloc[i]\n",
    "            * lambda_rate.iloc[i]\n",
    "            * s.sum()\n",
    "        )\n",
    "        clv.append(clv_i)\n",
    "\n",
    "    return pd.Series(clv, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ae604028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gamma_gamma(\n",
    "    X_train: pd.DataFrame,\n",
    "    model_params: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a Gamma-Gamma model for monetary value.\n",
    "    Logs likelihood-based metrics only.\n",
    "    \"\"\"\n",
    "\n",
    "    model_params = model_params or {}\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"gamma_gamma\")\n",
    "\n",
    "    try:\n",
    "        # ======================\n",
    "        # Data selection (MANDATORY)\n",
    "        # ======================\n",
    "        summary_gg = X_train[\n",
    "            (X_train[\"period_transaction_count\"] > 0)\n",
    "            &\n",
    "            (X_train[\"period_total_amount\"] > 0)\n",
    "        ].copy()\n",
    "\n",
    "        mlflow.log_metric(\"n_customers_train\", len(summary_gg))\n",
    "\n",
    "        # ======================\n",
    "        # Train Gamma-Gamma\n",
    "        # ======================\n",
    "        ggf = GammaGammaFitter(**model_params)\n",
    "\n",
    "        ggf.fit(\n",
    "            frequency=summary_gg[\"period_transaction_count\"],\n",
    "            monetary_value=summary_gg[\"period_total_amount\"],\n",
    "        )\n",
    "\n",
    "        # ======================\n",
    "        # Evaluation (LIKELIHOOD)\n",
    "        # ======================\n",
    "        nll = float(ggf._negative_log_likelihood_)\n",
    "        mlflow.log_metric(\"neg_log_likelihood\", nll)\n",
    "\n",
    "        # Optional sanity metrics\n",
    "        avg_monetary_pred = float(\n",
    "            ggf.conditional_expected_average_profit(\n",
    "                summary_gg[\"period_transaction_count\"],\n",
    "                summary_gg[\"period_total_amount\"],\n",
    "            ).mean()\n",
    "        )\n",
    "        mlflow.log_metric(\"avg_predicted_monetary_value\", avg_monetary_pred)\n",
    "\n",
    "        # ======================\n",
    "        # Log parameters\n",
    "        # ======================\n",
    "        for k, v in ggf.params_.items():\n",
    "            mlflow.log_metric(f\"param_{k}\", float(v))\n",
    "\n",
    "        for k, v in model_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        # ======================\n",
    "        # Model artifact\n",
    "        # ======================\n",
    "        log_lifetimes_model(ggf)\n",
    "\n",
    "        mlflow.set_tag(\"run_status\", \"success\")\n",
    "\n",
    "        return {\n",
    "            \"model\": ggf,\n",
    "            \"neg_log_likelihood\": nll,\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        error_msg = traceback.format_exc()\n",
    "\n",
    "        mlflow.set_tag(\"run_status\", \"failed\")\n",
    "        mlflow.log_text(error_msg, artifact_file=\"training_error.txt\")\n",
    "\n",
    "        for k, v in model_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        print(\"[WARN] Gamma-Gamma training failed\")\n",
    "        print(error_msg)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f02829",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed9123",
   "metadata": {},
   "source": [
    "### Pipeline 1: For BG-NBD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c00a29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    clv_features_cut_30d_df,\n",
    "    truth_clv_30d_df,\n",
    "    transactions_30d_cut_df,\n",
    "    customer_ids_30d_cut,\n",
    ") = get_bgf_clv_features_df(\n",
    "    transactions_path=f\"../{SEED_TRANSACTIONS}\",\n",
    "    customers_path=f\"../{SEED_CUSTOMERS}\",\n",
    "    observed_date=OBSERVED_DATE,\n",
    "    cutoff_days=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fa2bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_features_cut_30d_df = clv_features_cut_30d_df.merge(\n",
    "    truth_clv_30d_df,\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d835b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = (\n",
    "    BASE_GOLD_DIR\n",
    "    / \"cut_30d\"\n",
    "    / \"features\"\n",
    "    / \"clv\"\n",
    "    / \"bgf_gg\"\n",
    "    / \"raw\"\n",
    "    / \"features.csv\"\n",
    ")\n",
    "\n",
    "# create parent directories if they don't exist\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clv_features_cut_30d_df.to_csv(\n",
    "    output_path\n",
    "    ,index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab965ddc",
   "metadata": {},
   "source": [
    "### Split dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db915327",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_bgf,\n",
    "    X_val_bgf,\n",
    "    X_test_bgf,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test\n",
    ") = split_train_test_val(\n",
    "    customers_modeling_df=clv_features_cut_30d_df,\n",
    "    targets=['CLV_30d'],\n",
    "    test_size=0.33,\n",
    "    val_size=0.33,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ffaaf",
   "metadata": {},
   "source": [
    "### Pipeline 2: For Survival model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3634461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_features_labels = pd.read_csv(\n",
    "    BASE_GOLD_DIR\n",
    "    / \"cut_30d\"\n",
    "    / \"features\"\n",
    "    / \"clv\"\n",
    "    / \"survival_gg\"\n",
    "    / \"raw\"\n",
    "    / \"features.csv\"\n",
    ", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e2320bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = survival_features_labels[['is_churn_30_days']]\n",
    "raw_survival_df = survival_features_labels.drop(columns=['is_churn_30_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "34ba15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_survival_raw = raw_survival_df.loc[X_train_bgf.index]\n",
    "X_val_survival_raw   = raw_survival_df.loc[X_val_bgf.index]\n",
    "X_test_survival_raw  = raw_survival_df.loc[X_test_bgf.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "24b06fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = (\n",
    "    BASE_GOLD_DIR\n",
    "    / \"cut_30d\"\n",
    "    / \"features\"\n",
    "    / \"clv\"\n",
    "    / \"survival_gg\"\n",
    "    / \"transformed\"\n",
    "    / \"survival_pipeline.pkl\"\n",
    ")\n",
    "\n",
    "with open(input_path, \"rb\") as f:\n",
    "    survival_pipeline = cloudpickle.load(f)\n",
    "\n",
    "X_train_survival_transformed = survival_pipeline.transform(X_train_survival_raw)\n",
    "X_test_survival_transformed = survival_pipeline.transform(X_test_survival_raw)\n",
    "X_val_survival_transformed = survival_pipeline.transform(X_val_survival_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95836923",
   "metadata": {},
   "source": [
    "### Add label for Gamma-Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551c728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803248de",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    clv_features_cut_30d_df,\n",
    "    truth_clv_30d_df,\n",
    "    transactions_30d_cut_df,\n",
    "    customer_ids_30d_cut,\n",
    ") = get_bgf_clv_features_df(\n",
    "    transactions_path=f\"../{SEED_TRANSACTIONS}\",\n",
    "    customers_path=f\"../{SEED_CUSTOMERS}\",\n",
    "    observed_date=OBSERVED_DATE,\n",
    "    cutoff_days=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "18c73022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>T</th>\n",
       "      <th>recency</th>\n",
       "      <th>period_total_amount</th>\n",
       "      <th>period_first_transaction_date</th>\n",
       "      <th>period_last_transaction_date</th>\n",
       "      <th>period_transaction_count</th>\n",
       "      <th>days_until_observed</th>\n",
       "      <th>period_tenure_days</th>\n",
       "      <th>CLV_30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>7</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "      <td>1001.05</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>221.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>15</td>\n",
       "      <td>259</td>\n",
       "      <td>255</td>\n",
       "      <td>975.54</td>\n",
       "      <td>2025-03-17</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>252.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00005</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>105.99</td>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>103.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00011</td>\n",
       "      <td>11</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>691.24</td>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>140.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00012</td>\n",
       "      <td>19</td>\n",
       "      <td>142</td>\n",
       "      <td>138</td>\n",
       "      <td>1716.30</td>\n",
       "      <td>2025-07-12</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>138</td>\n",
       "      <td>363.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>C02992</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>184.32</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>992.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>C02995</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>231.28</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>229.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>C02997</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>116.89</td>\n",
       "      <td>2025-11-02</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>342.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>C02998</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>545.58</td>\n",
       "      <td>2025-10-26</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>959.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>C02999</td>\n",
       "      <td>60</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>2319.34</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>443.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  frequency    T  recency  period_total_amount  \\\n",
       "0         C00000          7   82       78              1001.05   \n",
       "1         C00001         15  259      255               975.54   \n",
       "2         C00005          2   18       17               105.99   \n",
       "3         C00011         11   93       93               691.24   \n",
       "4         C00012         19  142      138              1716.30   \n",
       "...          ...        ...  ...      ...                  ...   \n",
       "1135      C02992          1    3        0               184.32   \n",
       "1136      C02995          8   39       36               231.28   \n",
       "1137      C02997          7   29       26               116.89   \n",
       "1138      C02998         10   36       36               545.58   \n",
       "1139      C02999         60  189      189              2319.34   \n",
       "\n",
       "     period_first_transaction_date period_last_transaction_date  \\\n",
       "0                       2025-09-10                   2025-11-27   \n",
       "1                       2025-03-17                   2025-11-27   \n",
       "2                       2025-11-13                   2025-11-30   \n",
       "3                       2025-08-30                   2025-12-01   \n",
       "4                       2025-07-12                   2025-11-27   \n",
       "...                            ...                          ...   \n",
       "1135                    2025-11-28                   2025-11-28   \n",
       "1136                    2025-10-23                   2025-11-28   \n",
       "1137                    2025-11-02                   2025-11-28   \n",
       "1138                    2025-10-26                   2025-12-01   \n",
       "1139                    2025-05-26                   2025-12-01   \n",
       "\n",
       "      period_transaction_count  days_until_observed  period_tenure_days  \\\n",
       "0                            8                    4                  78   \n",
       "1                           16                    4                 255   \n",
       "2                            3                    1                  17   \n",
       "3                           12                    0                  93   \n",
       "4                           20                    4                 138   \n",
       "...                        ...                  ...                 ...   \n",
       "1135                         2                    3                   0   \n",
       "1136                         9                    3                  36   \n",
       "1137                         8                    3                  26   \n",
       "1138                        11                    0                  36   \n",
       "1139                        61                    0                 189   \n",
       "\n",
       "      CLV_30d  \n",
       "0      221.80  \n",
       "1      252.92  \n",
       "2      103.13  \n",
       "3      140.51  \n",
       "4      363.35  \n",
       "...       ...  \n",
       "1135   992.58  \n",
       "1136   229.42  \n",
       "1137   342.94  \n",
       "1138   959.45  \n",
       "1139   443.54  \n",
       "\n",
       "[1140 rows x 11 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clv_features_cut_30d_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c787acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd38d721",
   "metadata": {},
   "source": [
    "# Log Gamma-Gamma Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32ef06",
   "metadata": {},
   "source": [
    "# Approach 1 - BG-NBD + Gamma–Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87f553",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2b5ca",
   "metadata": {},
   "source": [
    "I already trained and logged a model on 30 days cut off data, so I will just call the model instead of repeating the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360a34d",
   "metadata": {},
   "source": [
    "### BG-NBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "20c2c458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e4aa5c53e3477db34349b08e1f9bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bgf, metadata = load_prod_bg_nbd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bedaa",
   "metadata": {},
   "source": [
    "### Gamma-Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "39edc32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifetimes.GammaGammaFitter: fitted with 763 subjects, p: 3.18, q: 0.27, v: 3.18>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ggf = GammaGammaFitter(penalizer_coef=0.01)\n",
    "\n",
    "summary_gg = X_train_bgf[X_train_bgf[\"period_total_amount\"] > 0]\n",
    "\n",
    "ggf.fit(\n",
    "    summary_gg[\"period_transaction_count\"],\n",
    "    summary_gg[\"period_total_amount\"],\n",
    ")\n",
    "\n",
    "X_train_bgf.loc[summary_gg.index, \"expected_avg_order_value\"] = (\n",
    "    ggf.conditional_expected_average_profit(\n",
    "        summary_gg[\"period_transaction_count\"],\n",
    "        summary_gg[\"period_total_amount\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "X_train_bgf[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    X_train_bgf[\"frequency\"],\n",
    "    X_train_bgf[\"recency\"],\n",
    "    X_train_bgf[\"T\"],\n",
    "    X_train_bgf[\"period_total_amount\"],\n",
    "    time=30,\n",
    "    discount_rate=0.0,\n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "X_test_bgf[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    X_test_bgf[\"frequency\"],\n",
    "    X_test_bgf[\"recency\"],\n",
    "    X_test_bgf[\"T\"],\n",
    "    X_test_bgf[\"period_total_amount\"],\n",
    "    time=30,\n",
    "    discount_rate=0.0,\n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "X_val_bgf[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    X_val_bgf[\"frequency\"],\n",
    "    X_val_bgf[\"recency\"],\n",
    "    X_val_bgf[\"T\"],\n",
    "    X_val_bgf[\"period_total_amount\"],\n",
    "    time=30,\n",
    "    discount_rate=0.0,\n",
    "    freq=\"D\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f34f1",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6312f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs = {\n",
    "    'train': [X_train_bgf, y_train],\n",
    "    'test': [X_test_bgf, y_test],\n",
    "    'val': [X_val_bgf, y_val]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e63ee66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for train set:\n",
      "53181.358468091144\n",
      "Evaluation results for test set:\n",
      "57465.24565346731\n",
      "Evaluation results for val set:\n",
      "63287.73624104562\n"
     ]
    }
   ],
   "source": [
    "results_df = {}\n",
    "for split in split_dfs.keys():\n",
    "\n",
    "    print(f'Evaluation results for {split} set:')\n",
    "\n",
    "    X = split_dfs[split][0].copy()\n",
    "    y = split_dfs[split][1]\n",
    "\n",
    "    X[\"pred_CLV_30d\"] = ggf.customer_lifetime_value(\n",
    "        bgf,\n",
    "        X[\"frequency\"],\n",
    "        X[\"recency\"],\n",
    "        X[\"T\"],\n",
    "        X[\"period_total_amount\"],\n",
    "        time=30,\n",
    "        discount_rate=0.0,\n",
    "        freq=\"D\"\n",
    "    )\n",
    "\n",
    "    mae = (y[\"CLV_30d\"] - X[\"pred_CLV_30d\"]).abs().mean()\n",
    "\n",
    "    results_df[split] = mae\n",
    "\n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a8802",
   "metadata": {},
   "source": [
    "That seems like a pretty big deviation. I will benchmark it with LGBM some time later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa49c5",
   "metadata": {},
   "source": [
    "# Approach 2 - Survival Analysis + Gamma–Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c7414",
   "metadata": {},
   "source": [
    "I already trained and logged a model on 30 days cut off data, so I will just call the model instead of repeating the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "103262f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfm_recency_all_time</th>\n",
       "      <th>rfm_frequency_all_time</th>\n",
       "      <th>rfm_monetary_all_time</th>\n",
       "      <th>tenure_all_time</th>\n",
       "      <th>rfm_recency_30d</th>\n",
       "      <th>rfm_frequency_30d</th>\n",
       "      <th>rfm_monetary_30d</th>\n",
       "      <th>tenure_30d</th>\n",
       "      <th>rfm_recency_60d</th>\n",
       "      <th>rfm_frequency_60d</th>\n",
       "      <th>...</th>\n",
       "      <th>q70_days_since_first_transaction</th>\n",
       "      <th>q80_days_since_first_transaction</th>\n",
       "      <th>q90_days_since_first_transaction</th>\n",
       "      <th>q95_days_since_first_transaction</th>\n",
       "      <th>q99_days_since_first_transaction</th>\n",
       "      <th>period_total_amount</th>\n",
       "      <th>period_transaction_count</th>\n",
       "      <th>period_tenure_days</th>\n",
       "      <th>T</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C01693</th>\n",
       "      <td>-0.853786</td>\n",
       "      <td>-0.181913</td>\n",
       "      <td>-0.366090</td>\n",
       "      <td>-0.282967</td>\n",
       "      <td>-0.748893</td>\n",
       "      <td>-0.544658</td>\n",
       "      <td>-0.504787</td>\n",
       "      <td>-0.771715</td>\n",
       "      <td>-0.735856</td>\n",
       "      <td>-0.775701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161959</td>\n",
       "      <td>-0.156042</td>\n",
       "      <td>-0.157743</td>\n",
       "      <td>-0.155195</td>\n",
       "      <td>-0.158228</td>\n",
       "      <td>-0.366090</td>\n",
       "      <td>-0.181913</td>\n",
       "      <td>-0.282967</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02471</th>\n",
       "      <td>-0.816735</td>\n",
       "      <td>-0.114298</td>\n",
       "      <td>-0.389487</td>\n",
       "      <td>-0.170036</td>\n",
       "      <td>-0.865413</td>\n",
       "      <td>-0.311917</td>\n",
       "      <td>-0.519346</td>\n",
       "      <td>-0.472652</td>\n",
       "      <td>-0.753408</td>\n",
       "      <td>-0.686506</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161959</td>\n",
       "      <td>-0.156042</td>\n",
       "      <td>-0.157743</td>\n",
       "      <td>-0.155195</td>\n",
       "      <td>-0.158228</td>\n",
       "      <td>-0.389487</td>\n",
       "      <td>-0.114298</td>\n",
       "      <td>-0.170036</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02941</th>\n",
       "      <td>-0.866136</td>\n",
       "      <td>3.536948</td>\n",
       "      <td>4.000940</td>\n",
       "      <td>2.568535</td>\n",
       "      <td>-0.865413</td>\n",
       "      <td>3.489517</td>\n",
       "      <td>3.854822</td>\n",
       "      <td>2.517973</td>\n",
       "      <td>-0.841167</td>\n",
       "      <td>3.684027</td>\n",
       "      <td>...</td>\n",
       "      <td>2.191455</td>\n",
       "      <td>2.158226</td>\n",
       "      <td>2.259845</td>\n",
       "      <td>2.163969</td>\n",
       "      <td>2.243595</td>\n",
       "      <td>4.000940</td>\n",
       "      <td>3.536948</td>\n",
       "      <td>2.568535</td>\n",
       "      <td>284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C00980</th>\n",
       "      <td>-0.680880</td>\n",
       "      <td>-0.046682</td>\n",
       "      <td>-0.084862</td>\n",
       "      <td>-0.071222</td>\n",
       "      <td>-0.850848</td>\n",
       "      <td>-0.079176</td>\n",
       "      <td>-0.185854</td>\n",
       "      <td>-0.205070</td>\n",
       "      <td>-0.841167</td>\n",
       "      <td>-0.418923</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.508919</td>\n",
       "      <td>-1.546751</td>\n",
       "      <td>-1.578991</td>\n",
       "      <td>-1.589982</td>\n",
       "      <td>-1.595282</td>\n",
       "      <td>-0.084862</td>\n",
       "      <td>-0.046682</td>\n",
       "      <td>-0.071222</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C00202</th>\n",
       "      <td>-0.853786</td>\n",
       "      <td>4.821646</td>\n",
       "      <td>2.651671</td>\n",
       "      <td>1.566274</td>\n",
       "      <td>-0.836283</td>\n",
       "      <td>4.575641</td>\n",
       "      <td>2.710339</td>\n",
       "      <td>1.384683</td>\n",
       "      <td>-0.823615</td>\n",
       "      <td>4.486778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889695</td>\n",
       "      <td>0.987668</td>\n",
       "      <td>0.928951</td>\n",
       "      <td>0.832748</td>\n",
       "      <td>0.776687</td>\n",
       "      <td>2.651671</td>\n",
       "      <td>4.821646</td>\n",
       "      <td>1.566274</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02749</th>\n",
       "      <td>-0.890837</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.358735</td>\n",
       "      <td>0.211105</td>\n",
       "      <td>-0.850848</td>\n",
       "      <td>0.463886</td>\n",
       "      <td>0.145637</td>\n",
       "      <td>-0.157850</td>\n",
       "      <td>-0.823615</td>\n",
       "      <td>0.027050</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.385372</td>\n",
       "      <td>-1.401774</td>\n",
       "      <td>-1.415377</td>\n",
       "      <td>-1.418924</td>\n",
       "      <td>-1.418624</td>\n",
       "      <td>0.358735</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.211105</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02882</th>\n",
       "      <td>-0.890837</td>\n",
       "      <td>-0.249529</td>\n",
       "      <td>0.850188</td>\n",
       "      <td>1.015737</td>\n",
       "      <td>-0.836283</td>\n",
       "      <td>-0.234337</td>\n",
       "      <td>1.007525</td>\n",
       "      <td>0.723598</td>\n",
       "      <td>-0.402370</td>\n",
       "      <td>-0.329728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270439</td>\n",
       "      <td>-0.274172</td>\n",
       "      <td>-0.304263</td>\n",
       "      <td>-0.319270</td>\n",
       "      <td>-0.326580</td>\n",
       "      <td>0.850188</td>\n",
       "      <td>-0.249529</td>\n",
       "      <td>1.015737</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02972</th>\n",
       "      <td>-0.582077</td>\n",
       "      <td>0.088549</td>\n",
       "      <td>0.774863</td>\n",
       "      <td>0.578130</td>\n",
       "      <td>-0.836283</td>\n",
       "      <td>0.075984</td>\n",
       "      <td>0.776868</td>\n",
       "      <td>0.629157</td>\n",
       "      <td>-0.753408</td>\n",
       "      <td>-0.062145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662172</td>\n",
       "      <td>-0.596344</td>\n",
       "      <td>-0.682774</td>\n",
       "      <td>-0.720731</td>\n",
       "      <td>-0.745216</td>\n",
       "      <td>0.774863</td>\n",
       "      <td>0.088549</td>\n",
       "      <td>0.578130</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02311</th>\n",
       "      <td>0.084844</td>\n",
       "      <td>-0.790454</td>\n",
       "      <td>-0.463610</td>\n",
       "      <td>-1.031134</td>\n",
       "      <td>-0.151727</td>\n",
       "      <td>-0.777399</td>\n",
       "      <td>-0.414240</td>\n",
       "      <td>-1.055037</td>\n",
       "      <td>-0.507681</td>\n",
       "      <td>-0.775701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161959</td>\n",
       "      <td>-0.156042</td>\n",
       "      <td>-0.157743</td>\n",
       "      <td>-0.155195</td>\n",
       "      <td>-0.158228</td>\n",
       "      <td>-0.463610</td>\n",
       "      <td>-0.790454</td>\n",
       "      <td>-1.031134</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02955</th>\n",
       "      <td>-0.878487</td>\n",
       "      <td>1.305631</td>\n",
       "      <td>2.768518</td>\n",
       "      <td>2.286208</td>\n",
       "      <td>-0.515852</td>\n",
       "      <td>1.162108</td>\n",
       "      <td>2.295476</td>\n",
       "      <td>1.809667</td>\n",
       "      <td>-0.507681</td>\n",
       "      <td>1.364968</td>\n",
       "      <td>...</td>\n",
       "      <td>1.588788</td>\n",
       "      <td>1.594425</td>\n",
       "      <td>1.407585</td>\n",
       "      <td>1.314501</td>\n",
       "      <td>1.677708</td>\n",
       "      <td>2.768518</td>\n",
       "      <td>1.305631</td>\n",
       "      <td>2.286208</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rfm_recency_all_time  rfm_frequency_all_time  \\\n",
       "customer_id                                                 \n",
       "C01693                  -0.853786               -0.181913   \n",
       "C02471                  -0.816735               -0.114298   \n",
       "C02941                  -0.866136                3.536948   \n",
       "C00980                  -0.680880               -0.046682   \n",
       "C00202                  -0.853786                4.821646   \n",
       "...                           ...                     ...   \n",
       "C02749                  -0.890837                0.899937   \n",
       "C02882                  -0.890837               -0.249529   \n",
       "C02972                  -0.582077                0.088549   \n",
       "C02311                   0.084844               -0.790454   \n",
       "C02955                  -0.878487                1.305631   \n",
       "\n",
       "             rfm_monetary_all_time  tenure_all_time  rfm_recency_30d  \\\n",
       "customer_id                                                            \n",
       "C01693                   -0.366090        -0.282967        -0.748893   \n",
       "C02471                   -0.389487        -0.170036        -0.865413   \n",
       "C02941                    4.000940         2.568535        -0.865413   \n",
       "C00980                   -0.084862        -0.071222        -0.850848   \n",
       "C00202                    2.651671         1.566274        -0.836283   \n",
       "...                            ...              ...              ...   \n",
       "C02749                    0.358735         0.211105        -0.850848   \n",
       "C02882                    0.850188         1.015737        -0.836283   \n",
       "C02972                    0.774863         0.578130        -0.836283   \n",
       "C02311                   -0.463610        -1.031134        -0.151727   \n",
       "C02955                    2.768518         2.286208        -0.515852   \n",
       "\n",
       "             rfm_frequency_30d  rfm_monetary_30d  tenure_30d  rfm_recency_60d  \\\n",
       "customer_id                                                                     \n",
       "C01693               -0.544658         -0.504787   -0.771715        -0.735856   \n",
       "C02471               -0.311917         -0.519346   -0.472652        -0.753408   \n",
       "C02941                3.489517          3.854822    2.517973        -0.841167   \n",
       "C00980               -0.079176         -0.185854   -0.205070        -0.841167   \n",
       "C00202                4.575641          2.710339    1.384683        -0.823615   \n",
       "...                        ...               ...         ...              ...   \n",
       "C02749                0.463886          0.145637   -0.157850        -0.823615   \n",
       "C02882               -0.234337          1.007525    0.723598        -0.402370   \n",
       "C02972                0.075984          0.776868    0.629157        -0.753408   \n",
       "C02311               -0.777399         -0.414240   -1.055037        -0.507681   \n",
       "C02955                1.162108          2.295476    1.809667        -0.507681   \n",
       "\n",
       "             rfm_frequency_60d  ...  q70_days_since_first_transaction  \\\n",
       "customer_id                     ...                                     \n",
       "C01693               -0.775701  ...                         -0.161959   \n",
       "C02471               -0.686506  ...                         -0.161959   \n",
       "C02941                3.684027  ...                          2.191455   \n",
       "C00980               -0.418923  ...                         -1.508919   \n",
       "C00202                4.486778  ...                          0.889695   \n",
       "...                        ...  ...                               ...   \n",
       "C02749                0.027050  ...                         -1.385372   \n",
       "C02882               -0.329728  ...                         -0.270439   \n",
       "C02972               -0.062145  ...                         -0.662172   \n",
       "C02311               -0.775701  ...                         -0.161959   \n",
       "C02955                1.364968  ...                          1.588788   \n",
       "\n",
       "             q80_days_since_first_transaction  \\\n",
       "customer_id                                     \n",
       "C01693                              -0.156042   \n",
       "C02471                              -0.156042   \n",
       "C02941                               2.158226   \n",
       "C00980                              -1.546751   \n",
       "C00202                               0.987668   \n",
       "...                                       ...   \n",
       "C02749                              -1.401774   \n",
       "C02882                              -0.274172   \n",
       "C02972                              -0.596344   \n",
       "C02311                              -0.156042   \n",
       "C02955                               1.594425   \n",
       "\n",
       "             q90_days_since_first_transaction  \\\n",
       "customer_id                                     \n",
       "C01693                              -0.157743   \n",
       "C02471                              -0.157743   \n",
       "C02941                               2.259845   \n",
       "C00980                              -1.578991   \n",
       "C00202                               0.928951   \n",
       "...                                       ...   \n",
       "C02749                              -1.415377   \n",
       "C02882                              -0.304263   \n",
       "C02972                              -0.682774   \n",
       "C02311                              -0.157743   \n",
       "C02955                               1.407585   \n",
       "\n",
       "             q95_days_since_first_transaction  \\\n",
       "customer_id                                     \n",
       "C01693                              -0.155195   \n",
       "C02471                              -0.155195   \n",
       "C02941                               2.163969   \n",
       "C00980                              -1.589982   \n",
       "C00202                               0.832748   \n",
       "...                                       ...   \n",
       "C02749                              -1.418924   \n",
       "C02882                              -0.319270   \n",
       "C02972                              -0.720731   \n",
       "C02311                              -0.155195   \n",
       "C02955                               1.314501   \n",
       "\n",
       "             q99_days_since_first_transaction  period_total_amount  \\\n",
       "customer_id                                                          \n",
       "C01693                              -0.158228            -0.366090   \n",
       "C02471                              -0.158228            -0.389487   \n",
       "C02941                               2.243595             4.000940   \n",
       "C00980                              -1.595282            -0.084862   \n",
       "C00202                               0.776687             2.651671   \n",
       "...                                       ...                  ...   \n",
       "C02749                              -1.418624             0.358735   \n",
       "C02882                              -0.326580             0.850188   \n",
       "C02972                              -0.745216             0.774863   \n",
       "C02311                              -0.158228            -0.463610   \n",
       "C02955                               1.677708             2.768518   \n",
       "\n",
       "             period_transaction_count  period_tenure_days    T  E  \n",
       "customer_id                                                        \n",
       "C01693                      -0.181913           -0.282967  101  0  \n",
       "C02471                      -0.114298           -0.170036  114  0  \n",
       "C02941                       3.536948            2.568535  284  1  \n",
       "C00980                      -0.046682           -0.071222  132  0  \n",
       "C00202                       4.821646            1.566274  231  0  \n",
       "...                               ...                 ...  ... ..  \n",
       "C02749                       0.899937            0.211105  135  0  \n",
       "C02882                      -0.249529            1.015737  192  0  \n",
       "C02972                       0.088549            0.578130  205  0  \n",
       "C02311                      -0.790454           -1.031134  133  0  \n",
       "C02955                       1.305631            2.286208  281  1  \n",
       "\n",
       "[763 rows x 107 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_survival_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2386fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_survival_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2391c",
   "metadata": {},
   "source": [
    "### Survival Model (Weibull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "53ba5d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7722b55e2d84911ae07b89f1e19944b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "survival_model, metadata = load_survival_analysis_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6dc15510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>customer_id</th>\n",
       "      <th>C01693</th>\n",
       "      <th>C02471</th>\n",
       "      <th>C02941</th>\n",
       "      <th>C00980</th>\n",
       "      <th>C00202</th>\n",
       "      <th>C00226</th>\n",
       "      <th>C02911</th>\n",
       "      <th>C00223</th>\n",
       "      <th>C02850</th>\n",
       "      <th>C01910</th>\n",
       "      <th>...</th>\n",
       "      <th>C02951</th>\n",
       "      <th>C00240</th>\n",
       "      <th>C00893</th>\n",
       "      <th>C01282</th>\n",
       "      <th>C00339</th>\n",
       "      <th>C02749</th>\n",
       "      <th>C02882</th>\n",
       "      <th>C02972</th>\n",
       "      <th>C02311</th>\n",
       "      <th>C02955</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.996729</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.988703</td>\n",
       "      <td>0.999488</td>\n",
       "      <td>0.991691</td>\n",
       "      <td>0.996499</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0.996448</td>\n",
       "      <td>0.994999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997969</td>\n",
       "      <td>0.997725</td>\n",
       "      <td>0.989381</td>\n",
       "      <td>0.993476</td>\n",
       "      <td>0.992364</td>\n",
       "      <td>0.995660</td>\n",
       "      <td>0.998287</td>\n",
       "      <td>0.995498</td>\n",
       "      <td>9.634624e-01</td>\n",
       "      <td>0.999747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31.0</th>\n",
       "      <td>0.996711</td>\n",
       "      <td>0.996407</td>\n",
       "      <td>0.999786</td>\n",
       "      <td>0.987596</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>0.990876</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>0.998451</td>\n",
       "      <td>0.996098</td>\n",
       "      <td>0.994507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997769</td>\n",
       "      <td>0.997501</td>\n",
       "      <td>0.988340</td>\n",
       "      <td>0.992835</td>\n",
       "      <td>0.991615</td>\n",
       "      <td>0.995234</td>\n",
       "      <td>0.998118</td>\n",
       "      <td>0.995055</td>\n",
       "      <td>9.599336e-01</td>\n",
       "      <td>0.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32.0</th>\n",
       "      <td>0.996398</td>\n",
       "      <td>0.996066</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.986422</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.990011</td>\n",
       "      <td>0.995788</td>\n",
       "      <td>0.998304</td>\n",
       "      <td>0.995727</td>\n",
       "      <td>0.993985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997556</td>\n",
       "      <td>0.997263</td>\n",
       "      <td>0.987236</td>\n",
       "      <td>0.992155</td>\n",
       "      <td>0.990819</td>\n",
       "      <td>0.994780</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.994585</td>\n",
       "      <td>9.561998e-01</td>\n",
       "      <td>0.999696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.0</th>\n",
       "      <td>0.996066</td>\n",
       "      <td>0.995704</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.985178</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.989094</td>\n",
       "      <td>0.995401</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.995334</td>\n",
       "      <td>0.993432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997331</td>\n",
       "      <td>0.997011</td>\n",
       "      <td>0.986067</td>\n",
       "      <td>0.991435</td>\n",
       "      <td>0.989977</td>\n",
       "      <td>0.994300</td>\n",
       "      <td>0.997749</td>\n",
       "      <td>0.994087</td>\n",
       "      <td>9.522575e-01</td>\n",
       "      <td>0.999668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.0</th>\n",
       "      <td>0.995716</td>\n",
       "      <td>0.995321</td>\n",
       "      <td>0.999722</td>\n",
       "      <td>0.983865</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.988126</td>\n",
       "      <td>0.994991</td>\n",
       "      <td>0.997982</td>\n",
       "      <td>0.994918</td>\n",
       "      <td>0.992847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997093</td>\n",
       "      <td>0.996744</td>\n",
       "      <td>0.984831</td>\n",
       "      <td>0.990673</td>\n",
       "      <td>0.989086</td>\n",
       "      <td>0.993793</td>\n",
       "      <td>0.997548</td>\n",
       "      <td>0.993561</td>\n",
       "      <td>9.481035e-01</td>\n",
       "      <td>0.999638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344.0</th>\n",
       "      <td>0.037961</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>0.808822</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.572283</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.021795</td>\n",
       "      <td>0.214595</td>\n",
       "      <td>0.020618</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108803</td>\n",
       "      <td>0.083343</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>0.154014</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>2.326023e-18</td>\n",
       "      <td>0.758885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351.0</th>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.022685</td>\n",
       "      <td>0.798682</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.553603</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.017361</td>\n",
       "      <td>0.195829</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095359</td>\n",
       "      <td>0.071896</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.137801</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>2.080179e-19</td>\n",
       "      <td>0.746536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354.0</th>\n",
       "      <td>0.028687</td>\n",
       "      <td>0.020660</td>\n",
       "      <td>0.794259</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.545575</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.015706</td>\n",
       "      <td>0.188098</td>\n",
       "      <td>0.014788</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089980</td>\n",
       "      <td>0.067369</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.131216</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>7.187856e-20</td>\n",
       "      <td>0.741165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355.0</th>\n",
       "      <td>0.027871</td>\n",
       "      <td>0.020019</td>\n",
       "      <td>0.792775</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.542897</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.185563</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088237</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>0.129070</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>5.024958e-20</td>\n",
       "      <td>0.739364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356.0</th>\n",
       "      <td>0.027075</td>\n",
       "      <td>0.019395</td>\n",
       "      <td>0.791285</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.540218</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>0.183049</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>3.506272e-20</td>\n",
       "      <td>0.737559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282 rows × 763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "customer_id    C01693    C02471    C02941    C00980    C00202    C00226  \\\n",
       "30.0         0.997006  0.996729  0.999806  0.988703  0.999488  0.991691   \n",
       "31.0         0.996711  0.996407  0.999786  0.987596  0.999438  0.990876   \n",
       "32.0         0.996398  0.996066  0.999766  0.986422  0.999385  0.990011   \n",
       "33.0         0.996066  0.995704  0.999744  0.985178  0.999328  0.989094   \n",
       "34.0         0.995716  0.995321  0.999722  0.983865  0.999268  0.988126   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "344.0        0.037961  0.028056  0.808822  0.000004  0.572283  0.000112   \n",
       "351.0        0.031251  0.022685  0.798682  0.000002  0.553603  0.000065   \n",
       "354.0        0.028687  0.020660  0.794259  0.000001  0.545575  0.000051   \n",
       "355.0        0.027871  0.020019  0.792775  0.000001  0.542897  0.000047   \n",
       "356.0        0.027075  0.019395  0.791285  0.000001  0.540218  0.000044   \n",
       "\n",
       "customer_id    C02911    C00223    C02850    C01910  ...    C02951    C00240  \\\n",
       "30.0         0.996499  0.998590  0.996448  0.994999  ...  0.997969  0.997725   \n",
       "31.0         0.996154  0.998451  0.996098  0.994507  ...  0.997769  0.997501   \n",
       "32.0         0.995788  0.998304  0.995727  0.993985  ...  0.997556  0.997263   \n",
       "33.0         0.995401  0.998147  0.995334  0.993432  ...  0.997331  0.997011   \n",
       "34.0         0.994991  0.997982  0.994918  0.992847  ...  0.997093  0.996744   \n",
       "...               ...       ...       ...       ...  ...       ...       ...   \n",
       "344.0        0.021795  0.214595  0.020618  0.004214  ...  0.108803  0.083343   \n",
       "351.0        0.017361  0.195829  0.016369  0.003044  ...  0.095359  0.071896   \n",
       "354.0        0.015706  0.188098  0.014788  0.002638  ...  0.089980  0.067369   \n",
       "355.0        0.015185  0.185563  0.014290  0.002514  ...  0.088237  0.065909   \n",
       "356.0        0.014679  0.183049  0.013807  0.002395  ...  0.086520  0.064474   \n",
       "\n",
       "customer_id    C00893    C01282    C00339    C02749    C02882    C02972  \\\n",
       "30.0         0.989381  0.993476  0.992364  0.995660  0.998287  0.995498   \n",
       "31.0         0.988340  0.992835  0.991615  0.995234  0.998118  0.995055   \n",
       "32.0         0.987236  0.992155  0.990819  0.994780  0.997939  0.994585   \n",
       "33.0         0.986067  0.991435  0.989977  0.994300  0.997749  0.994087   \n",
       "34.0         0.984831  0.990673  0.989086  0.993793  0.997548  0.993561   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "344.0        0.000009  0.000793  0.000234  0.008703  0.154014  0.007285   \n",
       "351.0        0.000004  0.000519  0.000142  0.006564  0.137801  0.005436   \n",
       "354.0        0.000003  0.000430  0.000114  0.005797  0.131216  0.004779   \n",
       "355.0        0.000003  0.000404  0.000106  0.005560  0.129070  0.004576   \n",
       "356.0        0.000003  0.000379  0.000099  0.005331  0.126948  0.004381   \n",
       "\n",
       "customer_id        C02311    C02955  \n",
       "30.0         9.634624e-01  0.999747  \n",
       "31.0         9.599336e-01  0.999722  \n",
       "32.0         9.561998e-01  0.999696  \n",
       "33.0         9.522575e-01  0.999668  \n",
       "34.0         9.481035e-01  0.999638  \n",
       "...                   ...       ...  \n",
       "344.0        2.326023e-18  0.758885  \n",
       "351.0        2.080179e-19  0.746536  \n",
       "354.0        7.187856e-20  0.741165  \n",
       "355.0        5.024958e-20  0.739364  \n",
       "356.0        3.506272e-20  0.737559  \n",
       "\n",
       "[282 rows x 763 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survival_model.predict_survival_function(X_train_survival_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788f09f",
   "metadata": {},
   "source": [
    "### Gamma-Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ec28da6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifetimes.GammaGammaFitter: fitted with 763 subjects, p: 1.35, q: 0.19, v: 1.31>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggf = GammaGammaFitter(penalizer_coef=0.05)\n",
    "\n",
    "summary_gg = X_train_survival_transformed[\n",
    "    (X_train_survival_transformed[\"period_transaction_count\"] > 0)\n",
    "    &\n",
    "    (X_train_survival_transformed[\"period_total_amount\"] > 0)\n",
    "]\n",
    "#summary_gg = X_train_survival_transformed[X_train_survival_transformed[\"period_total_amount\"] > 0]\n",
    "\n",
    "ggf.fit(\n",
    "    summary_gg[\"period_transaction_count\"],\n",
    "    summary_gg[\"period_total_amount\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5b535",
   "metadata": {},
   "source": [
    "### Time-Dependent CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0a2dea89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "C01693      48.054052\n",
       "C02471      43.648346\n",
       "C02941    1111.694882\n",
       "C00980      71.134128\n",
       "C00202    1261.334172\n",
       "             ...     \n",
       "C02749     231.316466\n",
       "C02882      89.763756\n",
       "C02972     117.455829\n",
       "C02311       6.687918\n",
       "C02955     421.090500\n",
       "Length: 763, dtype: float64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_survival_clv(\n",
    "    survival_model,\n",
    "    ggf,\n",
    "    X_train_survival_transformed,\n",
    "    horizon_days=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef86a15",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2eccc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs = {\n",
    "    'train': [X_train_survival_transformed, y_train],\n",
    "    'test': [X_test_survival_transformed, y_test],\n",
    "    'val': [X_val_survival_transformed, y_val]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b937a16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for train set:\n",
      "317.51514088192545\n",
      "Evaluation results for test set:\n",
      "310.1090018471975\n",
      "Evaluation results for val set:\n",
      "345.60517674358726\n"
     ]
    }
   ],
   "source": [
    "results_df = {}\n",
    "for split in split_dfs.keys():\n",
    "\n",
    "    print(f'Evaluation results for {split} set:')\n",
    "\n",
    "    X = split_dfs[split][0].copy()\n",
    "    y = split_dfs[split][1]\n",
    "\n",
    "    X[\"pred_CLV_30d\"] = predict_survival_clv(\n",
    "        survival_model,\n",
    "        ggf,\n",
    "        X,\n",
    "        horizon_days=30,\n",
    "    )\n",
    "\n",
    "    mae = (y[\"CLV_30d\"] - X[\"pred_CLV_30d\"]).abs().mean()\n",
    "\n",
    "    results_df[split] = mae\n",
    "\n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aec0bd",
   "metadata": {},
   "source": [
    "# Compare Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da5bba",
   "metadata": {},
   "source": [
    "Summary on model performance on current train, test, val set:\n",
    "\n",
    "Approach 1\n",
    "- Evaluation results for train set: 317.51514088192545\n",
    "- Evaluation results for test set: 310.1090018471975\n",
    "- Evaluation results for val set: 345.60517674358726\n",
    "\n",
    "Approach 2\n",
    "- Evaluation results for train set: 317.51514088192545\n",
    "- Evaluation results for test set: 310.1090018471975\n",
    "- Evaluation results for val set: 345.60517674358726\n",
    "\n",
    "The performance of Survival Analysis + Gamma-Gamma is much better than that of BG-NBD + Gamma-Gamma. It is not unexpected though, considering the fact that the Survival Analysis model use multivariates (much more features than the features allowed in BGF). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef76bd",
   "metadata": {},
   "source": [
    "# Log Gamma-Gamma Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295a28d",
   "metadata": {},
   "source": [
    "## Setup Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f7ddbf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifetimes.GammaGammaFitter: fitted with 763 subjects, p: 1.35, q: 0.19, v: 1.31>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggf = GammaGammaFitter(penalizer_coef=0.05)\n",
    "\n",
    "summary_gg = X_train_survival_transformed[\n",
    "    (X_train_survival_transformed[\"period_transaction_count\"] > 0)\n",
    "    &\n",
    "    (X_train_survival_transformed[\"period_total_amount\"] > 0)\n",
    "]\n",
    "#summary_gg = X_train_survival_transformed[X_train_survival_transformed[\"period_total_amount\"] > 0]\n",
    "\n",
    "ggf.fit(\n",
    "    summary_gg[\"period_transaction_count\"],\n",
    "    summary_gg[\"period_total_amount\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7618cb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.041919993226887"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggf._negative_log_likelihood_"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "maipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
