{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1790b9e8",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e689c",
   "metadata": {},
   "source": [
    "I will also do holdout benchmarking for the Survival models. The holdout period will also be 30 days.\n",
    "\n",
    "It means I cut off 30 days from all available data, train and inference on that data, then evaluate against the truth 30 days later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b031f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5e538",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0e3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c5da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fdf914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8549109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.transforms import (\n",
    "    transform_transactions_df,\n",
    "    transform_customers_df,\n",
    "    get_customers_screenshot_summary_from_transactions_df,\n",
    "    add_churn_status\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb281a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a504f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter, WeibullAFTFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "16ad3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1f734296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e562053",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2728fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0060452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "137094be",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DATA_DATE = pd.Timestamp('2025-12-31')\n",
    "MAX_DATA_DATE_STR = MAX_DATA_DATE.strftime(\"%d_%m_%Y\")\n",
    "CUTOFF_DATE = MAX_DATA_DATE - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd62bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold\" / MAX_DATA_DATE_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc48712",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLRUNS_DIR = PROJECT_ROOT / \"mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adac241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_CUSTOMERS=os.getenv(\"SEED_CUSTOMERS\")\n",
    "SEED_TRANSACTIONS=os.getenv(\"SEED_TRANSACTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5013ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['is_churn_30_days']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa7ad0",
   "metadata": {},
   "source": [
    "## 02 Notebook Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb25626",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "40d625d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfm_window_features(customers_df, transactions_df, observed_date):\n",
    "\n",
    "    rfm_time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    for rfm_time_window in rfm_time_windows:\n",
    "\n",
    "        if rfm_time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(rfm_time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "        # Get a Customers Screenshot Summary DataFrame. It has RFM features and other variables that RFM features depend on.\n",
    "        summary_modeling_df = get_customers_screenshot_summary_from_transactions_df(\n",
    "            transactions_df=filtered_transactions_df,\n",
    "            observed_date=observed_date,\n",
    "            column_names=[\"customer_id\", \"transaction_date\", \"amount\"]\n",
    "        )\n",
    "\n",
    "        # Keep only customer_id and the RFM columns we care about\n",
    "        summary_modeling_df = summary_modeling_df[[\n",
    "            'customer_id',\n",
    "            'days_until_observed',\n",
    "            'period_transaction_count',\n",
    "            'period_total_amount',\n",
    "            'period_tenure_days'\n",
    "        ]]\n",
    "\n",
    "        # Rename columns in the summary DF, not the main DF\n",
    "        summary_modeling_df = summary_modeling_df.rename(columns={\n",
    "            'days_until_observed': f'rfm_recency_{rfm_time_window}',\n",
    "            'period_transaction_count': f'rfm_frequency_{rfm_time_window}',\n",
    "            'period_total_amount': f'rfm_monetary_{rfm_time_window}',\n",
    "            'period_tenure_days': f'tenure_{rfm_time_window}'\n",
    "        })\n",
    "        \n",
    "        # Merge with current data used for modelling.\n",
    "        customers_df = pd.merge(\n",
    "            customers_df,\n",
    "            summary_modeling_df,\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4444a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_features(customers_df, transactions_df, observed_date, feature_list):\n",
    "\n",
    "    time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    for time_window in time_windows:\n",
    "\n",
    "        if time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "    customers_list = filtered_transactions_df['customer_id'].unique()\n",
    "\n",
    "    slopes = {}\n",
    "\n",
    "    for customer_id in customers_list:\n",
    "\n",
    "        customer_transactions = filtered_transactions_df[filtered_transactions_df['customer_id'] == customer_id]\n",
    "\n",
    "        x = np.arange(len(customer_transactions)) #time axis\n",
    "        slopes[customer_id] = {} #initiate value list\n",
    "\n",
    "        for feature_name in feature_list:\n",
    "            y = customer_transactions[feature_name].values\n",
    "            x_valid = x[~np.isnan(y)]\n",
    "            y_valid = y[~np.isnan(y)]\n",
    "\n",
    "            if len(y_valid) < 2:\n",
    "                slopes[customer_id][feature_name] = np.nan\n",
    "            else:\n",
    "                slope = np.polyfit(x_valid, y_valid, 1)[0]\n",
    "                slopes[customer_id][feature_name] = slope\n",
    "\n",
    "    # Convert dict of dicts into dataframe\n",
    "    slope_features_df = pd.DataFrame.from_dict(slopes, orient='index')\n",
    "\n",
    "    # Rename columns to have slope_ prefix\n",
    "    slope_features_df = slope_features_df.rename(columns={f: f'slope_{f}' for f in slope_features_df.columns})\n",
    "\n",
    "    # Reset index to have customer_id as a column\n",
    "    slope_features_df = slope_features_df.reset_index().rename(columns={'index': 'customer_id'})\n",
    "\n",
    "    # Merge with current data used for modelling.\n",
    "    customers_df = pd.merge(\n",
    "        customers_df,\n",
    "        slope_features_df,\n",
    "        on=\"customer_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d5fc9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_statistics_features(customers_df, transactions_df, observed_date, feature_list):\n",
    "\n",
    "    time_windows = [\"all_time\", \"30d\", \"60d\", \"90d\"]\n",
    "\n",
    "    all_stats_df_list = []\n",
    "\n",
    "    for time_window in time_windows:\n",
    "\n",
    "        if time_window == \"all_time\":\n",
    "            filtered_transactions_df = transactions_df\n",
    "        else:\n",
    "            # Limit data to the new cutoff\n",
    "            days = int(time_window.strip(\"d\"))\n",
    "            filtered_transactions_df = transactions_df[\n",
    "                (transactions_df['transaction_date'] <= observed_date - pd.Timedelta(days=days))\n",
    "            ]\n",
    "\n",
    "        customers_list = filtered_transactions_df['customer_id'].unique()\n",
    "        stats_dict = {}\n",
    "\n",
    "        for customer_id in customers_list:\n",
    "\n",
    "            customer_transactions = filtered_transactions_df[\n",
    "                filtered_transactions_df['customer_id'] == customer_id\n",
    "            ]\n",
    "\n",
    "            stats_dict[customer_id] = {}\n",
    "\n",
    "            for feature_name in feature_list:\n",
    "\n",
    "                y = customer_transactions[feature_name].dropna().values\n",
    "\n",
    "                if len(y) < 2:\n",
    "                    # Less than 2 observations -> return NaN for all stats\n",
    "                    stats_dict[customer_id][f\"min_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"mean_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"mode_{feature_name}\"] = np.nan\n",
    "                    stats_dict[customer_id][f\"max_{feature_name}\"] = np.nan\n",
    "                    for q in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]:\n",
    "                        stats_dict[customer_id][f\"q{q}_{feature_name}\"] = np.nan\n",
    "                    continue\n",
    "\n",
    "                # Compute stats\n",
    "                stats_dict[customer_id][f\"min_{feature_name}\"] = np.min(y)\n",
    "                stats_dict[customer_id][f\"mean_{feature_name}\"] = np.mean(y)\n",
    "\n",
    "                # Compute mode safely\n",
    "                mode_result = stats.mode(y, nan_policy='omit')\n",
    "                if hasattr(mode_result.mode, \"__len__\"):\n",
    "                    # old SciPy: mode is array\n",
    "                    mode_val = mode_result.mode[0] if len(mode_result.mode) > 0 else np.nan\n",
    "                else:\n",
    "                    # new SciPy: mode is scalar\n",
    "                    mode_val = mode_result.mode if mode_result.count > 0 else np.nan\n",
    "\n",
    "                stats_dict[customer_id][f\"mode_{feature_name}\"] = mode_val\n",
    "\n",
    "                stats_dict[customer_id][f\"max_{feature_name}\"] = np.max(y)\n",
    "\n",
    "                # Quantiles\n",
    "                for q in [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]:\n",
    "                    stats_dict[customer_id][f\"q{q}_{feature_name}\"] = np.percentile(y, q)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        stats_df = pd.DataFrame.from_dict(stats_dict, orient='index').reset_index().rename(columns={'index': 'customer_id'})\n",
    "        all_stats_df_list.append(stats_df)\n",
    "\n",
    "    # Merge with customers_df (only keep last time_window stats)\n",
    "    final_stats_df = all_stats_df_list[-1]  # or merge all windows if needed\n",
    "    customers_df = pd.merge(customers_df, final_stats_df, on='customer_id', how='left')\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bceb6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_base(\n",
    "    seed_customers_path,\n",
    "    seed_transactions_path,\n",
    "    train_snapshot_date,\n",
    "    churn_windows=[30, 60, 90],\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads raw data, transforms it, limits it to modeling window,\n",
    "    builds customer modeling table, and adds churn labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Read data ---\n",
    "    customers_df = pd.read_csv(seed_customers_path)\n",
    "    transactions_df = pd.read_csv(seed_transactions_path)\n",
    "\n",
    "    # --- Transform data ---\n",
    "    transactions_df = transform_transactions_df(transactions_df)\n",
    "    customers_df = transform_customers_df(customers_df)\n",
    "\n",
    "    # --- Derive MAX_DATA_DATE internally ---\n",
    "    max_data_date = transactions_df[\"transaction_date\"].max()\n",
    "\n",
    "    # --- Limit transactions to snapshot ---\n",
    "    transactions_modeling_df = transactions_df.loc[\n",
    "        transactions_df[\"transaction_date\"] <= train_snapshot_date\n",
    "    ]\n",
    "\n",
    "    # --- Build customer modeling base ---\n",
    "    customers_modeling_df = (\n",
    "        pd.DataFrame({\n",
    "            \"customer_id\": transactions_modeling_df[\"customer_id\"].unique()\n",
    "        })\n",
    "        .merge(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "        .drop(columns=[\"signup_date\", \"true_lifetime_days\", \"termination_date\"])\n",
    "    )\n",
    "\n",
    "    # --- Add churn labels ---\n",
    "    for nday in churn_windows:\n",
    "        var_name = f\"is_churn_{nday}_days\"\n",
    "        observed_date = max_data_date - pd.Timedelta(days=nday)\n",
    "\n",
    "        customers_modeling_df = add_churn_status(\n",
    "            transformed_customers_df=customers_df,\n",
    "            observed_date=observed_date,\n",
    "            desired_df=None,\n",
    "        )\n",
    "\n",
    "        customers_modeling_df = customers_modeling_df.rename(columns={'is_churn': var_name})\n",
    "\n",
    "    return transactions_modeling_df, customers_modeling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037ba02",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c17d3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_in_df_cols(df):\n",
    "    # Get relative percentage of nulls by column\n",
    "    null_features_proportion = (\n",
    "        df.isna().sum() / len(df)\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    high_proportion = []\n",
    "    medium_proportion = []\n",
    "    low_proportion = []\n",
    "\n",
    "    for feature, proportion in null_features_proportion.items():\n",
    "        if proportion >= 0.20:\n",
    "            high_proportion.append(feature)\n",
    "        elif 0.05 <= proportion < 0.20:\n",
    "            medium_proportion.append(feature)\n",
    "        else:\n",
    "            low_proportion.append(feature)\n",
    "\n",
    "    # Build features DataFrame\n",
    "    features_df = null_features_proportion.reset_index()\n",
    "    features_df.columns = [\"feature\", \"nan_proportion\"]\n",
    "\n",
    "    features_df[\"NaN group\"] = features_df[\"feature\"].apply(\n",
    "        lambda f: (\n",
    "            \"High\" if f in high_proportion\n",
    "            else \"Medium\" if f in medium_proportion\n",
    "            else \"Low\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Print counts (same behavior as before)\n",
    "    print(\"Total features:\", len(df.columns))\n",
    "    print(\"Information on NaN values\")\n",
    "    print(\"====================================\")\n",
    "    print(\"Number of High Proportion Features:\", len(high_proportion))\n",
    "    print(\"Number of Medium Proportion Features:\", len(medium_proportion))\n",
    "    print(\"Number of Low Proportion Features:\", len(low_proportion))\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "565f059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_X_csv(X_by_target, BASE_GOLD_DIR):\n",
    "\n",
    "    for target in X_by_target.keys():\n",
    "\n",
    "        target_dir = BASE_GOLD_DIR / target\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_by_target[target].to_csv(\n",
    "            target_dir / \"X_train.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "        print(f\"[{target}] written to {target_dir}\")\n",
    "    \n",
    "    return \"All data saved successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c299d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_y_csv(\n",
    "        X_by_target,\n",
    "        y,\n",
    "        BASE_GOLD_DIR\n",
    "    ):\n",
    "\n",
    "    for target in targets:\n",
    "        target_dir = BASE_GOLD_DIR / target\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ----------------------------\n",
    "        # TRAIN labels\n",
    "        # ----------------------------\n",
    "        y.loc[\n",
    "            X_by_target[target].index, target\n",
    "        ].to_csv(\n",
    "            target_dir / \"y_train.csv\",\n",
    "            header=True,\n",
    "        )\n",
    "    \n",
    "    return \"All data saved successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7a562bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_features_csv(df, split, base_gold_dir, index_name='customer_id'):\n",
    "    path = Path(base_gold_dir) / \"raw\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"WRITING TO:\", path.resolve())\n",
    "\n",
    "    df.index.name = index_name\n",
    "    df.to_csv(\n",
    "        path / f\"{split}_features.csv\",\n",
    "        index=True, # keep customer_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f5d2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_by_target_csv(X_by_target, split, base_gold_dir, index_name='customer_id'):\n",
    "\n",
    "    for target, df in X_by_target.items():\n",
    "        \n",
    "        base_path = Path(base_gold_dir) / \"transformed\" / target\n",
    "        base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df.index.name = index_name\n",
    "        df.to_csv(\n",
    "            base_path / f\"X_{split}.csv\",\n",
    "            index=True,  # keep customer_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7665363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformed(BASE_GOLD_DIR, split, target):\n",
    "    return pd.read_csv(\n",
    "        BASE_GOLD_DIR / \"transformed\" / target / f\"X_{split}.csv\",\n",
    "        index_col=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9071f3",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1e47c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_feature_selection(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    target,\n",
    "    cutoff=0.0,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform mutual informationâ€“based feature selection for a given target.\n",
    "\n",
    "    Returns:\n",
    "        selected_df: DataFrame with selected features\n",
    "        mi_scores: DataFrame with MI scores per feature\n",
    "        selected_features: Index of selected feature names\n",
    "    \"\"\"\n",
    "\n",
    "    assert X_train.index.equals(y_train.index)\n",
    "\n",
    "    mi_train = mutual_info_classif(\n",
    "        X_train,\n",
    "        y_train[target],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    mi_scores = (\n",
    "        pd.DataFrame(\n",
    "            mi_train,\n",
    "            index=X_train.columns,\n",
    "            columns=[\"mutual_info\"]\n",
    "        )\n",
    "        .sort_values(by=\"mutual_info\", ascending=False)\n",
    "    )\n",
    "\n",
    "    selected_features = mi_scores.loc[\n",
    "        mi_scores[\"mutual_info\"] > cutoff\n",
    "    ].index\n",
    "\n",
    "    selected_df = X_train[selected_features]\n",
    "\n",
    "    return selected_df, mi_scores, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9047f",
   "metadata": {},
   "source": [
    "### Feature Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a08c5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_time_features(transactions_df):\n",
    "    \"\"\"\n",
    "    Add time-based and order-based transaction features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions_df : pd.DataFrame\n",
    "        Must contain: customer_id, transaction_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of transactions_df with added features\n",
    "    \"\"\"\n",
    "\n",
    "    df = transactions_df.sort_values(\n",
    "        [\"customer_id\", \"transaction_date\"]\n",
    "    ).copy()\n",
    "\n",
    "    df[\"customer_transaction_order\"] = (\n",
    "        df.groupby(\"customer_id\").cumcount()\n",
    "    )\n",
    "\n",
    "    df[\"prev_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"].shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"next_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"].shift(-1)\n",
    "    )\n",
    "\n",
    "    df[\"days_since_previous_transaction\"] = (\n",
    "        df[\"transaction_date\"] - df[\"prev_transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    df[\"days_until_next_transaction\"] = (\n",
    "        df[\"next_transaction_date\"] - df[\"transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    df[\"first_transaction_date\"] = (\n",
    "        df.groupby(\"customer_id\")[\"transaction_date\"]\n",
    "        .transform(\"min\")\n",
    "    )\n",
    "\n",
    "    df[\"days_since_first_transaction\"] = (\n",
    "        df[\"transaction_date\"] - df[\"first_transaction_date\"]\n",
    "    ).dt.days\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "344fab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_customer_features(\n",
    "    transactions_modeling_df,\n",
    "    customers_modeling_df,\n",
    "    observed_date,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Build raw customer-level features from transactions and customers data.\n",
    "    No imputing, scaling, or selection is performed here.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Transaction-level features\n",
    "    transactions_df = add_transaction_time_features(\n",
    "        transactions_modeling_df\n",
    "    )\n",
    "\n",
    "    # 2. RFM window features\n",
    "    customers_df = get_rfm_window_features(\n",
    "        customers_df=customers_modeling_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "    )\n",
    "\n",
    "    # 3. Activity trend (slopes)\n",
    "    customers_df = get_slope_features(\n",
    "        customers_df=customers_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # 4. Transaction statistics\n",
    "    customers_df = get_transaction_statistics_features(\n",
    "        customers_df=customers_df,\n",
    "        transactions_df=transactions_df,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    return customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "779a502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_numeric_transformers(\n",
    "    X_train_numeric_df,\n",
    "    imputer_params=None,\n",
    "    scaler_params=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit numeric imputer and scaler on training data only.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_numeric_imputed_scaled_df : pd.DataFrame\n",
    "    numeric_imputer : fitted IterativeImputer\n",
    "    scaler : fitted StandardScaler\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Defaults\n",
    "    # -------------------------------\n",
    "    if imputer_params is None:\n",
    "        imputer_params = dict(\n",
    "            estimator=LinearRegression(),\n",
    "            max_iter=20,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    if scaler_params is None:\n",
    "        scaler_params = {}\n",
    "\n",
    "    # -------------------------------\n",
    "    # Imputation (FIT)\n",
    "    # -------------------------------\n",
    "    numeric_imputer = IterativeImputer(**imputer_params)\n",
    "    X_train_numeric_imputed = numeric_imputer.fit_transform(X_train_numeric_df)\n",
    "\n",
    "    X_train_numeric_imputed_df = pd.DataFrame(\n",
    "        X_train_numeric_imputed,\n",
    "        columns=X_train_numeric_df.columns,\n",
    "        index=X_train_numeric_df.index,\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Scaling (FIT)\n",
    "    # -------------------------------\n",
    "    scaler = StandardScaler(**scaler_params)\n",
    "    X_train_numeric_imputed_scaled = scaler.fit_transform(\n",
    "        X_train_numeric_imputed_df\n",
    "    )\n",
    "\n",
    "    X_train_numeric_imputed_scaled_df = pd.DataFrame(\n",
    "        X_train_numeric_imputed_scaled,\n",
    "        columns=X_train_numeric_df.columns,\n",
    "        index=X_train_numeric_df.index,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8ae6fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_customers_numeric_features(\n",
    "    X_numeric,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply fitted numeric imputer and scaler.\n",
    "    \"\"\"\n",
    "\n",
    "    X_numeric_imputed = numeric_imputer.transform(X_numeric)\n",
    "    X_numeric_imputed_df = pd.DataFrame(\n",
    "        X_numeric_imputed,\n",
    "        columns=X_numeric.columns,\n",
    "        index=X_numeric.index,\n",
    "    )\n",
    "\n",
    "    X_numeric_scaled = scaler.transform(X_numeric_imputed_df)\n",
    "    X_numeric_scaled_df = pd.DataFrame(\n",
    "        X_numeric_scaled,\n",
    "        columns=X_numeric.columns,\n",
    "        index=X_numeric.index,\n",
    "    )\n",
    "\n",
    "    return X_numeric_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "14da35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_per_target(\n",
    "    X_train_transformed_df,\n",
    "    y_train,\n",
    "    targets,\n",
    "    artifact_dir=None,\n",
    "    cutoff=0.0,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform feature selection per target using mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    assert X_train_transformed_df.index.equals(y_train.index), (\n",
    "        \"X_train and y_train must be index-aligned\"\n",
    "    )\n",
    "\n",
    "    X_train_by_target = {}\n",
    "    selected_features_by_target = {}\n",
    "    mi_scores_by_target = {}\n",
    "\n",
    "    for target in targets:\n",
    "        X_selected_df, mi_scores, selected_features = (\n",
    "            mutual_information_feature_selection(\n",
    "                X_train=X_train_transformed_df,\n",
    "                y_train=y_train,\n",
    "                target=target,\n",
    "                cutoff=cutoff,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if artifact_dir is not None:\n",
    "            with open(\n",
    "                artifact_dir / f\"selected_features_{target}.json\",\n",
    "                \"w\",\n",
    "            ) as f:\n",
    "                json.dump(list(selected_features), f)\n",
    "\n",
    "        X_train_by_target[target] = X_selected_df\n",
    "        selected_features_by_target[target] = list(selected_features)\n",
    "        mi_scores_by_target[target] = mi_scores\n",
    "\n",
    "        print(f\"[{target}] selected {len(selected_features)} features\")\n",
    "\n",
    "    return (\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "88607532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_per_target(\n",
    "    X_transformed_df,\n",
    "    selected_features_by_target\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform feature selection per target using mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    for target, selected_features in selected_features_by_target.items():\n",
    "\n",
    "        missing_features = set(selected_features) - set(\n",
    "            X_transformed_df.columns\n",
    "        )\n",
    "        if missing_features:\n",
    "            raise ValueError(\n",
    "                f\"Missing selected features at inference time: {missing_features}\"\n",
    "            )\n",
    "\n",
    "        X_selected_features = X_transformed_df[selected_features]\n",
    "        X_by_target[target] = X_selected_features\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f4133c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(\n",
    "    customers_modeling_df,\n",
    "    targets,\n",
    "    test_size=0.33,\n",
    "    val_size=0.33,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split customer modeling dataframe into train / val / test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    customers_modeling_df : pd.DataFrame\n",
    "        Must contain customer_id and target columns.\n",
    "    targets : list[str]\n",
    "        Target column names.\n",
    "    test_size : float\n",
    "        Proportion of data used for test+val split.\n",
    "    val_size : float\n",
    "        Proportion of test split used for validation.\n",
    "    random_state : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Feature / target separation\n",
    "    # -------------------------------\n",
    "    X_df = customers_modeling_df.drop(columns=targets)\n",
    "    X_df = X_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    y_df = customers_modeling_df[[\"customer_id\"] + targets]\n",
    "    y_df = y_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Train / temp split\n",
    "    # -------------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_df,\n",
    "        y_df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Test / validation split\n",
    "    # -------------------------------\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6ff002b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_customer_features_pipeline_train(\n",
    "    transactions_modeling_df,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    observed_date,\n",
    "    targets,\n",
    "    ARTIFACT_DIR=None,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for TRAIN data.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Build raw customer features\n",
    "    # --------------------------------------------------\n",
    "    X_train_raw_features_df = build_customer_features(\n",
    "        transactions_modeling_df=transactions_modeling_df,\n",
    "        customers_modeling_df=X_train,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Numeric transform (impute + scale)\n",
    "    # --------------------------------------------------\n",
    "    X_train_raw_features_df = X_train_raw_features_df.set_index(\"customer_id\", drop=False)\n",
    "    X_train_raw_features_numeric_df = X_train_raw_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    numeric_imputer, scaler = fit_numeric_transformers(\n",
    "        X_train_raw_features_numeric_df,\n",
    "        imputer_params=None,\n",
    "        scaler_params=None,\n",
    "    )\n",
    "\n",
    "    X_train_transformed_df = transform_customers_numeric_features(\n",
    "        X_train_raw_features_numeric_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Feature selection per target (EXTRACTED)\n",
    "    # --------------------------------------------------\n",
    "    (\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "    ) = select_features_per_target(\n",
    "        X_train_transformed_df=X_train_transformed_df,\n",
    "        y_train=y_train,\n",
    "        targets=targets,\n",
    "        artifact_dir=ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Save transformers ONCE\n",
    "    # --------------------------------------------------\n",
    "    if ARTIFACT_DIR is not None:\n",
    "        joblib.dump(\n",
    "            numeric_imputer,\n",
    "            ARTIFACT_DIR / \"numeric_imputer.joblib\",\n",
    "        )\n",
    "        joblib.dump(\n",
    "            scaler,\n",
    "            ARTIFACT_DIR / \"scaler.joblib\",\n",
    "        )\n",
    "\n",
    "    X_train_raw_features_df = X_train_raw_features_df.drop(columns=['customer_id'])\n",
    "\n",
    "    return (\n",
    "        X_train_raw_features_df,\n",
    "        X_train_by_target,\n",
    "        selected_features_by_target,\n",
    "        mi_scores_by_target,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3fac144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_customer_features_pipeline_test(\n",
    "    transactions_modeling_df,\n",
    "    X_test,\n",
    "    observed_date,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for TEST / VAL / INFERENCE data.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Build raw customer-level features from transactions\n",
    "    2. Remove customer_id from feature space\n",
    "    3. Apply fitted numeric transformations (imputer + scaler)\n",
    "    4. Select precomputed feature subset (STRICT reuse)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Build raw customer features\n",
    "    # --------------------------------------------------\n",
    "    X_test_features_df = build_customer_features(\n",
    "        transactions_modeling_df=transactions_modeling_df,\n",
    "        customers_modeling_df=X_test,\n",
    "        observed_date=observed_date,\n",
    "        feature_list=feature_list,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Set customer_id as index and REMOVE from features\n",
    "    # --------------------------------------------------\n",
    "    if \"customer_id\" not in X_test_features_df.columns:\n",
    "        raise ValueError(\"customer_id column missing after feature building\")\n",
    "\n",
    "    X_test_features_df = X_test_features_df.set_index(\"customer_id\", drop=True)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Select numeric features and enforce column order\n",
    "    # --------------------------------------------------\n",
    "    X_test_numeric_features_df = X_test_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Enforce training-time column order (critical for IterativeImputer)\n",
    "    X_test_numeric_features_df = X_test_numeric_features_df[\n",
    "        numeric_imputer.feature_names_in_\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Apply fitted numeric transformations (NO FIT)\n",
    "    # --------------------------------------------------\n",
    "    X_test_numeric_features_transformed_df = transform_customers_numeric_features(\n",
    "        X_test_numeric_features_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Feature selection (STRICT reuse)\n",
    "    # --------------------------------------------------\n",
    "    missing_features = set(selected_features) - set(\n",
    "        X_test_numeric_features_transformed_df.columns\n",
    "    )\n",
    "    if missing_features:\n",
    "        raise ValueError(\n",
    "            f\"Missing selected features at inference time: {missing_features}\"\n",
    "        )\n",
    "\n",
    "    X_test_final_df = X_test_numeric_features_transformed_df[selected_features]\n",
    "\n",
    "    return X_test_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d09650da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_select_for_multiple_targets_test(\n",
    "    X_test_raw_features_df,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features_by_target\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and transform customer features for multiple targets\n",
    "    (test / val / inference).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_by_target : dict[str, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    # Select numeric features and enforce column order\n",
    "    X_test_numeric_features_df = X_test_raw_features_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Enforce training-time column order (critical for IterativeImputer)\n",
    "    X_test_numeric_features_df = X_test_numeric_features_df[\n",
    "        numeric_imputer.feature_names_in_\n",
    "    ]\n",
    "\n",
    "    X_test_transformed_df = transform_customers_numeric_features(\n",
    "        X_test_numeric_features_df,\n",
    "        numeric_imputer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    X_by_target = get_features_per_target(\n",
    "        X_test_transformed_df,\n",
    "        selected_features_by_target\n",
    "    )\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fb4bc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_transform_for_multiple_targets(\n",
    "    transactions_modeling_df,\n",
    "    X_df,\n",
    "    observed_date,\n",
    "    numeric_imputer,\n",
    "    scaler,\n",
    "    selected_features_by_target,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and transform customer features for multiple targets\n",
    "    (test / val / inference).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_by_target : dict[str, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    X_by_target = {}\n",
    "\n",
    "    for target, selected_features in selected_features_by_target.items():\n",
    "        X_by_target[target] = build_and_transform_customer_features_pipeline_test(\n",
    "            transactions_modeling_df=transactions_modeling_df,\n",
    "            X_test=X_df,\n",
    "            observed_date=observed_date,\n",
    "            numeric_imputer=numeric_imputer,\n",
    "            scaler=scaler,\n",
    "            selected_features=selected_features,\n",
    "            feature_list=[\n",
    "                \"amount\",\n",
    "                \"days_since_previous_transaction\",\n",
    "                \"days_until_next_transaction\",\n",
    "                \"customer_transaction_order\",\n",
    "                \"days_since_first_transaction\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return X_by_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7a1e0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9a5b2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lgb_feature_importance(\n",
    "    model,\n",
    "    importance_type=\"gain\",   # \"gain\" or \"split\"\n",
    "    normalize=False,\n",
    "    top_n=None,\n",
    "    title=None,\n",
    "    height=600,\n",
    "    as_percent=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot LightGBM feature importance for sklearn API models.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract feature names ---\n",
    "    if hasattr(model, \"feature_name_\"):\n",
    "        features = model.feature_name_\n",
    "    else:\n",
    "        raise ValueError(\"Model does not contain feature names\")\n",
    "\n",
    "    # --- Extract importance correctly ---\n",
    "    if importance_type == \"split\":\n",
    "        importance = model.feature_importances_\n",
    "    elif importance_type == \"gain\":\n",
    "        importance = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "    else:\n",
    "        raise ValueError(\"importance_type must be 'gain' or 'split'\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"importance\": importance\n",
    "    })\n",
    "\n",
    "    # Remove zero-importance features\n",
    "    df = df[df[\"importance\"] > 0]\n",
    "\n",
    "    # --- Normalize if requested ---\n",
    "    if normalize:\n",
    "        total = df[\"importance\"].sum()\n",
    "        df[\"importance\"] = df[\"importance\"] / total\n",
    "        if as_percent:\n",
    "            df[\"importance\"] *= 100\n",
    "            importance_label = \"Normalized Gain (%)\"\n",
    "            text_fmt = \".2f\"\n",
    "        else:\n",
    "            importance_label = \"Normalized Gain\"\n",
    "            text_fmt = \".4f\"\n",
    "    else:\n",
    "        importance_label = (\n",
    "            \"Gain\" if importance_type == \"gain\" else \"Split Count\"\n",
    "        )\n",
    "        text_fmt = \".2f\"\n",
    "\n",
    "    # Sort and keep top N\n",
    "    df = df.sort_values(\"importance\", ascending=False)\n",
    "    if top_n is not None:\n",
    "        df = df.head(top_n)\n",
    "\n",
    "    # Reverse for horizontal bar chart\n",
    "    df = df.sort_values(\"importance\", ascending=True)\n",
    "\n",
    "    if title is None:\n",
    "        norm_tag = \" (Normalized)\" if normalize else \"\"\n",
    "        title = f\"LightGBM Feature Importance ({importance_type.capitalize()}){norm_tag}\"\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x=\"importance\",\n",
    "        y=\"feature\",\n",
    "        orientation=\"h\",\n",
    "        title=title,\n",
    "        labels={\n",
    "            \"importance\": importance_label,\n",
    "            \"feature\": \"Feature\"\n",
    "        },\n",
    "        text=df[\"importance\"]\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        texttemplate=f\"%{{text:{text_fmt}}}\",\n",
    "        textposition=\"outside\",\n",
    "        cliponaxis=False\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        yaxis=dict(categoryorder=\"total ascending\"),\n",
    "        margin=dict(r=120)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5df83d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_model(model, X, y, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a binary classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    y_proba = model.predict(X, num_iteration=model.best_iteration_)\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(y, y_proba),\n",
    "        \"pr_auc\": average_precision_score(y, y_proba),\n",
    "        \"confusion_matrix\": confusion_matrix(y, y_pred)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a2c17977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_styled_df_confusion_matrix(cm):\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"Actual 0\", \"Actual 1\"],\n",
    "        columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "    )\n",
    "\n",
    "    styled_df = (\n",
    "        cm_df.style\n",
    "        .background_gradient(cmap=\"Blues\")\n",
    "        .format(\"{:.0f}\")\n",
    "    )\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "def32af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, X_val, y_val, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a binary classifier on train, validation, and test sets.\n",
    "    Prints:\n",
    "    - ROC-AUC\n",
    "    - PR-AUC (Precisionâ€“Recall)\n",
    "    - Accuracy\n",
    "    - Confusion Matrix\n",
    "    - Classification Report\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "\n",
    "    for split_name, X, y in [\n",
    "        (\"TRAIN\", X_train, y_train),\n",
    "        (\"TEST\", X_test, y_test),\n",
    "        (\"VALIDATION\", X_val, y_val),\n",
    "    ]:\n",
    "        # Predicted probabilities and labels\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Metrics\n",
    "        roc_auc = roc_auc_score(y, y_proba)\n",
    "        pr_auc = average_precision_score(y, y_proba)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[\"Actual 0\", \"Actual 1\"],\n",
    "            columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{split_name}\")\n",
    "        print(\"-\" * len(split_name))\n",
    "        print(f\"ROC-AUC:      {roc_auc:.4f}\")\n",
    "        print(f\"PR-AUC:       {pr_auc:.4f}\")\n",
    "        print(f\"Accuracy:     {acc:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm_df)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "47411abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    target,\n",
    "    dataset_version,\n",
    "):\n",
    "    param_grid = {\n",
    "        \"num_leaves\": [31, 63],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [-1, 6],\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=3,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train[target])\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # ---------- Validation predictions ----------\n",
    "    val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    val_pred = (val_proba >= 0.5).astype(int)  # explicit threshold\n",
    "\n",
    "    # ---------- Metrics ----------\n",
    "    roc_auc = roc_auc_score(y_val[target], val_proba)\n",
    "    pr_auc = average_precision_score(y_val[target], val_proba)\n",
    "    precision = precision_score(y_val[target], val_pred)\n",
    "    recall = recall_score(y_val[target], val_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val[target], val_pred)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_0\", \"actual_1\"],\n",
    "        columns=[\"pred_0\", \"pred_1\"],\n",
    "    )\n",
    "\n",
    "    # ---------- MLflow ----------\n",
    "    input_example = X_train.iloc[:5]\n",
    "    signature = infer_signature(\n",
    "        X_train,\n",
    "        best_model.predict_proba(X_train)[:, 1],\n",
    "    )\n",
    "\n",
    "    mlflow.log_param(\"dataset_version\", dataset_version)\n",
    "    mlflow.log_param(\"target\", target)\n",
    "    mlflow.log_params(grid.best_params_)\n",
    "\n",
    "    mlflow.log_metric(\"val_roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"val_pr_auc\", pr_auc)\n",
    "    mlflow.log_metric(\"val_precision\", precision)\n",
    "    mlflow.log_metric(\"val_recall\", recall)\n",
    "\n",
    "    mlflow.log_text(\n",
    "        cm_df.to_string(),\n",
    "        artifact_file=f\"confusion_matrix/{dataset_version}_{target}.txt\",\n",
    "    )\n",
    "\n",
    "    mlflow.lightgbm.log_model(\n",
    "        best_model,\n",
    "        name=f\"{dataset_version}_{target}\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f0d4f",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9d561c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promote_to_production(run_id):\n",
    "    client.set_tag(run_id, \"stage\", \"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "299475ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_runs():\n",
    "    return mlflow.search_runs(\n",
    "        filter_string=\"tags.stage = 'production'\",\n",
    "        search_all_experiments=True,\n",
    "        output_format=\"pandas\",\n",
    "    )\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "997a483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_production_models():\n",
    "    prod_runs = get_production_runs()\n",
    "\n",
    "    models = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for _, row in prod_runs.iterrows():\n",
    "        target = row[\"params.target\"]\n",
    "        dataset_version = row[\"params.dataset_version\"]\n",
    "        run_id = row[\"run_id\"]\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{dataset_version}_{target}\"\n",
    "        model = mlflow.lightgbm.load_model(model_uri)\n",
    "\n",
    "        models[target] = model\n",
    "        metadata[target] = {\n",
    "            \"dataset_version\": dataset_version,\n",
    "            \"run_id\": run_id,\n",
    "        }\n",
    "\n",
    "    return models, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a732b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_features(\n",
    "    customer_ids,\n",
    "    target,\n",
    "    metadata,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "):\n",
    "    if isinstance(customer_ids, str):\n",
    "        customer_ids = [customer_ids]\n",
    "\n",
    "    dataset_version = metadata[target][\"dataset_version\"]\n",
    "\n",
    "    if dataset_version == \"raw\":\n",
    "        X = raw_features_df.loc[customer_ids]\n",
    "    elif dataset_version == \"transformed\":\n",
    "        X = transformed_features_by_target[target].loc[customer_ids]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset version: {dataset_version}\")\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "705ac070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn(\n",
    "    customer_id: str,\n",
    "    horizon_days: int,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "    models,\n",
    "    metadata,\n",
    "):\n",
    "    # ------------------\n",
    "    # Validate horizon\n",
    "    # ------------------\n",
    "\n",
    "    target = f\"is_churn_{horizon_days}_days\"\n",
    "\n",
    "    if target not in models:\n",
    "        raise KeyError(f\"No production model loaded for target: {target}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Select features\n",
    "    # ------------------\n",
    "    X = get_customer_features(\n",
    "        customer_ids=[customer_id],\n",
    "        target=target,\n",
    "        metadata=metadata,\n",
    "        raw_features_df=raw_features_df,\n",
    "        transformed_features_by_target=transformed_features_by_target,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Predict\n",
    "    # ------------------\n",
    "    model = models[target]\n",
    "    churn_prob = float(model.predict_proba(X)[0, 1])\n",
    "\n",
    "    # ------------------\n",
    "    # Risk labeling (explicit, adjustable)\n",
    "    # ------------------\n",
    "    if churn_prob >= 0.7:\n",
    "        churn_label = \"high_risk\"\n",
    "    elif churn_prob >= 0.4:\n",
    "        churn_label = \"medium_risk\"\n",
    "    else:\n",
    "        churn_label = \"low_risk\"\n",
    "\n",
    "    return {\n",
    "        \"churn_probability\": round(churn_prob, 4),\n",
    "        \"churn_label\": churn_label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "86449d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churns(\n",
    "    customer_ids: list[str],\n",
    "    horizon_days: int,\n",
    "    raw_features_df,\n",
    "    transformed_features_by_target,\n",
    "    models,\n",
    "    metadata,\n",
    "):\n",
    "    # ------------------\n",
    "    # Validate horizon\n",
    "    # ------------------\n",
    "\n",
    "    target = f\"is_churn_{horizon_days}_days\"\n",
    "\n",
    "    if target not in models:\n",
    "        raise KeyError(f\"No production model loaded for target: {target}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Feature extraction (BULK)\n",
    "    # ------------------\n",
    "    X = get_customer_features(\n",
    "        customer_ids=customer_ids,\n",
    "        target=target,\n",
    "        metadata=metadata,\n",
    "        raw_features_df=raw_features_df,\n",
    "        transformed_features_by_target=transformed_features_by_target,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Predict (BULK)\n",
    "    # ------------------\n",
    "    model = models[target]\n",
    "    churn_probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # ------------------\n",
    "    # Risk labeling (vectorized)\n",
    "    # ------------------\n",
    "    churn_labels = np.where(\n",
    "        churn_probs >= 0.7,\n",
    "        \"high_risk\",\n",
    "        np.where(\n",
    "            churn_probs >= 0.4,\n",
    "            \"medium_risk\",\n",
    "            \"low_risk\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Output (aligned, explicit)\n",
    "    # ------------------\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"customer_id\": customer_ids,\n",
    "                \"churn_probability\": churn_probs.round(4),\n",
    "                \"churn_label\": churn_labels,\n",
    "            }\n",
    "        )\n",
    "        .set_index(\"customer_id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7caeac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(\n",
    "        dataset_version,\n",
    "        gold_data_version=MAX_DATA_DATE_STR,\n",
    "        gold_dir=\"default\",\n",
    "        targets=targets\n",
    "    ):\n",
    "    '''\n",
    "        The service preloads the feature dataframes for faster search.\n",
    "    '''\n",
    "    if gold_dir == \"default\":\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        gold_dir = PROJECT_ROOT / \"data\" / \"gold\" / gold_data_version\n",
    "    \n",
    "    if dataset_version == \"raw\":\n",
    "        feature_df = pd.read_csv(gold_dir / dataset_version / \"all_features.csv\", index_col=0)\n",
    "        return feature_df\n",
    "    elif dataset_version == \"transformed\":\n",
    "        X_by_target = {}\n",
    "        for target in targets:\n",
    "            feature_df = pd.read_csv(gold_dir / dataset_version / target / \"X_all.csv\", index_col=0)\n",
    "            X_by_target[target] = feature_df\n",
    "        return X_by_target\n",
    "    else:\n",
    "        return \"Invalid dataset version. Please use only `raw` and `transformed`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81606604",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb94d8",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44e4f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_duration_event(\n",
    "    customers_df: pd.DataFrame,\n",
    "    obs_end_date: pd.Timestamp,\n",
    "    start_col: str = \"signup_date\",\n",
    "    termination_col: str = \"termination_date\",\n",
    ") -> pd.DataFrame:\n",
    "    df = customers_df.copy()\n",
    "\n",
    "    # Ensure datetime\n",
    "    df[start_col] = pd.to_datetime(df[start_col])\n",
    "    df[termination_col] = pd.to_datetime(df[termination_col])\n",
    "\n",
    "    # Event indicator: churn happened by obs_end_date\n",
    "    df[\"E\"] = (\n",
    "        df[termination_col].notna()\n",
    "        & (df[termination_col] <= obs_end_date)\n",
    "    ).astype(int)\n",
    "\n",
    "    # End date for duration calculation\n",
    "    df[\"end_date\"] = df[termination_col].where(\n",
    "        df[\"E\"] == 1,\n",
    "        obs_end_date,\n",
    "    )\n",
    "\n",
    "    # Duration (in days)\n",
    "    df[\"T\"] = (df[\"end_date\"] - df[start_col]).dt.days\n",
    "\n",
    "    # Safety checks\n",
    "    if (df[\"T\"] < 0).any():\n",
    "        raise ValueError(\"Negative durations found â€” check date logic\")\n",
    "\n",
    "    return df.drop(columns=[\"end_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "287187af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_to_churn_proba(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    horizon_days: int,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns P(churn within horizon_days)\n",
    "    \"\"\"\n",
    "\n",
    "    surv_fn = model.predict_survival_function(X)\n",
    "\n",
    "    probs = []\n",
    "    for i in range(surv_fn.shape[1]):\n",
    "        s = surv_fn.iloc[:, i]\n",
    "        s_h = (\n",
    "            s.loc[s.index <= horizon_days].iloc[-1]\n",
    "            if (s.index <= horizon_days).any()\n",
    "            else s.iloc[0]\n",
    "        )\n",
    "        probs.append(1 - s_h)\n",
    "\n",
    "    return pd.Series(probs, index=X.index, name=\"p_churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c2ef3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3f289521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_churn_predictions(\n",
    "    feature_df: pd.DataFrame,\n",
    "    target_df: pd.DataFrame | None = None,\n",
    "    threshold: float = 0.5,\n",
    "    target_col: str = \"is_churn\",\n",
    "    proba_col: str = \"p_churn\",\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_df: must contain [proba_col]\n",
    "    target_df:\n",
    "        - if provided: must contain [target_col]\n",
    "        - if None: target_col must already be in feature_df\n",
    "\n",
    "    Assumes customer_id is the index in both DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decide where labels come from\n",
    "    if target_df is None:\n",
    "        missing = {target_col, proba_col} - set(feature_df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(\n",
    "                f\"feature_df is missing required columns: {missing}\"\n",
    "            )\n",
    "        eval_df = feature_df.copy()\n",
    "\n",
    "    else:\n",
    "        missing_f = {proba_col} - set(feature_df.columns)\n",
    "        missing_t = {target_col} - set(target_df.columns)\n",
    "\n",
    "        if missing_f or missing_t:\n",
    "            raise ValueError(\n",
    "                f\"Missing columns â€” \"\n",
    "                f\"feature_df: {missing_f}, target_df: {missing_t}\"\n",
    "            )\n",
    "\n",
    "        # Index-based alignment\n",
    "        eval_df = feature_df.join(\n",
    "            target_df[[target_col]],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Binary predictions\n",
    "    eval_df[\"pred_churn\"] = (eval_df[proba_col] >= threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[proba_col]\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            eval_df[target_col],\n",
    "            eval_df[\"pred_churn\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        eval_df[target_col],\n",
    "        eval_df[\"pred_churn\"],\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"actual_no_churn\", \"actual_churn\"],\n",
    "        columns=[\"pred_no_churn\", \"pred_churn\"]\n",
    "    )\n",
    "\n",
    "    return metrics, cm_df, eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184821d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "af59c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_survival(\n",
    "    customer_id: str,\n",
    "    survival_df: pd.DataFrame,\n",
    "    model,\n",
    "    horizons: list[int] = [30, 60, 90],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns survival curve at given horizons + expected remaining lifetime\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract row\n",
    "    row = survival_df.loc[customer_id]\n",
    "\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Customer {customer_id} not found\")\n",
    "\n",
    "    X = row.drop(columns=[\"customer_id\", \"T\", \"E\"], errors=\"ignore\")\n",
    "\n",
    "    # Survival function (continuous)\n",
    "    surv_fn = model.predict_survival_function(X)\n",
    "\n",
    "    # Discrete horizon extraction\n",
    "    survival_curve = []\n",
    "    for h in horizons:\n",
    "        prob = float(\n",
    "            surv_fn.loc[surv_fn.index <= h].iloc[-1, 0]\n",
    "            if (surv_fn.index <= h).any()\n",
    "            else surv_fn.iloc[0, 0]\n",
    "        )\n",
    "        survival_curve.append(\n",
    "            {\"day\": h, \"prob\": round(prob, 4)}\n",
    "        )\n",
    "\n",
    "    # Expected remaining lifetime\n",
    "    expected_lifetime = float(\n",
    "        model.predict_expectation(X).iloc[0]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"survival_curve\": survival_curve,\n",
    "        \"expected_remaining_lifetime\": round(expected_lifetime, 2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b1178",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d149dc",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab68fb7",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23ed33",
   "metadata": {},
   "source": [
    "customers_df = pd.read_csv(f\"../{SEED_CUSTOMERS}\")\n",
    "transactions_df = pd.read_csv(f\"../{SEED_TRANSACTIONS}\")\n",
    "\n",
    "transactions_df = transform_transactions_df(transactions_df)\n",
    "customers_df = transform_customers_df(customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63a4faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_modeling_df, customers_modeling_df = build_training_base(\n",
    "    seed_customers_path=f\"../{SEED_CUSTOMERS}\",\n",
    "    seed_transactions_path=f\"../{SEED_TRANSACTIONS}\",\n",
    "    train_snapshot_date=CUTOFF_DATE,\n",
    "    churn_windows=[30],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "543b8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features_df = build_customer_features(\n",
    "    transactions_modeling_df,\n",
    "    customers_modeling_df,\n",
    "    observed_date=CUTOFF_DATE,\n",
    "    feature_list=[\n",
    "        \"amount\",\n",
    "        \"days_since_previous_transaction\",\n",
    "        \"days_until_next_transaction\",\n",
    "        \"customer_transaction_order\",\n",
    "        \"days_since_first_transaction\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_features_df = raw_features_df.drop(columns=['signup_date', 'termination_date', 'true_lifetime_days', 'is_churn_30_days'])\n",
    "#raw_features_df = raw_features_df.set_index('customer_id', drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c4597",
   "metadata": {},
   "source": [
    "### Get duration and event features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a709b8",
   "metadata": {},
   "source": [
    "Generate two requirements for Survival Analysis:\n",
    "- T: Customer tenure, defined as the number of days between the customers' signup date and the customer's last observed date.\n",
    "- E: Churn event, defined as the binary state whether the customer have churned before or at the last observed date.\n",
    "Just as before, \"churn\" time means the date where the customer stop interacting with the product,  `termination_date` computed using `true_lifetime_days`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3484c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_df = add_duration_event(\n",
    "    customers_df=raw_features_df,\n",
    "    obs_end_date=MAX_DATA_DATE,\n",
    "    start_col=\"signup_date\",\n",
    "    termination_col=\"termination_date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "487b8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create eval set\n",
    "label_df = survival_df[['customer_id', 'is_churn_30_days']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e5042413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target columns\n",
    "survival_df = survival_df.drop(columns=['signup_date', 'termination_date', 'true_lifetime_days', 'is_churn_30_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a69a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = label_df.set_index('customer_id', drop=True)\n",
    "survival_df = survival_df.set_index('customer_id', drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486091b9",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1fbe65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    c for c in survival_df.columns\n",
    "    if c not in {\"customer_id\", \"T\", \"E\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "97d83d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "survival_df[feature_cols] = num_imputer.fit_transform(\n",
    "    survival_df[feature_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d3678254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong-mai/venvs/maipy/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/hong-mai/venvs/maipy/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "corr_with_T = survival_df[feature_cols].corrwith(survival_df[\"T\"]).abs()\n",
    "feature_cols = corr_with_T[corr_with_T < 0.95].index.tolist()\n",
    "\n",
    "std = survival_df[feature_cols].std()\n",
    "feature_cols = std[std > 1e-6].index.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "survival_df[feature_cols] = scaler.fit_transform(\n",
    "    survival_df[feature_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "227cd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_df = survival_df[feature_cols + [\"T\", \"E\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ae0da",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "db0997b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_truths_df = survival_df.join(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "26ff0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_truths_df = customers_truths_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a535d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test\n",
    ") = split_train_test_val(\n",
    "    customers_modeling_df=customers_truths_df,\n",
    "    targets=targets,\n",
    "    test_size=0.33,\n",
    "    val_size=0.33,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d0572313",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs = {\n",
    "    'train': [X_train, y_train],\n",
    "    'test': [X_test, y_test],\n",
    "    'val': [X_val, y_val]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0eb27",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0c655",
   "metadata": {},
   "source": [
    "## Cox PH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125a18c",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b0f3a36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 3000 total observations, 1144 right-censored observations>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cph = CoxPHFitter(penalizer=0.1)\n",
    "cph.fit(\n",
    "    survival_df,\n",
    "    duration_col=\"T\",\n",
    "    event_col=\"E\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7eab2",
   "metadata": {},
   "source": [
    "### Predict Survival Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9d676cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_surv_curves = cph.predict_survival_function(\n",
    "    survival_df[feature_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0f63e37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C00000</th>\n",
       "      <th>C00001</th>\n",
       "      <th>C00002</th>\n",
       "      <th>C00003</th>\n",
       "      <th>C00004</th>\n",
       "      <th>C00005</th>\n",
       "      <th>C00006</th>\n",
       "      <th>C00007</th>\n",
       "      <th>C00008</th>\n",
       "      <th>C00009</th>\n",
       "      <th>...</th>\n",
       "      <th>C02990</th>\n",
       "      <th>C02991</th>\n",
       "      <th>C02992</th>\n",
       "      <th>C02993</th>\n",
       "      <th>C02994</th>\n",
       "      <th>C02995</th>\n",
       "      <th>C02996</th>\n",
       "      <th>C02997</th>\n",
       "      <th>C02998</th>\n",
       "      <th>C02999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>0.974221</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>9.141179e-01</td>\n",
       "      <td>9.471023e-01</td>\n",
       "      <td>9.550767e-01</td>\n",
       "      <td>9.485393e-01</td>\n",
       "      <td>9.654350e-01</td>\n",
       "      <td>9.538251e-01</td>\n",
       "      <td>0.993385</td>\n",
       "      <td>9.675878e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>9.474965e-01</td>\n",
       "      <td>9.413818e-01</td>\n",
       "      <td>9.611170e-01</td>\n",
       "      <td>8.973998e-01</td>\n",
       "      <td>9.576353e-01</td>\n",
       "      <td>0.989617</td>\n",
       "      <td>9.553734e-01</td>\n",
       "      <td>9.614131e-01</td>\n",
       "      <td>0.993743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31.0</th>\n",
       "      <td>0.972333</td>\n",
       "      <td>0.997094</td>\n",
       "      <td>9.080412e-01</td>\n",
       "      <td>9.432867e-01</td>\n",
       "      <td>9.518216e-01</td>\n",
       "      <td>9.448243e-01</td>\n",
       "      <td>9.629158e-01</td>\n",
       "      <td>9.504817e-01</td>\n",
       "      <td>0.992895</td>\n",
       "      <td>9.652226e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>9.437085e-01</td>\n",
       "      <td>9.371674e-01</td>\n",
       "      <td>9.582899e-01</td>\n",
       "      <td>8.902129e-01</td>\n",
       "      <td>9.545611e-01</td>\n",
       "      <td>0.988850</td>\n",
       "      <td>9.521392e-01</td>\n",
       "      <td>9.586070e-01</td>\n",
       "      <td>0.993280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32.0</th>\n",
       "      <td>0.971301</td>\n",
       "      <td>0.996984</td>\n",
       "      <td>9.047341e-01</td>\n",
       "      <td>9.412059e-01</td>\n",
       "      <td>9.500456e-01</td>\n",
       "      <td>9.427982e-01</td>\n",
       "      <td>9.615405e-01</td>\n",
       "      <td>9.486576e-01</td>\n",
       "      <td>0.992627</td>\n",
       "      <td>9.639312e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998990</td>\n",
       "      <td>9.416427e-01</td>\n",
       "      <td>9.348700e-01</td>\n",
       "      <td>9.567469e-01</td>\n",
       "      <td>8.863058e-01</td>\n",
       "      <td>9.528835e-01</td>\n",
       "      <td>0.988431</td>\n",
       "      <td>9.503746e-01</td>\n",
       "      <td>9.570755e-01</td>\n",
       "      <td>0.993026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.0</th>\n",
       "      <td>0.969369</td>\n",
       "      <td>0.996778</td>\n",
       "      <td>8.985594e-01</td>\n",
       "      <td>9.373128e-01</td>\n",
       "      <td>9.467211e-01</td>\n",
       "      <td>9.390071e-01</td>\n",
       "      <td>9.589644e-01</td>\n",
       "      <td>9.452435e-01</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>9.615120e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998921</td>\n",
       "      <td>9.377775e-01</td>\n",
       "      <td>9.305731e-01</td>\n",
       "      <td>9.538575e-01</td>\n",
       "      <td>8.790186e-01</td>\n",
       "      <td>9.497429e-01</td>\n",
       "      <td>0.987645</td>\n",
       "      <td>9.470714e-01</td>\n",
       "      <td>9.542075e-01</td>\n",
       "      <td>0.992551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346.0</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>3.502684e-19</td>\n",
       "      <td>6.759098e-12</td>\n",
       "      <td>3.574009e-10</td>\n",
       "      <td>1.385167e-11</td>\n",
       "      <td>5.891537e-08</td>\n",
       "      <td>1.921529e-10</td>\n",
       "      <td>0.043232</td>\n",
       "      <td>1.690447e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>8.230297e-12</td>\n",
       "      <td>3.843592e-13</td>\n",
       "      <td>7.061650e-09</td>\n",
       "      <td>5.633046e-23</td>\n",
       "      <td>1.267684e-09</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>4.139878e-10</td>\n",
       "      <td>8.169773e-09</td>\n",
       "      <td>0.051280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351.0</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>3.502684e-19</td>\n",
       "      <td>6.759098e-12</td>\n",
       "      <td>3.574009e-10</td>\n",
       "      <td>1.385167e-11</td>\n",
       "      <td>5.891537e-08</td>\n",
       "      <td>1.921529e-10</td>\n",
       "      <td>0.043232</td>\n",
       "      <td>1.690447e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>8.230297e-12</td>\n",
       "      <td>3.843592e-13</td>\n",
       "      <td>7.061650e-09</td>\n",
       "      <td>5.633046e-23</td>\n",
       "      <td>1.267684e-09</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>4.139878e-10</td>\n",
       "      <td>8.169773e-09</td>\n",
       "      <td>0.051280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354.0</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>3.502684e-19</td>\n",
       "      <td>6.759098e-12</td>\n",
       "      <td>3.574009e-10</td>\n",
       "      <td>1.385167e-11</td>\n",
       "      <td>5.891537e-08</td>\n",
       "      <td>1.921529e-10</td>\n",
       "      <td>0.043232</td>\n",
       "      <td>1.690447e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>8.230297e-12</td>\n",
       "      <td>3.843592e-13</td>\n",
       "      <td>7.061650e-09</td>\n",
       "      <td>5.633046e-23</td>\n",
       "      <td>1.267684e-09</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>4.139878e-10</td>\n",
       "      <td>8.169773e-09</td>\n",
       "      <td>0.051280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355.0</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>3.502684e-19</td>\n",
       "      <td>6.759098e-12</td>\n",
       "      <td>3.574009e-10</td>\n",
       "      <td>1.385167e-11</td>\n",
       "      <td>5.891537e-08</td>\n",
       "      <td>1.921529e-10</td>\n",
       "      <td>0.043232</td>\n",
       "      <td>1.690447e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>8.230297e-12</td>\n",
       "      <td>3.843592e-13</td>\n",
       "      <td>7.061650e-09</td>\n",
       "      <td>5.633046e-23</td>\n",
       "      <td>1.267684e-09</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>4.139878e-10</td>\n",
       "      <td>8.169773e-09</td>\n",
       "      <td>0.051280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356.0</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>3.502684e-19</td>\n",
       "      <td>6.759098e-12</td>\n",
       "      <td>3.574009e-10</td>\n",
       "      <td>1.385167e-11</td>\n",
       "      <td>5.891537e-08</td>\n",
       "      <td>1.921529e-10</td>\n",
       "      <td>0.043232</td>\n",
       "      <td>1.690447e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>8.230297e-12</td>\n",
       "      <td>3.843592e-13</td>\n",
       "      <td>7.061650e-09</td>\n",
       "      <td>5.633046e-23</td>\n",
       "      <td>1.267684e-09</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>4.139878e-10</td>\n",
       "      <td>8.169773e-09</td>\n",
       "      <td>0.051280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows Ã— 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         C00000    C00001        C00002        C00003        C00004  \\\n",
       "29.0   1.000000  1.000000  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "30.0   0.974221  0.997295  9.141179e-01  9.471023e-01  9.550767e-01   \n",
       "31.0   0.972333  0.997094  9.080412e-01  9.432867e-01  9.518216e-01   \n",
       "32.0   0.971301  0.996984  9.047341e-01  9.412059e-01  9.500456e-01   \n",
       "33.0   0.969369  0.996778  8.985594e-01  9.373128e-01  9.467211e-01   \n",
       "...         ...       ...           ...           ...           ...   \n",
       "346.0  0.000004  0.277453  3.502684e-19  6.759098e-12  3.574009e-10   \n",
       "351.0  0.000004  0.277453  3.502684e-19  6.759098e-12  3.574009e-10   \n",
       "354.0  0.000004  0.277453  3.502684e-19  6.759098e-12  3.574009e-10   \n",
       "355.0  0.000004  0.277453  3.502684e-19  6.759098e-12  3.574009e-10   \n",
       "356.0  0.000004  0.277453  3.502684e-19  6.759098e-12  3.574009e-10   \n",
       "\n",
       "             C00005        C00006        C00007    C00008        C00009  ...  \\\n",
       "29.0   1.000000e+00  1.000000e+00  1.000000e+00  1.000000  1.000000e+00  ...   \n",
       "30.0   9.485393e-01  9.654350e-01  9.538251e-01  0.993385  9.675878e-01  ...   \n",
       "31.0   9.448243e-01  9.629158e-01  9.504817e-01  0.992895  9.652226e-01  ...   \n",
       "32.0   9.427982e-01  9.615405e-01  9.486576e-01  0.992627  9.639312e-01  ...   \n",
       "33.0   9.390071e-01  9.589644e-01  9.452435e-01  0.992125  9.615120e-01  ...   \n",
       "...             ...           ...           ...       ...           ...  ...   \n",
       "346.0  1.385167e-11  5.891537e-08  1.921529e-10  0.043232  1.690447e-07  ...   \n",
       "351.0  1.385167e-11  5.891537e-08  1.921529e-10  0.043232  1.690447e-07  ...   \n",
       "354.0  1.385167e-11  5.891537e-08  1.921529e-10  0.043232  1.690447e-07  ...   \n",
       "355.0  1.385167e-11  5.891537e-08  1.921529e-10  0.043232  1.690447e-07  ...   \n",
       "356.0  1.385167e-11  5.891537e-08  1.921529e-10  0.043232  1.690447e-07  ...   \n",
       "\n",
       "         C02990        C02991        C02992        C02993        C02994  \\\n",
       "29.0   1.000000  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "30.0   0.999094  9.474965e-01  9.413818e-01  9.611170e-01  8.973998e-01   \n",
       "31.0   0.999027  9.437085e-01  9.371674e-01  9.582899e-01  8.902129e-01   \n",
       "32.0   0.998990  9.416427e-01  9.348700e-01  9.567469e-01  8.863058e-01   \n",
       "33.0   0.998921  9.377775e-01  9.305731e-01  9.538575e-01  8.790186e-01   \n",
       "...         ...           ...           ...           ...           ...   \n",
       "346.0  0.651230  8.230297e-12  3.843592e-13  7.061650e-09  5.633046e-23   \n",
       "351.0  0.651230  8.230297e-12  3.843592e-13  7.061650e-09  5.633046e-23   \n",
       "354.0  0.651230  8.230297e-12  3.843592e-13  7.061650e-09  5.633046e-23   \n",
       "355.0  0.651230  8.230297e-12  3.843592e-13  7.061650e-09  5.633046e-23   \n",
       "356.0  0.651230  8.230297e-12  3.843592e-13  7.061650e-09  5.633046e-23   \n",
       "\n",
       "             C02995    C02996        C02997        C02998    C02999  \n",
       "29.0   1.000000e+00  1.000000  1.000000e+00  1.000000e+00  1.000000  \n",
       "30.0   9.576353e-01  0.989617  9.553734e-01  9.614131e-01  0.993743  \n",
       "31.0   9.545611e-01  0.988850  9.521392e-01  9.586070e-01  0.993280  \n",
       "32.0   9.528835e-01  0.988431  9.503746e-01  9.570755e-01  0.993026  \n",
       "33.0   9.497429e-01  0.987645  9.470714e-01  9.542075e-01  0.992551  \n",
       "...             ...       ...           ...           ...       ...  \n",
       "346.0  1.267684e-09  0.007159  4.139878e-10  8.169773e-09  0.051280  \n",
       "351.0  1.267684e-09  0.007159  4.139878e-10  8.169773e-09  0.051280  \n",
       "354.0  1.267684e-09  0.007159  4.139878e-10  8.169773e-09  0.051280  \n",
       "355.0  1.267684e-09  0.007159  4.139878e-10  8.169773e-09  0.051280  \n",
       "356.0  1.267684e-09  0.007159  4.139878e-10  8.169773e-09  0.051280  \n",
       "\n",
       "[296 rows x 3000 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cox_surv_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ad4de",
   "metadata": {},
   "source": [
    "### Expected Remaining Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "25026afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_expected_lifetime = cph.predict_expectation(\n",
    "    survival_df[feature_cols]\n",
    ")\n",
    "\n",
    "cox_expected_lifetime.name = \"expected_remaining_lifetime_cox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "82655a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C00000    120.717984\n",
       "C00001    254.174138\n",
       "C00002     65.053664\n",
       "C00003     85.901774\n",
       "C00004     93.409884\n",
       "             ...    \n",
       "C02995     96.161200\n",
       "C02996    172.377696\n",
       "C02997     93.719250\n",
       "C02998    100.605858\n",
       "C02999    204.311693\n",
       "Name: expected_remaining_lifetime_cox, Length: 3000, dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cox_expected_lifetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d982b",
   "metadata": {},
   "source": [
    "## Weibull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366427ae",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "99b5cbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifelines.WeibullAFTFitter: fitted with 3000 total observations, 1144 right-censored observations>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aft = WeibullAFTFitter(penalizer=0.1)\n",
    "aft.fit(\n",
    "    survival_df,\n",
    "    duration_col=\"T\",\n",
    "    event_col=\"E\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd09a37",
   "metadata": {},
   "source": [
    "### Predict Survival Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "24455d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_surv_curves = aft.predict_survival_function(\n",
    "    survival_df[feature_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6089ed26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>customer_id</th>\n",
       "      <th>C00000</th>\n",
       "      <th>C00001</th>\n",
       "      <th>C00002</th>\n",
       "      <th>C00003</th>\n",
       "      <th>C00004</th>\n",
       "      <th>C00005</th>\n",
       "      <th>C00006</th>\n",
       "      <th>C00007</th>\n",
       "      <th>C00008</th>\n",
       "      <th>C00009</th>\n",
       "      <th>...</th>\n",
       "      <th>C02990</th>\n",
       "      <th>C02991</th>\n",
       "      <th>C02992</th>\n",
       "      <th>C02993</th>\n",
       "      <th>C02994</th>\n",
       "      <th>C02995</th>\n",
       "      <th>C02996</th>\n",
       "      <th>C02997</th>\n",
       "      <th>C02998</th>\n",
       "      <th>C02999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29.0</th>\n",
       "      <td>0.989434</td>\n",
       "      <td>0.999170</td>\n",
       "      <td>9.417857e-01</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>9.725375e-01</td>\n",
       "      <td>9.677516e-01</td>\n",
       "      <td>0.988243</td>\n",
       "      <td>0.974418</td>\n",
       "      <td>0.996349</td>\n",
       "      <td>0.982180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999603</td>\n",
       "      <td>9.665129e-01</td>\n",
       "      <td>9.601217e-01</td>\n",
       "      <td>0.980250</td>\n",
       "      <td>9.477686e-01</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.994763</td>\n",
       "      <td>0.975182</td>\n",
       "      <td>0.986254</td>\n",
       "      <td>0.997656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>0.988511</td>\n",
       "      <td>0.999097</td>\n",
       "      <td>9.368387e-01</td>\n",
       "      <td>0.976115</td>\n",
       "      <td>9.701623e-01</td>\n",
       "      <td>9.649700e-01</td>\n",
       "      <td>0.987217</td>\n",
       "      <td>0.972203</td>\n",
       "      <td>0.996029</td>\n",
       "      <td>0.980630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999568</td>\n",
       "      <td>9.636265e-01</td>\n",
       "      <td>9.566969e-01</td>\n",
       "      <td>0.978534</td>\n",
       "      <td>9.433145e-01</td>\n",
       "      <td>0.982099</td>\n",
       "      <td>0.994305</td>\n",
       "      <td>0.973032</td>\n",
       "      <td>0.985056</td>\n",
       "      <td>0.997451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31.0</th>\n",
       "      <td>0.987543</td>\n",
       "      <td>0.999020</td>\n",
       "      <td>9.316690e-01</td>\n",
       "      <td>0.974116</td>\n",
       "      <td>9.676730e-01</td>\n",
       "      <td>9.620561e-01</td>\n",
       "      <td>0.986141</td>\n",
       "      <td>0.969881</td>\n",
       "      <td>0.995693</td>\n",
       "      <td>0.979005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999531</td>\n",
       "      <td>9.606031e-01</td>\n",
       "      <td>9.531117e-01</td>\n",
       "      <td>0.976735</td>\n",
       "      <td>9.386573e-01</td>\n",
       "      <td>0.980596</td>\n",
       "      <td>0.993823</td>\n",
       "      <td>0.970778</td>\n",
       "      <td>0.983799</td>\n",
       "      <td>0.997235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32.0</th>\n",
       "      <td>0.986528</td>\n",
       "      <td>0.998940</td>\n",
       "      <td>9.262766e-01</td>\n",
       "      <td>0.972023</td>\n",
       "      <td>9.650686e-01</td>\n",
       "      <td>9.590089e-01</td>\n",
       "      <td>0.985013</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>0.995341</td>\n",
       "      <td>0.977303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999493</td>\n",
       "      <td>9.574418e-01</td>\n",
       "      <td>9.493652e-01</td>\n",
       "      <td>0.974851</td>\n",
       "      <td>9.337967e-01</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.993318</td>\n",
       "      <td>0.968420</td>\n",
       "      <td>0.982482</td>\n",
       "      <td>0.997009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.0</th>\n",
       "      <td>0.985467</td>\n",
       "      <td>0.998856</td>\n",
       "      <td>9.206620e-01</td>\n",
       "      <td>0.969836</td>\n",
       "      <td>9.623482e-01</td>\n",
       "      <td>9.558275e-01</td>\n",
       "      <td>0.983832</td>\n",
       "      <td>0.964913</td>\n",
       "      <td>0.994972</td>\n",
       "      <td>0.975523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>9.541417e-01</td>\n",
       "      <td>9.454569e-01</td>\n",
       "      <td>0.972882</td>\n",
       "      <td>9.287325e-01</td>\n",
       "      <td>0.977375</td>\n",
       "      <td>0.992790</td>\n",
       "      <td>0.965956</td>\n",
       "      <td>0.981104</td>\n",
       "      <td>0.996772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346.0</th>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.676254</td>\n",
       "      <td>5.372235e-13</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>2.010431e-06</td>\n",
       "      <td>1.968358e-07</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.178550</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829316</td>\n",
       "      <td>1.076659e-07</td>\n",
       "      <td>4.729561e-09</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>1.060820e-11</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.331139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351.0</th>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.666728</td>\n",
       "      <td>1.928507e-13</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.249433e-06</td>\n",
       "      <td>1.124430e-07</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.167736</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823706</td>\n",
       "      <td>6.017341e-08</td>\n",
       "      <td>2.360098e-09</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>4.243119e-12</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.318130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354.0</th>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.660982</td>\n",
       "      <td>1.032063e-13</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>9.346574e-07</td>\n",
       "      <td>7.989874e-08</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.161461</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820302</td>\n",
       "      <td>4.219044e-08</td>\n",
       "      <td>1.544206e-09</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>2.425703e-12</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.072977</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.310444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355.0</th>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.659061</td>\n",
       "      <td>8.364481e-14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>8.477707e-07</td>\n",
       "      <td>7.122939e-08</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.159405</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819161</td>\n",
       "      <td>3.744418e-08</td>\n",
       "      <td>1.338997e-09</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>2.010049e-12</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.071647</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.307902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356.0</th>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.657138</td>\n",
       "      <td>6.773136e-14</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>7.686472e-07</td>\n",
       "      <td>6.347018e-08</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.157367</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818017</td>\n",
       "      <td>3.321526e-08</td>\n",
       "      <td>1.160366e-09</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>1.664310e-12</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.070336</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.305371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows Ã— 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "customer_id    C00000    C00001        C00002    C00003        C00004  \\\n",
       "29.0         0.989434  0.999170  9.417857e-01  0.978022  9.725375e-01   \n",
       "30.0         0.988511  0.999097  9.368387e-01  0.976115  9.701623e-01   \n",
       "31.0         0.987543  0.999020  9.316690e-01  0.974116  9.676730e-01   \n",
       "32.0         0.986528  0.998940  9.262766e-01  0.972023  9.650686e-01   \n",
       "33.0         0.985467  0.998856  9.206620e-01  0.969836  9.623482e-01   \n",
       "...               ...       ...           ...       ...           ...   \n",
       "346.0        0.006713  0.676254  5.372235e-13  0.000028  2.010431e-06   \n",
       "351.0        0.005599  0.666728  1.928507e-13  0.000019  1.249433e-06   \n",
       "354.0        0.005012  0.660982  1.032063e-13  0.000015  9.346574e-07   \n",
       "355.0        0.004829  0.659061  8.364481e-14  0.000014  8.477707e-07   \n",
       "356.0        0.004652  0.657138  6.773136e-14  0.000013  7.686472e-07   \n",
       "\n",
       "customer_id        C00005    C00006    C00007    C00008    C00009  ...  \\\n",
       "29.0         9.677516e-01  0.988243  0.974418  0.996349  0.982180  ...   \n",
       "30.0         9.649700e-01  0.987217  0.972203  0.996029  0.980630  ...   \n",
       "31.0         9.620561e-01  0.986141  0.969881  0.995693  0.979005  ...   \n",
       "32.0         9.590089e-01  0.985013  0.967452  0.995341  0.977303  ...   \n",
       "33.0         9.558275e-01  0.983832  0.964913  0.994972  0.975523  ...   \n",
       "...                   ...       ...       ...       ...       ...  ...   \n",
       "346.0        1.968358e-07  0.003807  0.000005  0.178550  0.000210  ...   \n",
       "351.0        1.124430e-07  0.003111  0.000003  0.167736  0.000154  ...   \n",
       "354.0        7.989874e-08  0.002750  0.000002  0.161461  0.000128  ...   \n",
       "355.0        7.122939e-08  0.002638  0.000002  0.159405  0.000120  ...   \n",
       "356.0        6.347018e-08  0.002531  0.000002  0.157367  0.000113  ...   \n",
       "\n",
       "customer_id    C02990        C02991        C02992    C02993        C02994  \\\n",
       "29.0         0.999603  9.665129e-01  9.601217e-01  0.980250  9.477686e-01   \n",
       "30.0         0.999568  9.636265e-01  9.566969e-01  0.978534  9.433145e-01   \n",
       "31.0         0.999531  9.606031e-01  9.531117e-01  0.976735  9.386573e-01   \n",
       "32.0         0.999493  9.574418e-01  9.493652e-01  0.974851  9.337967e-01   \n",
       "33.0         0.999453  9.541417e-01  9.454569e-01  0.972882  9.287325e-01   \n",
       "...               ...           ...           ...       ...           ...   \n",
       "346.0        0.829316  1.076659e-07  4.729561e-09  0.000083  1.060820e-11   \n",
       "351.0        0.823706  6.017341e-08  2.360098e-09  0.000059  4.243119e-12   \n",
       "354.0        0.820302  4.219044e-08  1.544206e-09  0.000048  2.425703e-12   \n",
       "355.0        0.819161  3.744418e-08  1.338997e-09  0.000045  2.010049e-12   \n",
       "356.0        0.818017  3.321526e-08  1.160366e-09  0.000042  1.664310e-12   \n",
       "\n",
       "customer_id    C02995    C02996    C02997    C02998    C02999  \n",
       "29.0         0.983532  0.994763  0.975182  0.986254  0.997656  \n",
       "30.0         0.982099  0.994305  0.973032  0.985056  0.997451  \n",
       "31.0         0.980596  0.993823  0.970778  0.983799  0.997235  \n",
       "32.0         0.979021  0.993318  0.968420  0.982482  0.997009  \n",
       "33.0         0.977375  0.992790  0.965956  0.981104  0.996772  \n",
       "...               ...       ...       ...       ...       ...  \n",
       "346.0        0.000401  0.084315  0.000007  0.001474  0.331139  \n",
       "351.0        0.000302  0.077083  0.000005  0.001163  0.318130  \n",
       "354.0        0.000254  0.072977  0.000004  0.001007  0.310444  \n",
       "355.0        0.000240  0.071647  0.000003  0.000959  0.307902  \n",
       "356.0        0.000226  0.070336  0.000003  0.000914  0.305371  \n",
       "\n",
       "[296 rows x 3000 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibull_surv_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6edcb",
   "metadata": {},
   "source": [
    "### Expected Remaining Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8c66be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_expected_lifetime = aft.predict_expectation(\n",
    "    survival_df[feature_cols]\n",
    ")\n",
    "\n",
    "weibull_expected_lifetime.name = \"expected_remaining_lifetime_weibull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5a5dd0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C00000    160.468950\n",
       "C00001    447.959516\n",
       "C00002     79.907829\n",
       "C00003    119.196354\n",
       "C00004    108.843922\n",
       "             ...    \n",
       "C02995    134.043481\n",
       "C02996    213.135931\n",
       "C02997    113.435729\n",
       "C02998    144.240529\n",
       "C02999    294.819440\n",
       "Name: expected_remaining_lifetime_weibull, Length: 3000, dtype: float64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibull_expected_lifetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29067804",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "710903d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong-mai/venvs/maipy/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hong-mai/venvs/maipy/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.7744558289247243, 'pr_auc': 0.8306874949590989, 'precision': 0.0, 'recall': 0.0}\n",
      "                 pred_no_churn  pred_churn\n",
      "actual_no_churn            928           0\n",
      "actual_churn              1082           0\n",
      "\n",
      "Evaluation results for test set:\n",
      "{'roc_auc': 0.769000456412597, 'pr_auc': 0.830018981638551, 'precision': 0.0, 'recall': 0.0}\n",
      "                 pred_no_churn  pred_churn\n",
      "actual_no_churn            313           0\n",
      "actual_churn               350           0\n",
      "\n",
      "Evaluation results for val set:\n",
      "{'roc_auc': 0.7512745539061328, 'pr_auc': 0.7952355818624359, 'precision': 0.0, 'recall': 0.0}\n",
      "                 pred_no_churn  pred_churn\n",
      "actual_no_churn            171           0\n",
      "actual_churn               156           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong-mai/venvs/maipy/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "results_df = {}\n",
    "\n",
    "for split in split_dfs.keys():\n",
    "    print(f\"\\nEvaluation results for {split} set:\")\n",
    "\n",
    "    X, y = split_dfs[split]\n",
    "\n",
    "    feature_cols = [\n",
    "        c for c in X.columns\n",
    "        if c not in {\"T\", \"E\"}\n",
    "    ]\n",
    "\n",
    "    X_eval = X[feature_cols].copy()\n",
    "\n",
    "    X_eval[\"p_churn\"] = survival_to_churn_proba(\n",
    "        model=aft,          # or cph\n",
    "        X=X_eval,\n",
    "        horizon_days=30,\n",
    "    )\n",
    "\n",
    "    metrics, cm_df, eval_df = evaluate_churn_predictions(\n",
    "        feature_df=X_eval,\n",
    "        target_df=y,\n",
    "        threshold=0.5,\n",
    "        target_col='is_churn_30_days',\n",
    "        proba_col='p_churn'\n",
    "    )\n",
    "\n",
    "    results_df[split] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"confusion_matrix\": cm_df,\n",
    "    }\n",
    "\n",
    "    print(metrics)\n",
    "    print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bc171821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for train set:\n",
      "{'roc_auc': 0.741440061508063, 'pr_auc': 0.8212848180756193, 'precision': 1.0, 'recall': 0.0073937153419593345}\n",
      "                 pred_no_churn  pred_churn\n",
      "actual_no_churn            928           0\n",
      "actual_churn              1074           8\n",
      "\n",
      "Evaluation results for test set:\n",
      "{'roc_auc': 0.7393427658603379, 'pr_auc': 0.819928792857274, 'precision': 1.0, 'recall': 0.008571428571428572}\n",
      "                 pred_no_churn  pred_churn\n",
      "actual_no_churn            313           0\n",
      "actual_churn               347           3\n",
      "\n",
      "Evaluation results for val set:\n",
      "{'roc_auc': 0.7227470385365122, 'pr_auc': 0.7876190804045469, 'precision': 0.0, 'recall': 0.0}\n",
      "                 pred_no_churn  pred_churn\n",
      "actual_no_churn            171           0\n",
      "actual_churn               156           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong-mai/venvs/maipy/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "results_df = {}\n",
    "\n",
    "for split in split_dfs.keys():\n",
    "    print(f\"\\nEvaluation results for {split} set:\")\n",
    "\n",
    "    X, y = split_dfs[split]\n",
    "\n",
    "    feature_cols = [\n",
    "        c for c in X.columns\n",
    "        if c not in {\"T\", \"E\"}\n",
    "    ]\n",
    "\n",
    "    X_eval = X[feature_cols].copy()\n",
    "\n",
    "    X_eval[\"p_churn\"] = survival_to_churn_proba(\n",
    "        model=cph,          # or cph\n",
    "        X=X_eval,\n",
    "        horizon_days=30,\n",
    "    )\n",
    "\n",
    "    metrics, cm_df, eval_df = evaluate_churn_predictions(\n",
    "        feature_df=X_eval,\n",
    "        target_df=y,\n",
    "        threshold=0.5,\n",
    "        target_col='is_churn_30_days',\n",
    "        proba_col='p_churn'\n",
    "    )\n",
    "\n",
    "    results_df[split] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"confusion_matrix\": cm_df,\n",
    "    }\n",
    "\n",
    "    print(metrics)\n",
    "    print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a803e80",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d5b3d636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'survival_curve': [{'day': 30, 'prob': 0.9761},\n",
       "  {'day': 60, 'prob': 0.8736},\n",
       "  {'day': 90, 'prob': 0.6909}],\n",
       " 'expected_remaining_lifetime': nan}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_user_survival(\n",
    "    customer_id=\"C00003\",\n",
    "    survival_df=survival_df,\n",
    "    model=aft,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "maipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
